{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Offline Training & Visualization\n",
    "Architecture: 4 → 64 → 64 → 32 → 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"Unified Training Utilities for Neural Network-Based Flow Field Compression\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport pyarrow.csv as pv\nfrom torcheval.metrics import PeakSignalNoiseRatio\nfrom torchmetrics.image import StructuralSimilarityIndexMeasure\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport json\n\n\nclass FlowFieldDataset(Dataset):\n    def __init__(self, filepath):\n        print(f\"Loading dataset from {filepath}\")\n        read_options = pv.ReadOptions(\n            column_names=['x', 'y', 'z', 't', 'Vx', 'Vy', 'Pressure', 'TKE']\n        )\n        table = pv.read_csv(filepath, read_options=read_options)\n        data = table.to_pandas().values\n        self.inputs = data[:, :4].astype(np.float32)\n        self.targets = data[:, 4:].astype(np.float32)\n        print(\"Applying min-max normalization to [0, 1] range\")\n        self.input_min = self.inputs.min(axis=0)\n        self.input_max = self.inputs.max(axis=0)\n        self.input_range = self.input_max - self.input_min\n        self.input_range[self.input_range == 0] = 1.0\n        self.inputs = (self.inputs - self.input_min) / self.input_range\n        self.target_min = self.targets.min(axis=0)\n        self.target_max = self.targets.max(axis=0)\n        self.target_range = self.target_max - self.target_min\n        self.target_range[self.target_range == 0] = 1.0\n        self.targets = (self.targets - self.target_min) / self.target_range\n        print(f\"Dataset loaded: {len(self)} samples\")\n        print(f\"Input shape: {self.inputs.shape}, Target shape: {self.targets.shape}\")\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return torch.from_numpy(self.inputs[idx]), torch.from_numpy(self.targets[idx])\n\n    def denormalize_input(self, normalized):\n        if isinstance(normalized, torch.Tensor):\n            normalized = normalized.cpu().numpy()\n        return normalized * self.input_range + self.input_min\n\n    def denormalize_target(self, normalized):\n        if isinstance(normalized, torch.Tensor):\n            normalized = normalized.cpu().numpy()\n        return normalized * self.target_range + self.target_min\n\n\nclass BaseCompressor(nn.Module):\n    \"\"\"4 -> 64 -> 64 -> 32 -> 4\"\"\"\n    def __init__(self):\n        super(BaseCompressor, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(4, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 4)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass LargeCompressor(nn.Module):\n    \"\"\"4 -> 128 -> 128 -> 64 -> 4\"\"\"\n    def __init__(self):\n        super(LargeCompressor, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(4, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 4)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\ndef compute_psnr_ssim(predictions, targets, device):\n    predictions = predictions.to(device)\n    targets = targets.to(device)\n\n    # PSNR\n    psnr_metric = PeakSignalNoiseRatio().to(device)\n    psnr_metric.update(predictions, targets)\n    psnr = psnr_metric.compute().item()\n\n    # SSIM — reshape to (N, 1, 1, 4) then permute to (1, 1, N, 4)\n    pred_ssim = predictions.view(-1, 1, 1, predictions.shape[1])\n    target_ssim = targets.view(-1, 1, 1, targets.shape[1])\n    pred_ssim = pred_ssim.permute(1, 2, 0, 3)\n    target_ssim = target_ssim.permute(1, 2, 0, 3)\n\n    ssim_metric = StructuralSimilarityIndexMeasure(\n        gaussian_kernel=False, kernel_size=1\n    ).to(device)\n    ssim_metric.update(pred_ssim, target_ssim)\n    ssim = ssim_metric.compute().item()\n\n    return psnr, ssim\n\n\ndef compute_relative_error(predictions, targets):\n    target_norm = torch.norm(targets)\n    error = torch.norm(predictions - targets)\n    return (error / target_norm * 100).item()\n\n\ndef train_model(model, train_loader, dataset, device, epochs, model_name, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    metrics = {'loss': [], 'psnr': [], 'ssim': [], 'relative_error': [], 'time_per_epoch': []}\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"\\nTraining: {model_name}\")\n    print(f\"Training samples: {len(dataset)}\")\n    print(f\"Epochs: {epochs}\")\n    print(f\"Device: {device}\")\n    print(f\"Model parameters: {total_params:,}\")\n    print(f\"Model size: {total_params * 4 / 1024:.2f} KB\\n\")\n    for epoch in range(epochs):\n        epoch_start = time.time()\n        model.train()\n        epoch_loss = 0.0\n        all_predictions = []\n        all_targets = []\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            all_predictions.append(outputs.detach())\n            all_targets.append(targets)\n        epoch_loss /= len(train_loader)\n        metrics['loss'].append(epoch_loss)\n        all_predictions = torch.cat(all_predictions, dim=0)\n        all_targets = torch.cat(all_targets, dim=0)\n        psnr, ssim = compute_psnr_ssim(all_predictions, all_targets, device)\n        rel_error = compute_relative_error(all_predictions, all_targets)\n        metrics['psnr'].append(psnr)\n        metrics['ssim'].append(ssim)\n        metrics['relative_error'].append(rel_error)\n        epoch_time = time.time() - epoch_start\n        metrics['time_per_epoch'].append(epoch_time)\n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            print(f\"Epoch {epoch+1}/{epochs}: Loss={epoch_loss:.6f}, \"\n                  f\"PSNR={psnr:.2f} dB, SSIM={ssim:.4f}, \"\n                  f\"RE={rel_error:.2f}%, Time={epoch_time:.2f}s\")\n    model_path = os.path.join(output_dir, f'{model_name}_minmax.pth')\n    torch.save(model.state_dict(), model_path)\n    print(f\"\\nModel saved: {model_path}\")\n    norm_params = {\n        'input_min': dataset.input_min.tolist(),\n        'input_max': dataset.input_max.tolist(),\n        'input_range': dataset.input_range.tolist(),\n        'target_min': dataset.target_min.tolist(),\n        'target_max': dataset.target_max.tolist(),\n        'target_range': dataset.target_range.tolist()\n    }\n    norm_path = os.path.join(output_dir, f'{model_name}_normalization.json')\n    with open(norm_path, 'w') as f:\n        json.dump(norm_params, f, indent=2)\n    print(f\"Normalization parameters saved: {norm_path}\")\n    print(f\"\\nTraining completed\")\n    print(f\"Final Loss: {metrics['loss'][-1]:.6f}\")\n    print(f\"Final PSNR: {metrics['psnr'][-1]:.2f} dB\")\n    print(f\"Final SSIM: {metrics['ssim'][-1]:.4f}\")\n    print(f\"Final Relative Error: {metrics['relative_error'][-1]:.2f}%\")\n    print(f\"PSNR improvement: {metrics['psnr'][0]:.2f} \\u2192 {metrics['psnr'][-1]:.2f} dB \"\n          f\"({metrics['psnr'][-1]-metrics['psnr'][0]:+.2f} dB)\\n\")\n    return metrics\n\n\ndef export_metrics_csv(metrics, output_path):\n    df = pd.DataFrame(metrics)\n    df['epoch'] = range(1, len(df) + 1)\n    df = df[['epoch', 'loss', 'psnr', 'ssim', 'relative_error', 'time_per_epoch']]\n    df.to_csv(output_path, index=False)\n    print(f\"Metrics saved: {output_path}\")\n\n\nprint(\"Utilities loaded successfully.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DATA_FILE = \"/kaggle/input/ml-test-loader-original-data-csv/ML_test_loader_original_data.csv\"\nOUTPUT_DIR = \"/kaggle/working/results2/base_model_offline\"\nEPOCHS = 150\nBATCH_SIZE = 512\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\ndataset = FlowFieldDataset(DATA_FILE)\ntrain_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\nmodel = BaseCompressor().to(device)\n\nmetrics = train_model(\n    model=model,\n    train_loader=train_loader,\n    dataset=dataset,\n    device=device,\n    epochs=EPOCHS,\n    model_name='base_model',\n    output_dir=OUTPUT_DIR\n)\n\nexport_metrics_csv(metrics, f\"{OUTPUT_DIR}/base_model_metrics.csv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "epochs_range = range(1, len(metrics['loss']) + 1)\n",
    "\n",
    "axes[0, 0].plot(epochs_range, metrics['loss'], 'b-', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('MSE Loss', fontweight='bold')\n",
    "axes[0, 0].set_title('Training Loss (Normalized)', fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(epochs_range, metrics['psnr'], 'g-', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('PSNR (dB)', fontweight='bold')\n",
    "axes[0, 1].set_title('Peak Signal-to-Noise Ratio', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(epochs_range, metrics['ssim'], 'purple', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('SSIM', fontweight='bold')\n",
    "axes[1, 0].set_title('Structural Similarity Index', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_ylim([0, 1.05])\n",
    "\n",
    "axes[1, 1].plot(epochs_range, metrics['relative_error'], 'r-', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Relative Error (%)', fontweight='bold')\n",
    "axes[1, 1].set_title('Reconstruction Error', fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Base Model (64-64-32) - Offline Training',\n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/base_model_training_progress.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"Training plot saved: {OUTPUT_DIR}/base_model_training_progress.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MODEL_DIR = \"/kaggle/working/results2/base_model_offline\"\nVIZ_OUTPUT_DIR = \"/kaggle/working/results2/base_model_offline_viz\"\nTIMESTEP = 0.0396\n\nos.makedirs(VIZ_OUTPUT_DIR, exist_ok=True)\n\nprint(\"Base model offline visualization\")\nprint(f\"Device: {device}\")\n\nmodel.load_state_dict(torch.load(f\"{MODEL_DIR}/base_model_minmax.pth\", map_location=device))\nmodel.eval()\n\nprint(f\"Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n\nprint(f\"Generating predictions for {len(dataset):,} points\")\nall_inputs = torch.from_numpy(dataset.inputs).to(device)\nwith torch.no_grad():\n    all_predictions = model(all_inputs).cpu()\n\nall_targets = torch.from_numpy(dataset.targets)\n\npsnr, ssim = compute_psnr_ssim(all_predictions, all_targets, device)\nprint(f\"\\nEvaluation metrics (normalized space):\")\nprint(f\"PSNR: {psnr:.2f} dB\")\nprint(f\"SSIM: {ssim:.4f}\")\n\npredictions_denorm = dataset.denormalize_target(all_predictions.numpy())\ntargets_denorm = dataset.denormalize_target(all_targets.numpy())\ncoords_denorm = dataset.denormalize_input(dataset.inputs)\n\ntimestep_mask = np.abs(coords_denorm[:, 3] - TIMESTEP) < 1e-6\nx = coords_denorm[timestep_mask, 0]\ny = coords_denorm[timestep_mask, 1]\npred_t = predictions_denorm[timestep_mask]\ntarget_t = targets_denorm[timestep_mask]\nerrors_t = np.abs(target_t - pred_t)\n\nprint(f\"Visualizing {len(x):,} points at timestep {TIMESTEP}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Vx', 'Vy', 'Pressure', 'TKE']\n",
    "feature_indices = [0, 1, 2, 3]\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 20))\n",
    "\n",
    "for row, (idx, name) in enumerate(zip(feature_indices, feature_names)):\n",
    "    original = target_t[:, idx]\n",
    "    predicted = pred_t[:, idx]\n",
    "    error = errors_t[:, idx]\n",
    "\n",
    "    sc1 = axes[row, 0].scatter(x, y, c=original, cmap='jet', s=0.5, alpha=0.8)\n",
    "    axes[row, 0].set_title(f'Original: {name}', fontweight='bold')\n",
    "    axes[row, 0].set_aspect('equal')\n",
    "    axes[row, 0].grid(True, alpha=0.3)\n",
    "    plt.colorbar(sc1, ax=axes[row, 0])\n",
    "\n",
    "    sc2 = axes[row, 1].scatter(x, y, c=predicted, cmap='jet', s=0.5, alpha=0.8)\n",
    "    axes[row, 1].set_title(f'Prediction: {name}', fontweight='bold')\n",
    "    axes[row, 1].set_aspect('equal')\n",
    "    axes[row, 1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(sc2, ax=axes[row, 1])\n",
    "\n",
    "    sc3 = axes[row, 2].scatter(x, y, c=error, cmap='hot', s=0.5, alpha=0.8)\n",
    "    axes[row, 2].set_title(f'Error: {name}', fontweight='bold')\n",
    "    axes[row, 2].set_aspect('equal')\n",
    "    axes[row, 2].grid(True, alpha=0.3)\n",
    "    plt.colorbar(sc3, ax=axes[row, 2])\n",
    "\n",
    "plt.suptitle(f'Base Model (64-64-32) - PSNR: {psnr:.2f} dB - Timestep: {TIMESTEP}',\n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{VIZ_OUTPUT_DIR}/base_flow_visualization.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Visualization saved: {VIZ_OUTPUT_DIR}/base_flow_visualization.png\")\n",
    "plt.show()\n",
    "\n",
    "viz_metrics = {\n",
    "    'model': 'base_model_minmax.pth',\n",
    "    'architecture': '4-64-64-32-4',\n",
    "    'parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'psnr_db': float(psnr),\n",
    "    'ssim': float(ssim)\n",
    "}\n",
    "\n",
    "with open(f\"{VIZ_OUTPUT_DIR}/evaluation_metrics.json\", 'w') as f:\n",
    "    json.dump(viz_metrics, f, indent=2)\n",
    "\n",
    "print(\"Base model visualization completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}