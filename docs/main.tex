%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
%%%%%%%%%%%% This report template was constructed by
%%%%%%%%%%%% Fredrik Hedström & Jörg Schminder, Linköping university, 2016
%%%%%%%%%%%% and correspond to LiU's official layout for Master-These reports. 
%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt,a4paper,twoside,oneside]{article}

    %\usepackage{fontspec}
    %\setmainfont{Calibri} 

\usepackage{nameref}
\usepackage[left=3.5cm, right=3.5cm, top=3.5cm, bottom=3.5cm]{geometry} 
\usepackage{multirow}
\usepackage{graphicx}
\graphicspath{{./images/}}
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage{setspace}
\usepackage{boxedminipage}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{subfigure}
\usepackage{soul}
\usepackage{ctable}
\usepackage[font=footnotesize, font+=bf, skip=5pt]{caption}
\usepackage{subfig}
\usepackage{float}
\usepackage[super]{nth}
\usepackage{blindtext}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{epstopdf}
\newcommand{\mycomment}[1]{}
\usepackage{bibunits}
\usepackage{sectsty}
\usepackage{titlesec}
\usepackage[acronym]{glossaries}


\titleformat*{\section}{\Huge\bfseries}
\titleformat*{\subsection}{\LARGE\bfseries}
\titleformat*{\subsubsection}{\Large\bfseries}
\titleformat*{\paragraph}{\large\bfseries}

%\setacronymstyle{long-short}
\makeglossaries

\newacronym{Adam}{Adam}{Adaptive Moment Estimation}
\newacronym{AI}{AI}{Artificial Intelligence}
\newacronym{API}{API}{Application Programming Interface}
\newacronym{BTU}{BTU}{Brandenburgische Technische Universität Cottbus-Senftenberg}
\newacronym{CAD}{CAD}{Computer-aided design}
\newacronym{CAE}{CAE}{Complete ABAQUS Environment}
\newacronym{CFD}{CFD}{Computational Fluid Dynamics}
\newacronym{CHT}{CHT}{Conjugate Heat Transfer}
\newacronym{CPS4}{CPS4}{Constant Plane Stress 4 Nodes}
\newacronym{CPS8}{CPS8}{Constant Plane Stress 8 Nodes}
\newacronym{CPU}{CPU}{Central Processing Unit}
\newacronym{CRUD}{CRUD}{Create, Read, Update, Delete}
\newacronym{CSV}{CSV}{Comma Separated Values}
\newacronym{CSG}{CSG}{Constructive Solid Geometry}
\newacronym{D}{D}{Dimensionality: Number of Columns}
\newacronym{DC3D10}{DC3D10}{10 Node Quadratic Tetrahedron Element}
\newacronym{DL}{DL}{Deep Learning}
\newacronym{DNS}{DNS}{Direct Numerical Simulation}
\newacronym{DOF}{DOF}{Degree of Freedom}
\newacronym{EB}{EB}{Exabyte}
\newacronym{FEA}{FEA}{Finite Element Analysis}
\newacronym{FEDES}{FEDES}{Finite Element Data Exchange System}
\newacronym{FEM}{FEM}{Finite Element Method}
\newacronym{FORTRAN}{FORTRAN}{FORmula TRANslation}
\newacronym{FVM}{FVM}{Finite Volume Method}
\newacronym{GB}{GB}{Gigabyte}
\newacronym{GPU}{GPU}{Graphics Processing Unit}
\newacronym{GPR}{GPR}{Gaussian Process Regression}
\newacronym{HDF5}{HDF5}{Hierarchical Data Format 5}
\newacronym{HPC}{HPC}{High Performance Computing}
\newacronym{HTC}{HTC}{Heat Transfer Coefficient ($W m^{-2} K^{-1}$)}
\newacronym{I/O}{I/O}{Input/Output}
\newacronym{KBE}{KBE}{Knowledge Based Engineering}
\newacronym{KD}{KD}{K-Dimensional}
\newacronym{KPI}{KPI}{Key Performance Indicator}
\newacronym{M1}{M1}{Mesh1 with 33612 Data Points}
\newacronym{M2}{M2}{Mesh2 with 98292 Data Points}
\newacronym{M3}{M3}{Mesh3 with 249324 Data Points}
\newacronym{M4}{M4}{Mesh4 with 421788 Data Points}
\newacronym{MAE}{MAE}{Mean Absolute Error}
\newacronym{MB}{MB}{Megabyte}
\newacronym{ML}{ML}{Machine Learning}
\newacronym{MLP}{MLP}{Multi-Layer Perceptron}
\newacronym{MPI}{MPI}{Message Passing Interface}
\newacronym{MSE}{MSE}{Mean Squared Error}
\newacronym{MUTEX}{MUTEX}{Mutual Exclusive Object}
\newacronym{N}{N}{Data Points}
\newacronym{NetCDF}{NetCDF}{Network Common Data Form}
\newacronym{NeRF}{NeRF}{Neural Radiance Fields}
\newacronym{NN}{NN}{Neural Network}
\newacronym{NRMSE}{NRMSE}{Normalized Root Mean Squared Error}
\newacronym{O}{O}{Order: Big O Notation}
\newacronym{PB}{PB}{Petabyte}
\newacronym{PBS}{PBS}{Portable Batch System}
\newacronym{PCA}{PCA}{Principal Component Analysis}
\newacronym{PDE}{PDE}{Partial Differential Equation}
\newacronym{PINN}{PINN}{Physics Informed Neural Network}
\newacronym{POD}{POD}{Proper Orthogonal Decomposition}
\newacronym{PSNR}{PSNR}{Peak Signal-to-Noise Ratio}
\newacronym{RAPID}{RAPID}{Robust Aircraft Parametric Interactive Design}
\newacronym{RDMA}{RDMA}{Remote Direct Memory Access}
\newacronym{ReLU}{ReLU}{Rectified Linear Unit}
\newacronym{RGB}{RGB}{Red Green Blue}
\newacronym{RMSE}{RMSE}{Root Mean Squared Error}
\newacronym{ROM}{ROM}{Reduced Order Model}
\newacronym{SE}{SE}{Siemens Energy}
\newacronym{SGD}{SGD}{Stochastic Gradient Descent}
\newacronym{SINK}{SINK}{Reference Fluid Temperature $[K]$}
\newacronym{SIREN}{SIREN}{Sinusoidal Representation Networks}
\newacronym{SLURM}{SLURM}{Simple Linux Utility for Resource Management}
\newacronym{SNAME}{SNAME}{Surface Name}
\newacronym{SSIM}{SSIM}{Structural Similarity Index Measure}
\newacronym{SVD}{SVD}{Singular Value Decomposition}
\newacronym{TB}{TB}{Terabyte}
\newacronym{TEN}{TEN}{Tetrahedral Network}
\newacronym{TKE}{TKE}{Turbulent Kinetic Energy}
\newacronym{TPU}{TPU}{Tensor Processing Unit}
\newacronym{UDDS}{UDDS}{User Defined Data Structure}
\newacronym{UEXTERNALDB}{UEXTERNALDB}{User Subroutine to Manage External Database}
\newacronym{VTK}{VTK}{Visualization Toolkit}
\newacronym{VTP}{VTP}{VTK PolyData Format}
\newacronym{XML}{XML}{Extensible Markup Language}

%\defaultbibliography{references.bib}
%\defaultbibliographystyle{vancouver}

\begin{document}
%\begin{bibunit}[plain]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
%%%%%%%%%%%%    											Title page 1
%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
\newgeometry{left=1.5cm, right=1.5cm, top=0cm, bottom=.9cm}

\vspace*{2.5cm}

\begin{center}
{\Huge\textbf{
Concurrent Neural Network Training for\\
Compression of Spatio-Temporal Data
}\par}

\vspace{0.5cm}

{\Large\textbf{
Data Compression and Reconstruction\\
Using Neural Networks (NNs)
}\par}
\end{center}

\vspace{1cm}

\begin{center}
{\large \textbf{Mahesh Sadupalli}}\\
\vspace{0.2cm}
{\large M.Sc. Artificial Intelligence}
\end{center}

\vspace{1.5cm}

\begin{center}
{\large \textbf{First Examiner: Prof. Dr.-Ing. Michael Oevermann}}\\[0.2cm]
{\large \textbf{Mentor: M.Sc. Abhishek Dhiman}}
\end{center}

\vspace{1cm}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth, height=9cm]{images/1.Front_page_design.png}
\end{figure}

\vfill

\noindent
\begin{minipage}{6.2cm}
    \includegraphics*[width=6.9cm]{2.btu_logo.png}
\end{minipage}\hfill
\begin{minipage}{9cm}
    \begin{onehalfspacing}
    \raggedleft
    BTU Cottbus - Senftenberg\\
    FG Num. Mathematik und Wiss. Rechnen\\
    Master Thesis 30 ECTS $|$ Artificial Intelligence\\
    \end{onehalfspacing}
\end{minipage}

\restoregeometry
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
%%%%%%%%%%%%    				
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
%%%%%%%%%%%%    											Abstract
%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\thispagestyle{plain}
\pagenumbering{arabic}

% self review: completed
\section*{Abstract:}
\label{abstract}
This master\'s thesis investigates the application of neural networks for concurrent and real-time data compression in streaming spatio-temporal datasets~\cite{siren,nerf}. As modern applications generate increasingly large data volumes due to higher resolutions and longer run-times, traditional storage and post-processing approaches face significant I/O bottlenecks and scalability limitations~\cite{hey_fourth_2009,damaris}. This work proposes an in-situ and in-transit compression framework~\cite{paraview_catalyst,adios2} that employs deep learning neural networks to learn compact representations of data during runtime.


The methodology integrates neural networks that approximate data patterns as continuous functions of their inputs~\cite{siren}, replacing large discrete datasets with a compact set of network parameters. This enables concurrent, real-time compression without interrupting primary workflows~\cite{in_situ_framework_CFD_ML}, reducing the need for storing full data snapshots while maintaining sufficient accuracy for downstream analysis and visualization.


The framework is validated across diverse test cases, ranging from simplified benchmarks to complex, real-world scenarios. Evaluation metrics include data reduction effectiveness, reconstruction accuracy, and computational overhead. Results demonstrate substantial reduction in storage requirements with minimal impact on runtime performance.


The in-situ and in-transit implementation leverages modern computing infrastructures~\cite{catalyst_adios2_kitware,insitu_intransit_kitware2}, utilizing \acrfull{CPU}s and \acrfull{HPC} environments for data generation, neural network training, and inference. This work contributes to the broader field of machine learning–driven data management~\cite{brunton_CFD_ML_development} by providing a practical solution for handling large scale streaming datasets in real-time applications.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
%%%%%%%%%%%%    											Acknowledgements
%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\thispagestyle{plain}
\pagenumbering{arabic}

% self review: completed
\section*{Acknowledgments: TODO}
\label{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
%%%%%%%%%%%%    											Nomenclature
%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\thispagestyle{plain}
\printglossary[type=\acronymtype]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
%%%%%%%%%%%%    											Table of content
%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\thispagestyle{plain}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\thispagestyle{plain}
\pagenumbering{arabic}

% self review: completed
\section{Introduction}
\label{introduction}
The trajectory of modern science has been irrevocably altered by the emergence of what computer scientist Jim Gray termed the ``Fourth Paradigm'' of scientific discovery: data-intensive science~\cite{hey_fourth_2009}. Historically, the scientific method progressed through three distinct phases: empirical observation (the First Paradigm), theoretical generalization (the Second Paradigm), and computational simulation (the Third Paradigm). Today, we stand firmly in an era where the unification of theory, experiment, and simulation results in data volumes so vast that the data itself becomes the primary instrument of discovery. The central question -- why numerical simulations generate such colossal amounts of data is not merely a matter of improved hardware or larger storage arrays. It is a fundamental consequence of the mathematical and physical imperatives required to model the natural world with fidelity.

As computational power capable of performing $10^{18}$ calculations per second -- the volume of data generated scales not linearly, but often geometrically or exponentially~\cite{Multiphysics_simulations_hallenges_opportunities}. This growth is driven by the need to resolve finer spatial scales, capture faster temporal dynamics, model higher-dimensional phase spaces, and quantify uncertainty through massive ensembles. The resulting datasets, now measured in \acrfull{PB} and rapidly approaching \acrfull{EB}, present a paradox~\cite{hey_fourth_2009}: the very fidelity that makes these simulations valuable also makes their output increasingly difficult to store, move and analyze.

The data we are dealing with is spatio-temporal. This means it captures information across both space and time. Think of a typical simulation dataset: millions of spatial points, where each point has multiple field values like velocity, pressure, or temperature. These snapshots are recorded repeatedly as the simulation progresses through time. For example, a single timestep with 8 million spatial points and 8 field variables generates roughly 256 \acrfull{MB} of data in raw floating-point format. When we run simulations for thousands of timesteps, this quickly adds up to hundreds of \acrfull{GB} or even \acrshort{TB} for just one simulation run. These datasets are not static files sitting on a disk. They are streaming outputs from running simulations in many scientific domains: fluid dynamics, climate modeling, molecular dynamics, structural analysis, and electromagnetic simulations. The common challenge across all these fields is the same. We need to capture, store, and analyze high-dimensional spatial fields while they are being generated. This creates an urgent need for efficient data management that works in real-time, running alongside the simulation itself.

Most scientific computing workflows follow a simple sequence. First, the simulation runs and writes complete field data to disk at regular intervals. Then, after the simulation finishes, we analyze the stored data. This approach has worked for years, but it faces serious problems in modern computing environments~\cite{in_situ_kitware1}. The first problem is \acrfull{I/O} bandwidth~\cite{damaris,adios2}. Writing full-resolution data to storage takes a lot of time. In fact, disk writing often becomes the main bottleneck because our computers can calculate much faster than they can save data. The second problem is storage space~\cite{hey_fourth_2009}. We simply cannot store everything at full resolution. Scientists have to make hard choices: either save fewer timesteps or use coarser spatial grids. Both options mean losing important details. The third problem is delayed feedback~\cite{paraview_catalyst}. When simulations run for hours or days, we have to wait until the end to see if something went wrong or if we need to adjust parameters. These limitations point us toward a different approach: in-situ and in-transit processing~\cite{paraview_catalyst,damaris}. The idea is to compress data while the simulation is running, reducing or even eliminating the need to write everything to disk while still preserving enough accuracy for later analysis and visualization.

Current compression methods fall into two main types: lossless and lossy. Lossless compression methods like gzip, bzip2, and \acrfull{HDF5} with compression guarantee that we can perfectly reconstruct the original data. However, they typically compress scientific floating-point data by only 2 to 5 times. That is not enough for the data volumes we face today. Also, lossless methods need to encode and decode entire datasets, which makes them slow and impractical when we only want to look at a small region of the data. Lossy compression methods allow some controlled error in exchange for better compression ratios. Examples include wavelet-based techniques, transform coding similar to JPEG, and specialized scientific compressors like SZ~\cite{sz_compressor} and ZFP~\cite{zfp_compressor,zfp_2025}. These can compress data much more, but they come with trade-offs~\cite{mgard_multivariate}. If we compress too aggressively, we lose important details, especially in regions with sharp gradients or complex features. If we use conservative error limits, we do not save much space. More importantly, conventional lossy compressors treat data as discrete arrays of numbers rather than continuous fields. They miss the opportunity to exploit the smooth mathematical structure underneath. Both lossless and lossy methods also struggle with streaming data~\cite{damaris}. They usually need access to complete datasets to work efficiently, which does not fit well with simulations that generate data continuously in real-time.

This thesis takes a different approach: using neural networks for compression~\cite{physics_drive_CFD_Compression}. Instead of treating spatio-temporal fields as discrete arrays that need encoding, we treat them as continuous functions that can be learned~\cite{siren,nerf}. Neural networks are good at approximating complex relationships between inputs and outputs~\cite{pinn}. We can train a network to learn the mapping from coordinates and time to field values. In mathematical terms, we learn a function $f(\mathbf{x},t)$ that maps spatial coordinates $\mathbf{x}$ and time $t$ to field variables $\mathbf{u}(\mathbf{x},t)$~\cite{siren}. Once trained, the entire dataset is represented by the network's parameters, which are typically much smaller than storing all the original data points. This approach has several advantages. First, we have query flexibility~\cite{nerf}. We can reconstruct field values at any coordinate without decompressing entire datasets. Second, neural networks naturally create smooth approximations, which helps filter out noise while keeping important features~\cite{siren}. Third, we can train the network concurrently while the simulation runs, enabling real-time compression through in-situ or in-transit frameworks~\cite{in_situ_framework_CFD_ML}. Fourth, a single compact model can represent multiple timesteps or field variables together. The main challenge is designing network architectures and training methods that achieve high compression while maintaining accuracy good enough for scientific analysis~\cite{brunton_CFD_ML_development}. Finding the right balance between model size and reconstruction quality is the core technical problem this work addresses.


% self review: completed
\subsection{Problem Statement}
\label{problem_statemet}

The core problem this thesis addresses is the inability of current data management approaches to handle streaming spatio-temporal datasets from long-running simulations in real-time~\cite{in_situ_kitware1}. Traditional workflows operate on a store-first, analyze-later paradigm~\cite{paraview_catalyst}. This means simulations must pause or slow down to write data to disk, analysts must wait for simulations to complete before examining results, and storage systems must accommodate full-resolution snapshots at every timestep~\cite{Multiphysics_simulations_hallenges_opportunities}. As simulation resolutions increase and runtime durations extend, this approach creates three critical bottlenecks~\cite{hey_fourth_2009,damaris}. First, \acrshort{I/O} operations consume an increasing fraction of total runtime, sometimes exceeding the actual computation time~\cite{adios2}. Second, storage capacity limits force researchers to reduce either temporal sampling frequency or spatial resolution, sacrificing scientific fidelity~\cite{hey_fourth_2009}. Third, the inability to monitor results during runtime prevents adaptive decision-making, meaning failed or mis-configured simulations waste hours or days of computational resources before problems are detected~\cite{in_situ_kitware1,paraview_catalyst}.

Existing compression solutions do not adequately address this problem~\cite{sz_compressor,zfp_compressor}. Lossless methods provide insufficient compression ratios for modern data volumes. Lossy compressors~\cite{sz_compressor,zfp_2025,mgard_multivariate} require careful tuning of error thresholds that vary across different physical phenomena and cannot guarantee preservation of scientifically important features. More fundamentally, both approaches are designed for post-processing rather than concurrent operation~\cite{damaris}. They lack the ability to compress data incrementally as it streams from running simulations while providing immediate decompression for visualization and analysis~\cite{paraview_catalyst}. Recent advances in \acrshort{ML} and \acrshort{DL} have demonstrated that neural networks can learn compact representations of complex data~\cite{siren,nerf,physics_drive_CFD_Compression}, but applying these techniques to real-time compression of scientific simulation data remains an open challenge~\cite{in_situ_framework_CFD_ML}. The gap is not just technical but also practical: how do we integrate neural network training and inference into existing simulation workflows without disrupting their operation or requiring extensive code modifications~\cite{combining_ML_CFD_openfoam_smarsim}?

This thesis addresses the following specific problem: \textbf{How can we design, implement, and validate a neural network-based compression system that operates concurrently with running simulations, achieves significant data reduction while maintaining scientific accuracy, and integrates seamlessly with existing computational workflows?} This requires solving multiple sub-problems: developing neural network architectures that efficiently represent spatio-temporal fields, creating training protocols that work with streaming data, implementing in-situ and in-transit frameworks for concurrent processing, establishing accuracy metrics appropriate for scientific data, and demonstrating practical applicability across diverse simulation scenarios. The solution must balance competing objectives: high compression ratios versus reconstruction accuracy, training speed versus model quality, and system complexity versus ease of integration.

%\clearpage
\subsection{Literature Survey}
\label{literature_survey}

\subsubsection{Traditional Compression Methods for Scientific Data}

Data compression has been a fundamental challenge in scientific computing for decades. Traditional approaches fall into two categories: lossless and lossy compression. Lossless compression methods guarantee perfect reconstruction of original data. Common algorithms include gzip, bzip2, and LZ77-based techniques. These methods work by identifying and eliminating redundancy in data through pattern matching and entropy encoding. When applied to scientific datasets stored in formats like \acrshort{HDF5} or NetCDF, lossless compression typically achieves compression ratios between 2:1 and 5:1. However, floating-point data from simulations contains high entropy due to numerical precision, which limits the effectiveness of redundancy-based compression. More importantly, lossless methods require encoding entire datasets and do not support selective decompression of spatial regions, making them impractical for interactive visualization or analysis of large fields.

Lossy compression methods sacrifice perfect reconstruction for higher compression ratios by allowing controlled approximation errors. Transform-based methods like discrete cosine transforms and wavelet decompositions have been adapted from image compression to scientific data. More recently, error-bounded lossy compressors designed specifically for scientific applications have emerged. SZ~\cite{sz_compressor} uses prediction-based quantization with user-specified error bounds to compress floating-point arrays while guaranteeing maximum pointwise errors. ZFP~\cite{zfp_compressor,zfp_2025} employs block-based compression with fixed-rate or fixed-precision modes optimized for structured grids. MGARD~\cite{mgard_multivariate,mgard_plus} leverages multilevel decomposition for adaptive compression. These methods can achieve compression ratios of 10:1 to 100:1 depending on error tolerance and data characteristics. However, they face several limitations. First, appropriate error bounds are problem-dependent and difficult to specify a priori without understanding data distributions. Second, aggressive compression can destroy important features like shocks, vortices, or phase boundaries that are scientifically significant. Third, these methods treat data as discrete arrays rather than continuous fields, missing opportunities to exploit underlying smoothness. Finally, most lossy compressors are designed for post-processing of complete datasets rather than incremental compression of streaming data.

\subsubsection{In-Situ and In-Transit Processing Frameworks}

The growing gap between computational speed and \acrshort{I/O} bandwidth has motivated the development of in-situ and in-transit processing frameworks. In-situ processing performs analysis and visualization operations directly in simulation memory space, eliminating the need to write raw data to disk~\cite{paraview_catalyst}. In-transit processing offloads these operations to dedicated nodes that receive data from simulations through high-speed interconnects, providing concurrent processing without blocking simulation progress~\cite{damaris}. Several frameworks have been developed to support these paradigms.

ParaView Catalyst~\cite{paraview_catalyst,catalyst_revised} provides in-situ visualization capabilities by embedding ParaView pipelines directly into simulation codes. VisIt Libsim~\cite{visit_libsim} offers similar functionality with tight coupling between simulations and visualization. ADIOS (Adaptive \acrshort{I/O} System)~\cite{adios2} supports both in-situ and in-transit workflows through flexible transport methods including files, memory staging, and network protocols. Damaris~\cite{damaris} uses dedicated cores on compute nodes for asynchronous \acrshort{I/O} and processing. These frameworks have demonstrated significant performance improvements by reducing \acrshort{I/O} overhead and enabling real-time monitoring of running simulations.

However, existing in-situ and in-transit frameworks focus primarily on visualization, feature extraction, and statistical analysis rather than data compression. While some frameworks support integration with traditional compressors like SZ or ZFP, they do not provide native compression capabilities optimized for concurrent operation. The challenge of training data-driven compression models during runtime has not been addressed by these frameworks. This creates an opportunity for compression methods that can learn representations incrementally as data streams from simulations.

\subsubsection{Dimensionality Reduction and Reduced-Order Models}

Dimensionality reduction techniques have long been used in scientific computing to create compact representations of high-dimensional data. \acrfull{POD}, also known as \acrfull{PCA} in statistics, decomposes spatial fields into orthogonal basis functions ordered by energy content. By retaining only the most energetic modes, \acrshort{POD} can represent complex fields with far fewer degrees of freedom than the original discretization. \acrfull{SVD} provides the mathematical foundation for \acrshort{POD} and has been widely applied to fluid dynamics, structural mechanics, and other fields.

\acrfull{ROM} techniques extend \acrshort{POD} by using reduced bases not just for compression but also for accelerating simulations. Galerkin projection methods project governing equations onto reduced subspaces, enabling fast approximate solutions. Dynamic Mode Decomposition extracts spatiotemporal coherent structures from time-resolved data. These methods have proven effective for parametric studies and uncertainty quantification where many similar simulations must be performed.

Despite their success, \acrshort{POD} and \acrshort{ROM} methods face limitations for general compression of streaming data. First, \acrshort{POD} requires access to complete temporal sequences to compute optimal bases through eigendecomposition of correlation matrices. This is incompatible with real-time compression of data as it streams from running simulations. Second, \acrshort{POD} bases are linear combinations of snapshots, which may require many modes to represent nonlinear phenomena or sharp features. Third, while \acrshort{POD} provides excellent compression for similar data within a parameter regime, it generalizes poorly to different flow conditions or geometries without recomputing bases. Incremental or streaming variants of \acrshort{POD} have been proposed but still require storing snapshot matrices and performing expensive matrix operations, limiting their applicability to very large datasets.

\subsubsection{Machine Learning in Scientific Computing}

\acrfull{ML} and \acrfull{DL} techniques have gained significant attention in scientific computing over the past decade. Neural networks have been applied to accelerate simulations by learning surrogate models that approximate expensive physics computations. \acrfull{PINN}~\cite{pinn} embed physical laws directly into neural network training through physics-informed loss functions, enabling solutions to \acrfull{PDE}s with sparse data. Convolutional neural networks have been used for turbulence modeling and sub-grid scale closure in \acrfull{CFD}~\cite{brunton_CFD_ML_development}. Graph neural networks show promise for learning on irregular meshes.

For post-processing and analysis, neural networks have demonstrated capabilities in flow field reconstruction from sparse measurements, super-resolution of coarse simulation data, and feature extraction from complex datasets. Autoencoders, consisting of encoder and decoder networks, have been explored for learning compressed representations of simulation data~\cite{physics_drive_CFD_Compression}. These models learn to map high-dimensional inputs to low-dimensional latent spaces and reconstruct outputs from latent codes. Variational autoencoders and other generative models have been applied to scientific data compression.

Recent work on implicit neural representations has shown that neural networks can represent continuous functions by mapping coordinates directly to field values. Techniques like NeRF (Neural Radiance Fields)~\cite{nerf} for 3D scene representation and SIREN (Sinusoidal Representation Networks)~\cite{siren} for coordinate-based function approximation demonstrate that relatively small networks can encode complex spatial patterns. Some preliminary studies have explored applying these ideas to scientific visualization and data representation.

However, most \acrshort{ML} applications in scientific computing focus on offline training with complete datasets. Neural network training typically requires iterating over data multiple times (epochs) with careful hyperparameter tuning, which is incompatible with single-pass streaming scenarios. The integration of neural network training into running simulations for real-time compression has received limited attention. Questions of how to train networks concurrently without disrupting simulations, how to ensure compression quality with limited training iterations, and how to validate accuracy for scientific analysis remain largely unexplored.

\subsubsection{Gap Identification}

The literature reveals several distinct research areas that are relevant to the problem of compressing streaming spatio-temporal data from scientific simulations, but no existing work adequately addresses the complete challenge. Traditional compression methods, both lossless and lossy, provide post-processing solutions that are not designed for concurrent operation with running simulations. While they can be integrated into in-situ or in-transit frameworks, they do not learn from data characteristics and require careful parameter tuning. In-situ and in-transit processing frameworks have established the infrastructure for concurrent data processing but focus on visualization and analysis rather than compression. They lack native support for training data-driven models during runtime.

Dimensionality reduction techniques like \acrshort{POD} and \acrshort{ROM} provide mathematically rigorous compression but require batch processing of complete temporal sequences, making them unsuitable for streaming scenarios. They also rely on linear decompositions that may be inefficient for complex nonlinear phenomena. \acrshort{ML} and \acrshort{DL} methods have demonstrated impressive capabilities for learning compact representations and have been applied to various scientific computing tasks. However, existing applications focus primarily on offline training, surrogate modeling, or post-processing rather than real-time compression of streaming data.

The critical gap lies at the intersection of these areas: \textbf{there is no existing framework that combines neural network-based compression with in-situ and in-transit processing to enable concurrent, real-time compression of streaming spatio-temporal scientific data}. Specifically, the following questions remain unanswered in the literature. How can neural networks be trained efficiently on streaming data with limited passes through the dataset? How can training and inference be integrated into simulation workflows without causing \acrshort{I/O} bottlenecks or disrupting computational progress? What network architectures are most suitable for representing diverse spatio-temporal fields with varying characteristics? How can we ensure that compression preserves scientifically important features and maintains acceptable accuracy for downstream analysis? How do neural network-based compression methods compare quantitatively to traditional approaches in terms of compression ratio, reconstruction accuracy, and computational overhead?

This thesis addresses these gaps by developing, implementing, and validating a neural network-based compression system designed specifically for concurrent operation with streaming scientific simulations. By treating spatio-temporal fields as continuous functions learned by coordinate-based neural networks, and by integrating training and inference into in-situ and in-transit frameworks, this work bridges the gap between \acrshort{ML}-driven compression techniques and the practical requirements of large-scale scientific computing workflows.

\subsection{Aim}
\label{aim}

The primary aim of this thesis is to develop, implement, and validate a neural network-based data compression framework~\cite{siren,physics_drive_CFD_Compression} that operates concurrently with running scientific simulations to compress streaming spatio-temporal datasets in real-time~\cite{in_situ_framework_CFD_ML}. This framework should achieve significant data reduction while maintaining sufficient accuracy for scientific analysis and visualization, without disrupting the computational workflow of the primary simulation~\cite{paraview_catalyst,damaris}. The system must be practical, demonstrating applicability across different types of spatio-temporal data and providing measurable improvements over traditional compression approaches~\cite{sz_compressor,zfp_2025}.

To achieve this aim, the following specific objectives are pursued:

\begin{enumerate}
    \item \textbf{Design neural network architecture for compression:} Develop a coordinate-based \acrshort{MLP} architecture that maps spatio-temporal coordinates $(x, y, z, t)$ to flow field variables ($V_x$, $V_y$, $P$, \acrshort{TKE}), optimizing for both high compression ratios and acceptable reconstruction accuracy.

    \item \textbf{Implement offline training system:} Develop a complete offline training pipeline that processes the entire dataset with multiple epochs, establishing baseline performance metrics for compression ratio, \acrshort{PSNR}, and \acrshort{SSIM}.

    \item \textbf{Implement online training with temporal windows:} Design and implement an online training approach using sliding temporal windows that processes data incrementally, simulating real-time in-situ compression scenarios where data arrives sequentially.

    \item \textbf{Evaluate compression performance:} Establish comprehensive evaluation metrics including compression ratio, \acrshort{MSE}, \acrshort{PSNR}, \acrshort{SSIM}, memory usage, and training time to quantify the effectiveness of neural network-based compression.

    \item \textbf{Compare offline vs online approaches:} Conduct systematic comparison between offline and online training modes, analyzing trade-offs between reconstruction quality, training time, and practical applicability for different use cases.

    \item \textbf{Validate on \acrshort{CFD} dataset:} Demonstrate practical applicability through validation on a real-world vortex shedding \acrshort{CFD} simulation dataset containing approximately 8 million spatio-temporal samples across 300 timesteps.
\end{enumerate}
\subsection{Research Questions}
\label{research_questions}
% self review: completed

This thesis seeks to answer the following research questions:

\textbf{RQ1: How can neural network architectures and training protocols be designed to effectively learn compact representations of streaming spatio-temporal data with limited passes through the dataset?}

This question addresses the technical design challenges of neural network-based compression. It encompasses architectural choices such as network depth, width, and activation functions that map spatial coordinates and time to field variables. It also covers training strategies, loss functions, and optimization techniques suitable for streaming scenarios where data arrives incrementally and cannot be iterated over multiple times. The question investigates the trade-offs between model size, training efficiency, and reconstruction accuracy.

\textbf{RQ2: How can neural network training and inference be integrated into scientific simulation workflows to enable concurrent, real-time compression without disrupting computational progress or creating \acrshort{I/O} bottlenecks?}

This question focuses on the practical implementation of in-situ and in-transit compression systems~\cite{paraview_catalyst,damaris,adios2}. It addresses system architecture, data flow patterns, and memory-based staging mechanisms that allow neural networks to learn from streaming data while simulations run~\cite{in_situ_framework_CFD_ML}. The question investigates computational overhead, resource allocation, and the feasibility of concurrent operation compared to traditional file writing approaches~\cite{damaris}.

\textbf{RQ3: What compression performance, reconstruction accuracy, and practical applicability can neural network-based compression achieve compared to traditional methods across diverse spatio-temporal datasets?}

This question evaluates the effectiveness and generalizability of the proposed approach~\cite{physics_drive_CFD_Compression}. It covers compression ratios, reconstruction errors, preservation of scientifically important features, and computational costs. The question includes quantitative comparisons with lossless compressors (gzip, \acrshort{HDF5}), lossy compressors (SZ~\cite{sz_compressor}, ZFP~\cite{zfp_compressor,zfp_2025}), and dimensionality reduction techniques (\acrshort{POD}, \acrshort{PCA}). It also examines how well the approach generalizes across different physical phenomena, spatial resolutions, and temporal dynamics.


\subsection{Thesis Contributions}
\label{thesis_contributions}

This thesis makes the following contributions to the field of neural network-based scientific data compression:

\begin{enumerate}
    \item \textbf{Offline Regression System with Extreme Compression:} Development and validation of a coordinate-based neural network compression system that achieves a compression ratio of approximately 27,395:1 on \acrshort{CFD} flow field data. The system compresses 794.44~MB of spatio-temporal data into a 0.03~MB neural network model while maintaining a \acrshort{PSNR} of 16.80~dB and \acrshort{SSIM} of 0.9893.

    \item \textbf{Online Training with Temporal Windows:} Introduction of an incremental online training methodology using sliding temporal windows that enables real-time compression of streaming data. This approach achieves 12$\times$ faster training compared to offline mode while maintaining acceptable reconstruction quality (\acrshort{PSNR}: 15.67~dB, \acrshort{SSIM}: 0.9565).

    \item \textbf{Comprehensive Evaluation Framework:} Establishment of a multi-metric evaluation framework combining compression ratio, \acrshort{MSE}, \acrshort{PSNR}, \acrshort{SSIM}, per-variable analysis, and spatial error distribution to assess neural network compression performance for scientific applications.

    \item \textbf{Memory-Efficient Implementation:} Demonstration of constant memory usage (22.8~MB per timestep) independent of total dataset size, enabling scalability to arbitrarily large simulations without memory constraints.

    \item \textbf{Open-Source Software Deliverables:} Development of a complete, documented software package including offline training, online training, inference system, visualization tools, and ParaView export capabilities for integration with existing scientific workflows.
\end{enumerate}

\subsection{Limitation}
\label{limitations}

While this thesis aims to develop a comprehensive neural network-based compression framework for spatio-temporal data, certain limitations and scope boundaries must be acknowledged.

\textbf{Data Types and Application Domains:} This work focuses specifically on spatio-temporal datasets from scientific simulations, particularly continuous field data such as velocity, pressure, and temperature distributions. The framework is designed for structured data where spatial coordinates and field values have clear relationships. It does not address compression of unstructured data types such as text, images, video, or discrete event data. While the approach is validated across multiple test cases to demonstrate generalizability, it cannot cover all possible scientific domains or data characteristics.

\textbf{Implementation Scope:} The in-situ and in-transit implementation is developed and tested using \acrshort{CPU}-based computing environments. While the framework is designed to be extensible, this thesis does not explore distributed training across multiple compute nodes, \acrshort{GPU} acceleration for neural network training, or integration with specific \acrshort{HPC} job schedulers. The data generation component serves as a proxy for actual simulation codes to enable controlled testing, but integration with production simulation software is left for future work.

\textbf{Compression Guarantees:} Unlike error-bounded lossy compressors such as SZ~\cite{sz_compressor} and ZFP~\cite{zfp_compressor,zfp_2025} that provide strict mathematical guarantees on maximum pointwise errors, neural network-based compression offers statistical approximation without hard error bounds~\cite{siren}. While reconstruction accuracy can be measured and controlled through training objectives and validation, the approach does not guarantee that specific error thresholds will never be exceeded at any point in the domain. Users requiring provable error bounds may need to combine this approach with validation or use traditional error-bounded methods~\cite{mgard_multivariate}.

\textbf{Training and Scalability Constraints:} The effectiveness of neural network compression depends on the data containing learnable patterns and structure. Highly irregular, noisy, or random data may not compress well with this approach. The framework assumes that training can be performed with reasonable computational resources concurrent with data generation. Extremely high data generation rates that exceed training throughput may require buffering or sampling strategies. The thesis explores compression for datasets up to several million points per timestep, but scalability to extremely large problems with billions of points requires further investigation.

\textbf{Comparative Evaluation:} Performance comparisons focus on widely-used compression methods including lossless compressors, lossy compressors (SZ~\cite{sz_compressor}, ZFP~\cite{zfp_2025}), and dimensionality reduction techniques (\acrshort{POD}, \acrshort{PCA}). The thesis does not provide exhaustive comparisons with all existing compression algorithms or emerging methods~\cite{mgard_plus}. Benchmarking is conducted on available hardware and software environments, and performance characteristics may vary on different computing platforms.

\textbf{Generalization and Transferability:} While the framework is designed to be general, the specific network architectures and hyperparameters may require tuning for different types of physical phenomena or data characteristics. The thesis does not claim that a single trained model can compress arbitrary datasets without any adaptation. Transfer learning and domain adaptation strategies are beyond the scope of this work.

These limitations define the boundaries of the current work while identifying opportunities for future research and development.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Chapter 2  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\thispagestyle{empty}
\thispagestyle{plain}

\section{Theory}
\label{Theory}

This chapter presents the theoretical foundations underlying the neural network-based compression framework developed in this thesis. It covers the fundamental concepts of neural networks, implicit neural representations, compression theory, dimensionality reduction techniques, in-situ processing paradigms, and characteristics of spatio-temporal scientific data.

\subsection{Neural Networks Fundamentals}
\label{sec:neural_networks_fundamentals}

Neural networks are computational models inspired by the structure and function of biological neural systems. At their core, they are composed of interconnected processing units called neurons organized in layers that transform input data into desired outputs through learned mathematical operations~\cite{goodfellow_deep_learning}.

\subsubsection{Network Architecture}

A neural network typically consists of three types of layers: an input layer, one or more hidden layers, and an output layer. The input layer receives the raw data, hidden layers perform intermediate transformations, and the output layer produces the final prediction or representation. Each neuron in a layer is connected to neurons in the adjacent layers through weighted connections. These weights are the parameters that the network learns during training.

Mathematically, for a layer $l$ with $n$ neurons, the output of neuron $i$ can be expressed as:
\begin{equation}
z_i^{(l)} = \sum_{j=1}^{m} w_{ij}^{(l)} a_j^{(l-1)} + b_i^{(l)}
\end{equation}
where $w_{ij}^{(l)}$ represents the weight connecting neuron $j$ from the previous layer to neuron $i$ in the current layer, $a_j^{(l-1)}$ is the activation from the previous layer, $b_i^{(l)}$ is the bias term, and $m$ is the number of neurons in the previous layer. The weighted sum $z_i^{(l)}$ is then passed through a non-linear activation function to produce the neuron's output.

\subsubsection{Activation Functions}

Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns beyond simple linear transformations. Without activation functions, stacking multiple layers would be equivalent to a single linear transformation, severely limiting the network's representational capacity.

Common activation functions include:

\textbf{Rectified Linear Unit (ReLU):} Defined as $\sigma(z) = \max(0, z)$, ReLU has become the most widely used activation function due to its simplicity and effectiveness in training deep networks~\cite{relu_nair}. It addresses the vanishing gradient problem that affects sigmoid and tanh functions in deep architectures.

\textbf{Sigmoid:} The sigmoid function $\sigma(z) = \frac{1}{1 + e^{-z}}$ maps inputs to values between 0 and 1. While historically popular, it suffers from vanishing gradients for extreme input values and is now primarily used in output layers for binary classification tasks~\cite{goodfellow_deep_learning}.

\textbf{Hyperbolic Tangent (Tanh):} Defined as $\sigma(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$, tanh maps inputs to values between -1 and 1. It is zero-centered, which can help with gradient flow during training compared to sigmoid~\cite{goodfellow_deep_learning}.

\textbf{Sinusoidal Activation:} For representing continuous functions and their derivatives, sinusoidal activation functions like $\sigma(z) = \sin(z)$ have shown superior performance~\cite{siren}. These periodic activations enable networks to learn high-frequency details and represent derivatives accurately, which is particularly valuable for scientific applications.

The activation function applied to the weighted sum produces the final output of a neuron:
\begin{equation}
a_i^{(l)} = \sigma(z_i^{(l)})
\end{equation}

\subsubsection{Forward Propagation}

Forward propagation is the process of computing the network's output given an input. Data flows from the input layer through the hidden layers to the output layer. At each layer, the weighted sum of inputs is computed, the activation function is applied, and the results are passed to the next layer. This process continues until the output layer produces the final prediction.

For a network with $L$ layers, given an input $\mathbf{x}$, forward propagation computes:
\begin{align}
\mathbf{a}^{(0)} &= \mathbf{x} \\
\mathbf{z}^{(l)} &= \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)} \quad \text{for } l = 1, 2, \ldots, L \\
\mathbf{a}^{(l)} &= \sigma(\mathbf{z}^{(l)}) \quad \text{for } l = 1, 2, \ldots, L
\end{align}
where $\mathbf{W}^{(l)}$ is the weight matrix for layer $l$, $\mathbf{b}^{(l)}$ is the bias vector, and $\sigma$ is the activation function. The final output is $\mathbf{a}^{(L)}$.

\subsubsection{Loss Functions}

Loss functions quantify the difference between the network's predictions and the true target values. They provide a scalar measure of performance that the training process seeks to minimize. The choice of loss function depends on the task.

\textbf{Mean Squared Error (MSE):} For regression tasks, where the goal is to predict continuous values, MSE is commonly used~\cite{pinn}:
\begin{equation}
\mathcal{L}_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2
\end{equation}
where $N$ is the number of samples, $\hat{y}_i$ is the predicted value, and $y_i$ is the true value. MSE penalizes larger errors more heavily due to the squaring operation.

\textbf{Mean Absolute Error (MAE):} An alternative regression loss that is less sensitive to outliers:
\begin{equation}
\mathcal{L}_{\text{MAE}} = \frac{1}{N} \sum_{i=1}^{N} |\hat{y}_i - y_i|
\end{equation}

For the compression task addressed in this thesis, where neural networks learn to reconstruct spatio-temporal fields from coordinates, MSE is typically employed to measure reconstruction accuracy~\cite{siren,nerf}.

\subsubsection{Backpropagation and Optimization}

Training a neural network involves finding the optimal values for all weights and biases that minimize the loss function. This is accomplished through backpropagation combined with gradient-based optimization algorithms.

Backpropagation efficiently computes the gradient of the loss function with respect to each parameter by applying the chain rule of calculus~\cite{rumelhart_backprop}. Starting from the output layer and moving backward through the network, gradients are computed for each layer:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(l)}} \cdot \frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{W}^{(l)}}
\end{equation}

Once gradients are computed, parameters are updated using an optimization algorithm. The simplest approach is gradient descent:
\begin{equation}
\mathbf{W}^{(l)} \leftarrow \mathbf{W}^{(l)} - \eta \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}
\end{equation}
where $\eta$ is the learning rate, a hyperparameter controlling the step size of updates.

More sophisticated optimizers like Adam (Adaptive Moment Estimation) adapt the learning rate for each parameter based on estimates of first and second moments of the gradients~\cite{adam_optimizer}. Adam has become a standard choice for training neural networks due to its robustness and fast convergence properties. It combines ideas from momentum and adaptive learning rates:
\begin{align}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\theta_t &= \theta_{t-1} - \frac{\eta}{\sqrt{v_t} + \epsilon} m_t
\end{align}
where $g_t$ is the gradient at iteration $t$, $m_t$ and $v_t$ are estimates of the first and second moments, $\beta_1$ and $\beta_2$ are decay rates (typically 0.9 and 0.999), and $\epsilon$ is a small constant for numerical stability.

The training process iterates through the dataset multiple times (epochs), repeatedly performing forward propagation, loss computation, backpropagation, and parameter updates until convergence or a stopping criterion is met. For the streaming data compression scenario in this thesis, training must be adapted to work with limited passes through the data, which will be discussed in the Methodology chapter.

\subsection{Implicit Neural Representations}
\label{sec:implicit_neural_representations}

Implicit neural representations represent a paradigm shift in how neural networks are used for data representation. Instead of using networks to map discrete input samples to outputs, implicit representations use neural networks to parameterize continuous functions~\cite{siren,nerf}. This approach is particularly powerful for representing spatial data, temporal signals, and spatio-temporal fields common in scientific computing.

\subsubsection{Coordinate-Based Neural Networks}

Traditional neural networks for image processing, for example, operate on discrete pixel grids. An image is represented as a 2D array of pixel values, and the network learns to classify, segment, or transform this discrete representation. In contrast, coordinate-based neural networks treat data as continuous functions that can be evaluated at any point in the domain~\cite{siren}.

For an image, instead of storing pixel values directly, a coordinate-based network learns a function $f: \mathbb{R}^2 \rightarrow \mathbb{R}^3$ that maps 2D spatial coordinates $(x, y)$ to RGB color values $(r, g, b)$. The network's weights implicitly encode the entire image as a continuous function. To retrieve the color at any location, we simply query the network with those coordinates.

This concept generalizes to arbitrary dimensions. For spatio-temporal scientific data, we can define a function:
\begin{equation}
f_\theta: (\mathbf{x}, t) \rightarrow \mathbf{u}
\end{equation}
where $\mathbf{x} \in \mathbb{R}^d$ represents spatial coordinates (with $d=1, 2,$ or $3$ for 1D, 2D, or 3D spatial domains), $t \in \mathbb{R}$ represents time, and $\mathbf{u} \in \mathbb{R}^m$ represents the field variables of interest (velocity, pressure, temperature, etc.). The subscript $\theta$ indicates that the function is parameterized by the neural network's weights.

\subsubsection{Continuous Function Approximation}

The universal approximation theorem states that neural networks with sufficient width can approximate any continuous function to arbitrary accuracy~\cite{hornik_universal,cybenko_universal}. Implicit neural representations leverage this property to represent complex continuous functions with relatively compact network parameters~\cite{pinn}.

The key advantage is compression. Instead of storing discrete field values at millions of spatial points across multiple timesteps, we store only the network's weights. For a network with $L$ layers and $N$ neurons per layer, the number of parameters is approximately $O(LN^2)$, which can be orders of magnitude smaller than the original discrete data for sufficiently large datasets.

The network learns to approximate the underlying continuous function through supervised training. Given a set of samples $\{(\mathbf{x}_i, t_i, \mathbf{u}_i)\}_{i=1}^N$ from the discrete dataset, the network is trained to minimize the reconstruction error:
\begin{equation}
\mathcal{L} = \frac{1}{N} \sum_{i=1}^N \| f_\theta(\mathbf{x}_i, t_i) - \mathbf{u}_i \|^2
\end{equation}

Once trained, the network can interpolate values at coordinates not present in the training data, effectively providing a continuous representation of the field.

\subsubsection{SIREN: Sinusoidal Representation Networks}

A critical challenge in implicit neural representations is the choice of activation function. Standard activation functions like ReLU struggle to represent high-frequency details and cannot accurately capture derivatives of the represented function. This limitation is problematic for scientific data where derivatives (gradients, curvatures) often carry important physical meaning.

SIREN (Sinusoidal Representation Networks) addresses this by using periodic sinusoidal activation functions~\cite{siren}:
\begin{equation}
\sigma(z) = \sin(\omega z)
\end{equation}
where $\omega$ is a frequency parameter that can be tuned per layer.

The key insight is that the derivatives of sinusoidal functions are also sinusoidal, allowing the network to represent both the function and all its derivatives with the same representational capacity. For a SIREN network, if $f_\theta(\mathbf{x})$ represents a function, then $\nabla f_\theta(\mathbf{x})$ (the gradient) and $\nabla^2 f_\theta(\mathbf{x})$ (the Laplacian) can be computed through automatic differentiation and are also well-represented by the network~\cite{siren}.

The architecture of a SIREN network for representing a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ consists of:
\begin{align}
\mathbf{z}_0 &= \mathbf{x} \\
\mathbf{z}_{l+1} &= \sin(\omega_l (\mathbf{W}_l \mathbf{z}_l + \mathbf{b}_l)) \quad \text{for } l = 0, 1, \ldots, L-2 \\
\mathbf{y} &= \mathbf{W}_{L-1} \mathbf{z}_{L-1} + \mathbf{b}_{L-1}
\end{align}

Note that the final layer uses a linear activation to allow the network to represent values outside the $[-1, 1]$ range of the sine function. Proper initialization of weights is crucial for SIREN networks to converge effectively. The original work~\cite{siren} proposes a specific initialization scheme where weights are drawn from uniform distributions with carefully chosen bounds based on the layer depth.

\subsubsection{Neural Radiance Fields (NeRF)}

Neural Radiance Fields (NeRF) demonstrate the power of implicit neural representations for 3D scene reconstruction~\cite{nerf}. While originally developed for computer graphics, the underlying principles are highly relevant to scientific data compression.

NeRF represents a 3D scene as a continuous volumetric function that maps a 3D location $\mathbf{x}$ and viewing direction $\mathbf{d}$ to volume density $\sigma$ and color $\mathbf{c}$:
\begin{equation}
f_\theta: (\mathbf{x}, \mathbf{d}) \rightarrow (\sigma, \mathbf{c})
\end{equation}

The network is trained on 2D images with known camera poses. Through volume rendering equations, the network learns to reconstruct the 3D scene such that rendering from any viewpoint produces images consistent with the training views. This demonstrates that neural networks can learn complex 3D structures from 2D observations and interpolate to novel viewpoints.

For scientific data compression, the NeRF concept translates to learning a continuous representation of 3D spatial fields. Instead of viewing direction, we might have time as an additional input. Instead of color, we have physical field variables. The key takeaway is that neural networks can effectively compress high-dimensional spatial data into a compact set of parameters while preserving the ability to query field values at arbitrary locations.

\subsubsection{Application to Spatio-Temporal Data Compression}

Implicit neural representations provide an elegant framework for compressing spatio-temporal scientific data. The compression workflow consists of three stages:

\textbf{Encoding (Training):} Given discrete samples of a spatio-temporal field $\{(\mathbf{x}_i, t_i, \mathbf{u}_i)\}$, train a neural network $f_\theta$ to minimize reconstruction error. The network learns a continuous function that approximates the field. This training phase is the compression step where we replace a large discrete dataset with a small set of network parameters $\theta$.

\textbf{Storage:} Instead of storing the full discrete dataset, we store only the network architecture specification (number of layers, neurons per layer, activation functions) and the learned parameters $\theta$. For a network with $P$ parameters, this requires $P \times 4$ bytes (assuming 32-bit floats), which is typically much smaller than the original data.

\textbf{Decoding (Inference):} To reconstruct field values, we perform forward propagation through the network. Given any coordinates $(\mathbf{x}, t)$, the network produces the corresponding field values $\mathbf{u} = f_\theta(\mathbf{x}, t)$. This allows querying the field at arbitrary locations, including interpolating between original sampling points.

The compression ratio depends on the complexity of the field, the network size, and the acceptable reconstruction error. For smooth fields with learnable patterns, compression ratios of 100:1 or higher can be achieved while maintaining scientifically acceptable accuracy~\cite{physics_drive_CFD_Compression}. The trade-off between compression ratio (smaller networks) and reconstruction accuracy (larger networks) is a central consideration in this thesis and will be explored in detail in the Results chapter.

\subsection{Data Compression Theory}
\label{sec:data_compression_theory}

Data compression is the process of reducing the number of bits required to represent information. The fundamental goal is to eliminate redundancy and represent data more efficiently while preserving essential information. Compression techniques are broadly classified into two categories: lossless and lossy compression.

\subsubsection{Lossless Compression}

Lossless compression guarantees that the original data can be perfectly reconstructed from the compressed representation. No information is lost during the compression and decompression process. Lossless methods work by identifying and eliminating statistical redundancy in data.

Common lossless compression algorithms include:

\textbf{Entropy Coding:} Methods like Huffman coding~\cite{huffman_coding} and arithmetic coding assign shorter codes to frequently occurring symbols and longer codes to rare symbols. These techniques approach the theoretical limit defined by Shannon's source coding theorem~\cite{shannon_information}, which states that the average code length cannot be less than the entropy of the source.

\textbf{Dictionary-Based Methods:} Algorithms like LZ77 (used in gzip) and LZ78 replace repeated sequences of symbols with references to a dictionary. When a sequence appears multiple times, subsequent occurrences are encoded as pointers to the first occurrence, reducing the overall size.

For scientific floating-point data, lossless compression typically achieves modest compression ratios of 2:1 to 5:1. The fundamental limitation is that scientific simulation data contains high entropy due to numerical precision. Each floating-point value carries significant information content, and there is limited redundancy to exploit. Additionally, small numerical variations between similar values prevent effective pattern matching by dictionary-based methods.

\subsubsection{Lossy Compression}

Lossy compression allows controlled information loss in exchange for higher compression ratios. The key idea is to discard information that is less perceptually or scientifically important while preserving the essential characteristics of the data. Lossy compression is acceptable when perfect reconstruction is not required and some approximation error is tolerable~\cite{sz_compressor,zfp_compressor}.

For scientific data, lossy compression must be carefully controlled to ensure that the introduced errors do not compromise the validity of scientific analysis. Different application domains have different error tolerances. For example, visualization may tolerate higher errors than quantitative analysis of physical phenomena.

\subsubsection{Compression Ratio}

The compression ratio quantifies the reduction in data size achieved by compression:
\begin{equation}
\text{Compression Ratio} = \frac{\text{Size of Original Data}}{\text{Size of Compressed Data}}
\end{equation}

A compression ratio of 10:1 means the compressed data is one-tenth the size of the original. Higher ratios indicate better compression. For neural network-based compression, the compressed size includes the network architecture specification and learned parameters.

For a dataset with $N$ data points, each represented by $m$ floating-point values (4 bytes each), the original size is $4Nm$ bytes. If compressed using a neural network with $P$ parameters, the compressed size is approximately $4P$ bytes (plus negligible architecture metadata). The compression ratio is:
\begin{equation}
\text{Compression Ratio} = \frac{4Nm}{4P} = \frac{Nm}{P}
\end{equation}

\subsubsection{Error Metrics}

For lossy compression, we must quantify the approximation error between the original data and the reconstructed data. Several metrics are commonly used:

\textbf{Mean Squared Error (MSE):} The average squared difference between original and reconstructed values:
\begin{equation}
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (u_i - \hat{u}_i)^2
\end{equation}
where $u_i$ is the original value and $\hat{u}_i$ is the reconstructed value. MSE penalizes large errors heavily due to the squaring operation.

\textbf{Root Mean Squared Error (RMSE):} The square root of MSE, which has the same units as the data:
\begin{equation}
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (u_i - \hat{u}_i)^2}
\end{equation}

\textbf{Normalized Root Mean Squared Error (NRMSE):} RMSE normalized by the data range:
\begin{equation}
\text{NRMSE} = \frac{\text{RMSE}}{u_{\max} - u_{\min}}
\end{equation}
This metric is dimensionless and allows comparison across datasets with different scales.

\textbf{Peak Signal-to-Noise Ratio (PSNR):} Commonly used in image compression, PSNR is defined as:
\begin{equation}
\text{PSNR} = 10 \log_{10} \left( \frac{u_{\max}^2}{\text{MSE}} \right)
\end{equation}
where $u_{\max}$ is the maximum possible value in the data. Higher PSNR values indicate better reconstruction quality. PSNR is measured in decibels (dB).

\textbf{Relative Error:} The error relative to the magnitude of the data:
\begin{equation}
\text{Relative Error} = \frac{\| \mathbf{u} - \hat{\mathbf{u}} \|_2}{\| \mathbf{u} \|_2}
\end{equation}
where $\|\cdot\|_2$ denotes the Euclidean (L2) norm.

\subsubsection{Error-Bounded Lossy Compression}

Error-bounded lossy compressors provide guarantees on the maximum reconstruction error~\cite{sz_compressor,zfp_compressor,zfp_2025}. Users specify an error tolerance $\epsilon$, and the compressor guarantees that the maximum absolute or relative error does not exceed this bound.

For absolute error bounds:
\begin{equation}
|u_i - \hat{u}_i| \leq \epsilon \quad \forall i
\end{equation}

For relative error bounds:
\begin{equation}
\frac{|u_i - \hat{u}_i|}{|u_i|} \leq \epsilon \quad \forall i
\end{equation}

Error-bounded compressors like SZ~\cite{sz_compressor} use prediction-based quantization with error bounds, while ZFP~\cite{zfp_compressor,zfp_2025} uses block-based transforms with fixed-rate or fixed-precision modes. MGARD~\cite{mgard_multivariate,mgard_plus} employs multilevel decomposition with error control at multiple scales.

These guarantees are valuable for scientific applications where certain error thresholds must not be exceeded to maintain physical validity or numerical stability of downstream analysis. However, achieving strict error bounds typically limits achievable compression ratios compared to methods without hard guarantees.

\subsubsection{Rate-Distortion Theory}

Rate-distortion theory provides a theoretical framework for understanding the fundamental trade-off between compression ratio (rate) and reconstruction error (distortion). For a given source of data, there exists a rate-distortion function $R(D)$ that specifies the minimum achievable bit rate for a given distortion level $D$.

The rate-distortion function characterizes the inherent compressibility of data. For sources with more structure and redundancy, the rate-distortion curve is more favorable, meaning high compression can be achieved with low distortion. For complex, high-entropy sources, the trade-off is less favorable.

In practice, compression algorithms aim to operate near the rate-distortion curve. Neural network-based compression can potentially approach or even improve upon traditional methods by learning data-specific representations rather than using fixed transforms or dictionaries. The network architecture, training procedure, and capacity (number of parameters) determine where on the rate-distortion curve a particular neural compression system operates~\cite{physics_drive_CFD_Compression}.

\subsubsection{Compression for Streaming Data}

Traditional compression algorithms are designed for batch processing of complete datasets. They often require multiple passes through the data to build dictionaries, compute optimal transforms, or determine appropriate quantization parameters. This is incompatible with streaming scenarios where data arrives continuously and must be compressed in real-time.

Streaming compression requires algorithms that can:
\begin{itemize}
    \item Operate incrementally on data as it arrives
    \item Make compression decisions without requiring future data
    \item Adapt to changing data characteristics over time
    \item Maintain low latency and computational overhead
\end{itemize}

For neural network-based compression in streaming settings, the challenge is to train the network concurrently with data generation while achieving acceptable compression and accuracy with limited training iterations~\cite{in_situ_framework_CFD_ML}. This represents a departure from standard neural network training, which typically involves many passes (epochs) through the dataset. The methodology developed in this thesis addresses these challenges, as detailed in Chapter 3.

\subsection{Dimensionality Reduction Techniques}
\label{sec:dimensionality_reduction}

Dimensionality reduction techniques transform high-dimensional data into lower-dimensional representations while preserving essential information. These methods have been widely used in scientific computing for data compression, visualization, and building reduced-order models. Understanding these classical approaches provides important context for the neural network-based compression developed in this thesis.

\subsubsection{Singular Value Decomposition (SVD)}

Singular Value Decomposition is a fundamental matrix factorization technique that decomposes any matrix into three components. For a data matrix $\mathbf{U} \in \mathbb{R}^{m \times n}$ containing $n$ snapshots of a field with $m$ spatial points, the SVD is:
\begin{equation}
\mathbf{U} = \mathbf{\Phi} \mathbf{\Sigma} \mathbf{V}^T
\end{equation}
where:
\begin{itemize}
    \item $\mathbf{\Phi} \in \mathbb{R}^{m \times m}$ contains left singular vectors (spatial modes)
    \item $\mathbf{\Sigma} \in \mathbb{R}^{m \times n}$ is a diagonal matrix of singular values $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$
    \item $\mathbf{V} \in \mathbb{R}^{n \times n}$ contains right singular vectors (temporal coefficients)
\end{itemize}

The singular values $\sigma_i$ indicate the importance of each mode. Modes with larger singular values capture more of the data's variance. For compression, we can truncate the SVD by keeping only the first $k$ modes:
\begin{equation}
\mathbf{U} \approx \mathbf{U}_k = \mathbf{\Phi}_k \mathbf{\Sigma}_k \mathbf{V}_k^T
\end{equation}
where $\mathbf{\Phi}_k$ contains the first $k$ columns of $\mathbf{\Phi}$, $\mathbf{\Sigma}_k$ is a $k \times k$ diagonal matrix, and $\mathbf{V}_k$ contains the first $k$ columns of $\mathbf{V}$.

The Eckart-Young theorem~\cite{eckart_young} guarantees that this truncated SVD provides the best rank-$k$ approximation to $\mathbf{U}$ in the Frobenius norm sense. The reconstruction error is determined by the discarded singular values:
\begin{equation}
\| \mathbf{U} - \mathbf{U}_k \|_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2}
\end{equation}

\subsubsection{Principal Component Analysis (PCA)}

Principal Component Analysis is a statistical technique for finding the directions of maximum variance in high-dimensional data. When applied to mean-centered data, PCA is equivalent to computing the SVD of the data matrix.

For a dataset with snapshots $\{\mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_n\}$ where each $\mathbf{u}_i \in \mathbb{R}^m$, we first compute the mean:
\begin{equation}
\bar{\mathbf{u}} = \frac{1}{n} \sum_{i=1}^n \mathbf{u}_i
\end{equation}

Then construct the mean-centered data matrix:
\begin{equation}
\mathbf{U}_{\text{centered}} = [\mathbf{u}_1 - \bar{\mathbf{u}}, \mathbf{u}_2 - \bar{\mathbf{u}}, \ldots, \mathbf{u}_n - \bar{\mathbf{u}}]
\end{equation}

The covariance matrix is:
\begin{equation}
\mathbf{C} = \frac{1}{n-1} \mathbf{U}_{\text{centered}} \mathbf{U}_{\text{centered}}^T
\end{equation}

The eigenvectors of $\mathbf{C}$ are the principal components, and the eigenvalues indicate the variance captured by each component. In practice, computing the full covariance matrix is expensive for large $m$. Instead, we compute the SVD of $\mathbf{U}_{\text{centered}}$ directly, which gives us the principal components as the left singular vectors.

For compression, each snapshot can be represented using the first $k$ principal components:
\begin{equation}
\mathbf{u}_i \approx \bar{\mathbf{u}} + \sum_{j=1}^k \alpha_{ij} \boldsymbol{\phi}_j
\end{equation}
where $\boldsymbol{\phi}_j$ are the principal components and $\alpha_{ij}$ are the coefficients. Instead of storing $m$ values per snapshot, we store $k$ coefficients plus the $k$ principal components (shared across all snapshots).

\subsubsection{Proper Orthogonal Decomposition (POD)}

Proper Orthogonal Decomposition is the terminology used in fluid dynamics and mechanical engineering for what is essentially PCA applied to spatial fields~\cite{lumley_pod,sirovich_turbulence}. POD seeks orthogonal spatial modes that optimally represent the energy (variance) in the data.

The POD modes $\{\boldsymbol{\phi}_1, \boldsymbol{\phi}_2, \ldots, \boldsymbol{\phi}_k\}$ are the eigenfunctions of the spatial correlation operator. They satisfy the orthonormality condition:
\begin{equation}
\boldsymbol{\phi}_i^T \boldsymbol{\phi}_j = \delta_{ij}
\end{equation}
where $\delta_{ij}$ is the Kronecker delta.

Each snapshot can be expressed as a linear combination of POD modes:
\begin{equation}
\mathbf{u}(t) = \sum_{i=1}^{k} a_i(t) \boldsymbol{\phi}_i
\end{equation}
where $a_i(t)$ are the temporal coefficients.

The energy captured by the first $k$ modes is:
\begin{equation}
E_k = \frac{\sum_{i=1}^k \sigma_i^2}{\sum_{i=1}^r \sigma_i^2}
\end{equation}

For many physical systems, a small number of modes capture most of the energy. For example, 10-20 modes might capture 99\% of the energy in a flow field, enabling substantial compression.

\subsubsection{Reduced-Order Models (ROM)}

Reduced-Order Models extend dimensionality reduction from data compression to dynamical system modeling. Instead of just compressing snapshots, ROM techniques construct low-dimensional approximations of the governing equations themselves.

Given a high-dimensional dynamical system:
\begin{equation}
\frac{d\mathbf{u}}{dt} = \mathbf{f}(\mathbf{u}, t)
\end{equation}
where $\mathbf{u} \in \mathbb{R}^m$ is the state vector and $\mathbf{f}$ represents the dynamics, ROM seeks a low-dimensional approximation by projecting onto a reduced basis.

Using POD modes as the basis, we approximate:
\begin{equation}
\mathbf{u}(t) \approx \sum_{i=1}^k a_i(t) \boldsymbol{\phi}_i = \mathbf{\Phi}_k \mathbf{a}(t)
\end{equation}

Substituting into the governing equations and projecting onto the reduced basis (Galerkin projection):
\begin{equation}
\frac{d\mathbf{a}}{dt} = \mathbf{\Phi}_k^T \mathbf{f}(\mathbf{\Phi}_k \mathbf{a}, t)
\end{equation}

This yields a reduced system with only $k$ degrees of freedom instead of $m$. For $k \ll m$, the reduced system can be solved much faster than the original, enabling rapid evaluations for parametric studies or real-time simulation.

ROM methods have been successfully applied in many domains including aerodynamics, structural dynamics, and heat transfer. However, they face challenges when the dynamics exhibit strong nonlinearities or when the system operates outside the regime used to construct the basis.

\subsubsection{Limitations for Streaming Data Compression}

While POD, PCA, and ROM are powerful techniques, they have fundamental limitations for the streaming data compression scenario addressed in this thesis:

\textbf{Requirement for Complete Data:} Traditional POD requires access to all snapshots to compute the correlation matrix and perform eigendecomposition. This is incompatible with streaming scenarios where data arrives incrementally. While online or incremental variants of POD exist, they still require storing snapshot matrices and performing expensive matrix updates.

\textbf{Linear Basis Assumption:} POD represents data as linear combinations of basis functions. For fields with strong nonlinearities, sharp gradients, or discontinuities, many modes may be required to achieve acceptable accuracy. Neural networks can learn nonlinear representations that may be more efficient for complex phenomena.

\textbf{Fixed Basis:} The POD basis is computed from a specific dataset. If the data characteristics change significantly (e.g., different flow regimes, parameter variations), the basis may not generalize well, requiring recomputation of modes. This limits adaptability in streaming settings.

\textbf{Query Limitations:} POD provides coefficients only at the snapshot locations and times present in the training data. Interpolating to new spatial locations requires evaluating the basis functions, which may not be available for arbitrary coordinates. Neural networks naturally provide continuous interpolation.

\textbf{Storage Overhead:} While POD coefficients are compact, the basis functions themselves must be stored. For $k$ modes with $m$ spatial points, we must store $k \times m$ values for the basis plus $k$ coefficients per snapshot. For very large spatial domains, this can still be substantial.

Despite these limitations, POD and related techniques provide valuable baselines for comparison. The Results chapter will compare the neural network compression developed in this thesis against POD-based approaches to quantify relative advantages and identify scenarios where each method excels.

\subsection{In-Situ and In-Transit Processing}
\label{sec:insitu_intransit}

In-situ and in-transit processing represent paradigm shifts in how scientific simulations interact with data analysis and visualization workflows. These approaches address the fundamental I/O bottleneck problem by performing data processing concurrently with simulation execution rather than as a separate post-processing step~\cite{paraview_catalyst,damaris}.

\subsubsection{The I/O Bottleneck Problem}

Traditional scientific computing workflows follow a three-stage pipeline: simulation, storage, and analysis. During the simulation stage, the computational code generates field data at regular timesteps. This data is written to persistent storage (disk or parallel file systems). After the simulation completes, a separate analysis or visualization stage reads the stored data from disk for post-processing.

This workflow made sense when computation was slow and storage was fast relative to computation. However, modern high-performance computing has inverted this relationship. Computational capabilities have grown exponentially with massively parallel processors, while I/O bandwidth has not kept pace~\cite{adios2,damaris}. The result is an I/O bottleneck where writing data to disk becomes the dominant cost, often exceeding the time spent on actual computation.

For a simulation generating $S$ bytes of data per timestep with write bandwidth $B$ bytes per second, the time to write one timestep is $t_{\text{write}} = S/B$. If the computation time per timestep is $t_{\text{compute}}$, the simulation is I/O-bound when $t_{\text{write}} > t_{\text{compute}}$. In extreme cases, simulations spend 90\% or more of wall-clock time waiting for I/O operations to complete.

Beyond time costs, storage capacity poses hard limits. A simulation producing terabytes of data per run quickly exhausts available storage. Scientists must choose between temporal resolution (writing fewer snapshots) and spatial resolution (using coarser grids or storing fewer variables), both of which sacrifice scientific value.

\subsubsection{In-Situ Processing}

In-situ processing performs analysis and visualization operations directly within the simulation's memory space, eliminating the need to write raw data to disk~\cite{paraview_catalyst,in_situ_kitware1}. The simulation and analysis codes are tightly coupled, often running as a single executable or through shared memory communication.

The in-situ workflow proceeds as follows:
\begin{enumerate}
    \item The simulation generates a timestep of field data in memory
    \item Instead of writing to disk, the data is passed directly to analysis routines
    \item Analysis operations (compression, feature extraction, visualization rendering) execute on the same compute nodes
    \item Only the analysis results (compressed data, extracted features, rendered images) are written to disk
    \item The simulation continues to the next timestep
\end{enumerate}

Benefits of in-situ processing include:
\begin{itemize}
    \item \textbf{Eliminated I/O bottleneck:} Raw data never touches the file system
    \item \textbf{Reduced storage requirements:} Only analysis products are saved
    \item \textbf{Data locality:} Analysis operates on data already in memory cache
    \item \textbf{Full temporal resolution:} All timesteps can be processed without storage constraints
\end{itemize}

However, in-situ processing has drawbacks. It increases memory consumption on compute nodes, as both simulation and analysis data structures must coexist. It can interfere with computational performance if analysis operations compete for CPU resources. Additionally, tight coupling makes the system less flexible, as changing analysis parameters often requires re-running the entire simulation.

\subsubsection{In-Transit Processing}

In-transit processing offloads analysis to dedicated resources separate from the simulation compute nodes~\cite{damaris,adios2}. Data is transferred from simulation nodes to analysis nodes through high-speed interconnects (e.g., InfiniBand networks) rather than through the file system.

The in-transit workflow consists of:
\begin{enumerate}
    \item The simulation generates data and sends it to a data staging area
    \item The staging area buffers data in memory on dedicated nodes or cores
    \item Analysis tasks read from the staging area and process data concurrently
    \item The simulation continues without waiting for analysis to complete
    \item Processed results are eventually written to persistent storage
\end{enumerate}

In-transit processing provides several advantages over pure in-situ:
\begin{itemize}
    \item \textbf{Asynchronous execution:} Simulation and analysis proceed independently
    \item \textbf{Resource isolation:} Analysis does not compete with simulation for CPU/memory
    \item \textbf{Flexibility:} Multiple analysis tasks can process the same data stream
    \item \textbf{Scalability:} Analysis resources can be scaled independently from simulation
\end{itemize}

The key challenge is managing the data transfer between simulation and analysis. If analysis cannot keep pace with data generation, buffers may overflow, requiring either blocking the simulation or dropping data.

\subsubsection{Data Staging and Memory-Based Processing}

Data staging is the mechanism that enables in-transit processing. Instead of writing to a file system, data is written to a staging area that provides memory-based storage and communication between producers (simulations) and consumers (analysis tasks)~\cite{adios2,damaris}.

Staging systems typically provide:
\begin{itemize}
    \item \textbf{Memory buffers:} Fast temporary storage for data in transit
    \item \textbf{Asynchronous I/O:} Non-blocking write operations from simulations
    \item \textbf{Multiple transport methods:} Shared memory, sockets, RDMA, depending on architecture
    \item \textbf{Metadata management:} Information about data layout, variables, timesteps
    \item \textbf{Flow control:} Backpressure mechanisms when consumers fall behind producers
\end{itemize}

The ADIOS (Adaptive I/O System)~\cite{adios2} framework provides flexible staging capabilities with support for various transport methods. Simulations write to ADIOS using a unified API, and the actual transport (file, memory, network) is configurable at runtime. This enables switching between traditional file-based workflows and in-transit processing without code changes.

Damaris~\cite{damaris} uses dedicated cores on compute nodes for asynchronous I/O. While simulation code runs on some cores, other cores handle data transfer and processing. This hybrid approach combines benefits of in-situ (data locality) and in-transit (asynchronous execution) paradigms.

\subsubsection{Concurrent Execution Models}

In-situ and in-transit processing require careful orchestration of concurrent activities. Several execution models have emerged:

\textbf{Synchronous Coupling:} The simulation blocks at each timestep until analysis completes. This is the simplest model but provides no performance benefit over traditional workflows unless analysis is faster than disk I/O.

\textbf{Asynchronous Coupling:} The simulation continues while analysis proceeds concurrently. This requires buffering and coordination to handle different execution rates. If analysis falls behind, buffers fill up, and the simulation must eventually block or drop data.

\textbf{Time-Multiplexing:} Simulation and analysis alternate execution on the same resources. While simulation computes timestep $n$, analysis processes timestep $n-1$ on different cores. This requires resource partitioning and synchronization.

\textbf{Pipeline Parallelism:} Multiple stages (simulation, compression, analysis, I/O) execute concurrently in a pipeline. Data flows from stage to stage with minimal blocking. This provides maximum throughput but requires careful load balancing to avoid pipeline stalls.

For neural network-based compression in streaming scenarios, the execution model must support training the network concurrently with data generation. This thesis develops an asynchronous coupling approach where compression (network training and inference) proceeds in parallel with simulation, using memory-based staging to transfer data without file system I/O.

\subsubsection{Application to Neural Network Compression}

The in-situ and in-transit paradigms provide an ideal framework for neural network-based compression of streaming scientific data. The compression workflow integrates naturally into these paradigms:

\textbf{In-Situ Compression:} The neural network training and inference execute directly on simulation compute nodes. As each timestep is generated, samples are fed to the network for training. After training completes (or reaches acceptable accuracy), the network parameters are saved rather than the raw data. This eliminates traditional I/O entirely, replacing large raw data writes with small network parameter writes.

\textbf{In-Transit Compression:} Data is staged from simulation nodes to dedicated compression nodes where neural network training occurs. This decouples compression from simulation, allowing each to use resources optimally. The staging area can buffer data while the network trains, and multiple networks can be trained concurrently for different timesteps or spatial regions.

The key technical challenges addressed in this thesis include:
\begin{itemize}
    \item Training networks with limited passes through streaming data
    \item Determining when training has converged sufficiently for compression
    \item Balancing network size (compression ratio) against training time
    \item Managing memory for both data buffers and network training
    \item Providing real-time monitoring of compression quality
\end{itemize}

These challenges are addressed through the methodology presented in Chapter 3, with results demonstrating the feasibility and performance of concurrent neural network compression in Chapter 4.

\subsection{Spatio-Temporal Data in Scientific Computing}
\label{sec:spatiotemporal_data}

Spatio-temporal data represents physical quantities that vary across both space and time. Such data arises naturally from scientific simulations that model evolving physical systems. Understanding the characteristics of spatio-temporal data is essential for designing effective compression strategies.

\subsubsection{Characteristics of Spatio-Temporal Data}

Spatio-temporal data consists of field variables defined over a spatial domain that evolve through time. Mathematically, we can represent such data as functions:
\begin{equation}
\mathbf{u}(\mathbf{x}, t): \Omega \times [0, T] \rightarrow \mathbb{R}^m
\end{equation}
where $\Omega \subset \mathbb{R}^d$ is the spatial domain ($d = 1, 2,$ or $3$ for one, two, or three spatial dimensions), $[0, T]$ is the temporal interval, and $\mathbf{u} \in \mathbb{R}^m$ represents $m$ field variables.

Key characteristics include:

\textbf{Spatial Correlation:} Values at nearby spatial locations tend to be similar. Physical processes like diffusion, convection, and wave propagation create smooth gradients in most regions. This spatial coherence can be exploited for compression.

\textbf{Temporal Correlation:} Values at consecutive timesteps are often highly correlated. Most physical systems evolve continuously, so changes between timesteps are typically small relative to the absolute values. This temporal smoothness provides opportunities for compression across the time dimension.

\textbf{Multi-Scale Structure:} Physical phenomena often exhibit structure at multiple spatial and temporal scales. Large-scale patterns coexist with fine-scale details. Effective compression must capture both scales while recognizing that coarse-scale features typically contain more energy.

\textbf{Boundary Layers and Sharp Features:} While most regions may be smooth, spatio-temporal data often contains localized regions with sharp gradients, discontinuities, or thin boundary layers. These features are scientifically important and must be preserved during compression.

\textbf{High Dimensionality:} Scientific simulations commonly involve multiple coupled field variables. Each variable adds another dimension to the data space. Compression methods must handle this high-dimensional structure efficiently.

\subsubsection{Discretization of Continuous Fields}

Physical systems are governed by continuous partial differential equations (PDEs). Numerical simulations discretize these continuous equations onto finite grids or meshes. The discretization process determines the structure of the resulting data.

\textbf{Structured Grids:} Many simulations use regular Cartesian grids where spatial points are arranged in a uniform rectangular lattice. For a 3D grid with $n_x \times n_y \times n_z$ points and $m$ field variables, a single timestep contains $n_x \cdot n_y \cdot n_z \cdot m$ values. The regular structure simplifies data access and indexing but may be inefficient for domains with complex geometries.

\textbf{Unstructured Meshes:} Complex geometries often require unstructured meshes with arbitrary connectivity between points. While more flexible, unstructured meshes require storing both field values and mesh topology (connectivity information), increasing data size and complexity.

\textbf{Adaptive Meshes:} Some simulations use adaptive mesh refinement where grid resolution varies spatially and temporally based on solution features. Regions with sharp gradients receive fine resolution while smooth regions use coarse grids. This improves computational efficiency but creates variable data sizes across timesteps.

\textbf{Particle-Based Representations:} Some simulation methods (e.g., molecular dynamics, smoothed particle hydrodynamics) represent the system as collections of particles rather than fixed grids. Particle positions and properties change over time, creating irregular data patterns.

For this thesis, we focus on structured grid data, which represents the majority of large-scale scientific simulations. However, the neural network compression approach can be extended to other discretization schemes.

\subsubsection{Spatial and Temporal Resolution}

The resolution of spatio-temporal data determines both its scientific value and data management challenges.

\textbf{Spatial Resolution:} Finer spatial grids resolve smaller features but generate larger datasets. A doubling of resolution in each dimension increases data size by a factor of $2^d$ (where $d$ is the number of spatial dimensions). For 3D simulations, this means 8× more data. High-resolution simulations with billions of grid points per timestep are increasingly common.

\textbf{Temporal Resolution:} The timestep size determines how frequently the system state is recorded. Smaller timesteps capture faster dynamics but generate more data. For simulations with thousands to millions of timesteps, temporal resolution contributes significantly to total data volume.

\textbf{Resolution Trade-offs:} Scientists must balance resolution against computational cost and storage requirements. Insufficient resolution may miss important physics. Excessive resolution wastes resources. Compression enables higher resolution by reducing storage burden.

\subsubsection{Data from Diverse Scientific Domains}

Spatio-temporal data arises across many scientific and engineering disciplines:

\textbf{Fluid Dynamics:} Simulations of air flow around aircraft, ocean currents, weather patterns, and combustion generate velocity, pressure, temperature, and species concentration fields. These fields often exhibit turbulent structures with wide-ranging spatial scales~\cite{brunton_CFD_ML_development}.

\textbf{Climate Modeling:} Global climate models track atmospheric and oceanic variables over years to centuries. They generate massive datasets with temperature, pressure, humidity, wind, and precipitation fields across the entire Earth.

\textbf{Molecular Dynamics:} Simulations of molecular systems track positions, velocities, and forces for thousands to millions of atoms. While typically represented as particle data, coarse-grained fields (density, temperature) can be extracted.

\textbf{Structural Mechanics:} Finite element analysis of structures under load produces stress, strain, and displacement fields. Time-dependent simulations track how structures respond to dynamic loading.

\textbf{Electromagnetic Simulations:} Computational electromagnetics solves Maxwell's equations to compute electric and magnetic fields. Applications include antenna design, electromagnetic compatibility, and photonics.

\textbf{Astrophysics:} Cosmological simulations model galaxy formation, star evolution, and gravitational dynamics over cosmic timescales, generating density, temperature, and velocity fields.

While these domains involve different physics, the resulting spatio-temporal data shares common mathematical structure. Compression techniques that exploit spatial and temporal correlations, multi-scale structure, and smoothness can apply across domains~\cite{hey_fourth_2009}.

\subsubsection{Data Structures and Formats}

Scientific simulation data is typically stored in specialized file formats designed for performance and portability.

\textbf{HDF5 (Hierarchical Data Format 5):} A widely-used format that supports large, complex datasets with hierarchical organization~\cite{hdf5_format}. HDF5 provides compression, chunking, and parallel I/O capabilities.

\textbf{NetCDF (Network Common Data Form):} Common in climate and atmospheric sciences~\cite{netcdf_format}. Provides self-describing data with metadata about variables, dimensions, and coordinates.

\textbf{VTK (Visualization Toolkit Format):} Used for visualization and analysis~\cite{vtk_book}. Supports both structured and unstructured data.

These formats organize data as multi-dimensional arrays with associated metadata. A typical structure includes:
\begin{itemize}
    \item Spatial coordinate arrays defining the grid
    \item Time values for each snapshot
    \item Multi-dimensional arrays for each field variable
    \item Metadata describing units, variable names, simulation parameters
\end{itemize}

For compression, the raw field arrays are the primary target. A dataset with $N$ spatial points, $T$ timesteps, and $m$ variables contains $N \times T \times m$ floating-point values, typically requiring $4N \times T \times m$ bytes (for single precision) or $8N \times T \times m$ bytes (for double precision).

\subsubsection{Challenges for Data Management}

The characteristics of spatio-temporal scientific data create several challenges:

\textbf{Volume:} Modern simulations generate terabytes to petabytes of data, overwhelming storage and network infrastructure.

\textbf{Velocity:} Data is generated continuously during simulations. Real-time processing must keep pace with generation rates.

\textbf{Variety:} Different simulation codes use different data formats, mesh types, and variable naming conventions. Compression tools must handle this diversity.

\textbf{Veracity:} Compression must preserve scientific accuracy. Unlike image or video compression where some distortion is acceptable, scientific data compression must maintain validity for quantitative analysis.

\textbf{Value:} The scientific value is often concentrated in specific features or regions. Compression should preserve high-value information preferentially.

Neural network-based compression addresses these challenges by learning compact representations tailored to specific datasets while maintaining accuracy. The continuous function representation provided by implicit neural networks naturally handles irregular sampling, multi-scale structure, and interpolation needs. By integrating with in-situ and in-transit frameworks, neural network compression can operate on streaming data in real-time, addressing both volume and velocity challenges~\cite{in_situ_framework_CFD_ML,physics_drive_CFD_Compression}.

This chapter has established the theoretical foundations underlying the neural network-based compression framework. The next chapter presents the methodology, describing how these theoretical concepts are implemented and integrated into a practical system for compressing streaming spatio-temporal scientific data.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Chapter 3  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\thispagestyle{empty}
\thispagestyle{plain}

\section{Methodology}
\label{methodology}

This chapter describes the methodology developed for learning continuous representations of spatio-temporal flow field data using neural networks. The approach implements an offline regression model that learns to map spatial coordinates and time to flow field variables. This chapter presents the dataset characteristics, neural network architecture, training procedure, and evaluation metrics used to assess the model performance.

\subsection{Problem Formulation}
\label{sec:problem_formulation}

The objective is to learn a continuous function $f: \mathbb{R}^4 \rightarrow \mathbb{R}^4$ that maps spatio-temporal coordinates to flow field variables. Given a set of discrete samples from a flow simulation, the neural network learns to approximate this mapping.

\subsubsection{Mathematical Formulation}

The input space consists of spatial coordinates $(x, y, z)$ and temporal coordinate $t$:
\begin{equation}
\mathbf{x}_{\text{input}} = [x, y, z, t]^T \in \mathbb{R}^4
\end{equation}

The output space contains four flow field variables: x-velocity ($V_x$), y-velocity ($V_y$), pressure ($P$), and turbulent kinetic energy (TKE):
\begin{equation}
\mathbf{y}_{\text{output}} = [V_x, V_y, P, \text{TKE}]^T \in \mathbb{R}^4
\end{equation}

The neural network learns the mapping:
\begin{equation}
\hat{\mathbf{y}} = f_\theta(\mathbf{x}_{\text{input}})
\end{equation}
where $\theta$ represents the network parameters (weights and biases).

\subsubsection{Training Objective}

The network is trained to minimize the mean squared error between predicted and actual field values across all training samples:
\begin{equation}
\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} \| f_\theta(\mathbf{x}_i) - \mathbf{y}_i \|^2
\end{equation}
where $N$ is the number of training samples.

\subsection{Dataset Description}
\label{sec:dataset_description}

The dataset used in this work consists of spatio-temporal flow field data from a computational fluid dynamics (CFD) simulation. The data captures the evolution of flow variables over a 3D spatial domain across time.

\subsubsection{Dataset Characteristics}

The dataset contains 7,919,100 samples stored in CSV format. Each sample represents measurements at a specific spatial location and time instant. The dataset structure is:

\begin{itemize}
    \item \textbf{Total Samples:} 7,919,100 data points
    \item \textbf{Input Features (4):} Spatial coordinates $(x, y, z)$ and temporal coordinate $t$
    \item \textbf{Output Variables (4):} $V_x$ (x-velocity), $V_y$ (y-velocity), $P$ (pressure), TKE (turbulent kinetic energy)
    \item \textbf{Data Format:} CSV file with 8 columns (no header): $x, y, z, t, V_x, V_y, P, \text{TKE}$
    \item \textbf{File Size:} The raw dataset represents the complete spatio-temporal evolution of the flow field
\end{itemize}

The dataset exhibits typical characteristics of CFD simulation data: smooth spatial variations in most regions with localized gradients, temporal correlation between consecutive time steps, and coupled relationships between different flow variables.

\subsubsection{Data Preprocessing}

Several preprocessing steps are applied to prepare the data for neural network training:

\textbf{Data Loading:} The CSV file is read using PyArrow for efficient parsing of large files. The data is converted to NumPy arrays and then to PyTorch tensors for training.

\textbf{Normalization:} Both inputs and outputs are normalized using standardization (z-score normalization):
\begin{align}
\mathbf{x}_{\text{norm}} &= \frac{\mathbf{x} - \mu_x}{\sigma_x} \\
\mathbf{y}_{\text{norm}} &= \frac{\mathbf{y} - \mu_y}{\sigma_y}
\end{align}
where $\mu$ and $\sigma$ represent mean and standard deviation computed from the training data. Normalization ensures that all features have zero mean and unit variance, which stabilizes neural network training and improves convergence. The normalization parameters are saved for denormalization during inference.

\textbf{Train-Validation Split:} The dataset is randomly split into training (80\%) and validation (20\%) subsets:
\begin{itemize}
    \item Training samples: 6,335,280 (80\%)
    \item Validation samples: 1,583,820 (20\%)
\end{itemize}

The training set is used to optimize network parameters, while the validation set provides an independent evaluation of model performance and helps detect overfitting.

\textbf{Batch Processing:} Data is organized into mini-batches of size 256 for stochastic gradient descent. Mini-batch processing provides a balance between computational efficiency and gradient estimation accuracy.

\subsection{Neural Network Architecture}
\label{sec:network_architecture}

A multilayer perceptron (MLP) with ReLU activations is employed to learn the coordinate-to-field mapping. The network architecture is designed to balance representational capacity with computational efficiency.

\subsubsection{Network Structure}

The network is a fully-connected feedforward neural network consisting of:

\textbf{Input Layer:} Receives 4-dimensional coordinate vector $\mathbf{x} = [x, y, z, t]^T$ representing spatial position and time.

\textbf{Hidden Layers:} Three hidden layers with the following configuration:
\begin{itemize}
    \item First hidden layer: 64 neurons with ReLU activation
    \item Second hidden layer: 64 neurons with ReLU activation
    \item Third hidden layer: 32 neurons with ReLU activation
\end{itemize}

\textbf{Output Layer:} Linear layer producing 4 outputs $\mathbf{y} = [V_x, V_y, P, \text{TKE}]^T$ without activation function.

Mathematically, the network computes:
\begin{align}
\mathbf{h}_1 &= \text{ReLU}(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1) \\
\mathbf{h}_2 &= \text{ReLU}(\mathbf{W}_2 \mathbf{h}_1 + \mathbf{b}_2) \\
\mathbf{h}_3 &= \text{ReLU}(\mathbf{W}_3 \mathbf{h}_2 + \mathbf{b}_3) \\
\mathbf{y} &= \mathbf{W}_4 \mathbf{h}_3 + \mathbf{b}_4
\end{align}
where $\mathbf{W}_i$ and $\mathbf{b}_i$ are weight matrices and bias vectors for each layer, and ReLU is the rectified linear unit activation function~\cite{relu_nair}:
\begin{equation}
\text{ReLU}(z) = \max(0, z)
\end{equation}

\subsubsection{Network Parameters}

The architecture contains the following layer dimensions:
\begin{itemize}
    \item Layer 1: $\mathbf{W}_1 \in \mathbb{R}^{64 \times 4}$, $\mathbf{b}_1 \in \mathbb{R}^{64}$ → 320 parameters
    \item Layer 2: $\mathbf{W}_2 \in \mathbb{R}^{64 \times 64}$, $\mathbf{b}_2 \in \mathbb{R}^{64}$ → 4,160 parameters
    \item Layer 3: $\mathbf{W}_3 \in \mathbb{R}^{32 \times 64}$, $\mathbf{b}_3 \in \mathbb{R}^{32}$ → 2,080 parameters
    \item Layer 4: $\mathbf{W}_4 \in \mathbb{R}^{4 \times 32}$, $\mathbf{b}_4 \in \mathbb{R}^{4}$ → 132 parameters
\end{itemize}

Total number of trainable parameters: 6,692 parameters.

This relatively compact architecture is chosen to demonstrate the feasibility of learning continuous representations while maintaining a small parameter count suitable for compression applications.

\subsubsection{Network Initialization}

Weights are initialized using PyTorch's default initialization for linear layers (Kaiming uniform initialization~\cite{kaiming_init} for layers followed by ReLU activations). Biases are initialized to zero. This initialization strategy ensures that gradients flow properly during early training stages.

\subsection{Training Methodology - Offline Mode}
\label{sec:training_methodology_offline}

This section describes the offline training approach where the complete dataset is available and multiple passes (epochs) through the data are performed. The network is trained using standard supervised learning with mini-batch gradient descent. This offline mode serves as the baseline approach, establishing the best achievable reconstruction quality when all data is accessible for iterative training.

\subsubsection{Loss Function}

The network minimizes the mean squared error (MSE) between predicted and actual flow field values:
\begin{equation}
\mathcal{L}_{\text{MSE}} = \frac{1}{B} \sum_{i=1}^{B} \| \mathbf{y}_i - \hat{\mathbf{y}}_i \|^2
\end{equation}
where $B$ is the batch size, $\mathbf{y}_i$ are the true field values, and $\hat{\mathbf{y}}_i = f_\theta(\mathbf{x}_i)$ are the network predictions.

Expanding for the four output variables:
\begin{equation}
\mathcal{L}_{\text{MSE}} = \frac{1}{B} \sum_{i=1}^{B} \left[ (V_x^i - \hat{V}_x^i)^2 + (V_y^i - \hat{V}_y^i)^2 + (P^i - \hat{P}^i)^2 + (\text{TKE}^i - \widehat{\text{TKE}}^i)^2 \right]
\end{equation}

MSE is chosen as the loss function because it penalizes large errors heavily and provides smooth gradients for optimization.

\subsubsection{Optimization Algorithm}

The Adam optimizer~\cite{adam_optimizer} is employed with the following hyperparameters:

\begin{itemize}
    \item \textbf{Learning rate:} $\eta = 0.001$ (fixed throughout training)
    \item \textbf{Beta parameters:} $\beta_1 = 0.9$, $\beta_2 = 0.999$ (Adam default values)
    \item \textbf{Epsilon:} $\epsilon = 10^{-8}$ for numerical stability
\end{itemize}

Adam is selected for its adaptive learning rate properties and robustness to hyperparameter settings. It combines the benefits of momentum-based methods and adaptive learning rates, making it well-suited for this regression task.

\subsubsection{Training Configuration}

The training procedure uses the following configuration:

\begin{itemize}
    \item \textbf{Number of epochs:} 150 complete passes through the training data
    \item \textbf{Batch size:} 512 samples per mini-batch
    \item \textbf{Training samples per epoch:} $\lceil 7,919,100 / 512 \rceil = 15,467$ batches
    \item \textbf{Data usage:} 100\% of dataset used for training (no validation split)
\end{itemize}

\subsubsection{Training Procedure}

For each epoch, the training procedure follows these steps:

\textbf{Training Phase:}
\begin{enumerate}
    \item Shuffle training data to ensure random sampling across epochs
    \item For each mini-batch of 256 samples:
    \begin{enumerate}
        \item Load batch coordinates $\{\mathbf{x}_i\}$ and field values $\{\mathbf{y}_i\}$
        \item Forward propagation: compute predictions $\hat{\mathbf{y}}_i = f_\theta(\mathbf{x}_i)$
        \item Compute MSE loss
        \item Backpropagation: compute gradients $\nabla_\theta \mathcal{L}$
        \item Update parameters: $\theta \leftarrow \theta - \eta \cdot \text{Adam}(\nabla_\theta \mathcal{L})$
    \end{enumerate}
    \item Compute average training loss across all batches
\end{enumerate}

\textbf{Validation Phase:}
\begin{enumerate}
    \item Set model to evaluation mode (disable dropout if present)
    \item For each validation batch:
    \begin{enumerate}
        \item Forward propagation without gradient computation
        \item Compute predictions and accumulate loss
    \end{enumerate}
    \item Compute average validation loss
    \item If validation loss improves, save model checkpoint
\end{enumerate}

Training continues for all 150 epochs. The model weights from the final epoch are saved, as all data is used for training without a separate validation split in the final configuration.

\subsection{Training Methodology - Online Mode}
\label{sec:training_methodology_online}

This section describes the online training approach designed to simulate real-time, in-situ compression scenarios where data arrives sequentially as a stream rather than being available all at once. The online mode processes data through temporal windows, enabling incremental learning that mirrors how compression would operate during an actual running simulation.

\subsubsection{Motivation for Online Training}

While offline training achieves optimal reconstruction quality by iterating over the complete dataset multiple times, it is not applicable to real-world in-situ compression scenarios where:

\begin{itemize}
    \item Data is generated continuously by running simulations and cannot be stored entirely before processing
    \item Compression must occur concurrently with simulation execution to avoid \acrshort{I/O} bottlenecks
    \item Memory constraints prevent loading the entire temporal history simultaneously
    \item Real-time feedback on compression quality is required during simulation runtime
\end{itemize}

The online training mode addresses these requirements by processing data incrementally as it becomes available, maintaining a sliding window over recent timesteps. This approach trades some reconstruction accuracy for the ability to operate in streaming scenarios, making it suitable for in-situ and in-transit compression frameworks~\cite{paraview_catalyst,damaris,adios2}.

\subsubsection{Temporal Window Mechanism}

The core concept of online training is the temporal window, which defines a subset of consecutive timesteps that the network learns from at any given time. Rather than training on all 300 timesteps simultaneously, the online approach processes a moving window of $W$ timesteps.

\textbf{Window Definition:} A temporal window $\mathcal{W}_k$ at step $k$ contains data from timesteps $[t_k, t_k + W - 1]$:
\begin{equation}
\mathcal{W}_k = \{(\mathbf{x}_i, t_i, \mathbf{u}_i) : t_k \leq t_i < t_k + W\}
\end{equation}
where $W$ is the window size (number of timesteps) and $t_k$ is the starting timestep of window $k$.

\textbf{Sliding Window Progression:} As new timesteps become available, the window advances by a stride $S$:
\begin{equation}
t_{k+1} = t_k + S
\end{equation}

For overlapping windows ($S < W$), some timesteps appear in multiple consecutive windows, providing continuity in learning. For non-overlapping windows ($S = W$), each timestep is processed exactly once.

\subsubsection{Incremental Learning Approach}

Unlike offline training where the network is initialized randomly and trained from scratch on all data, online training employs incremental learning where the network retains knowledge from previous windows:

\begin{enumerate}
    \item \textbf{Initial Window:} The network is initialized with random weights and trained on the first window $\mathcal{W}_0$
    \item \textbf{Subsequent Windows:} For each new window $\mathcal{W}_k$ ($k > 0$), training continues from the weights learned in the previous window, preserving learned representations
    \item \textbf{Continuous Adaptation:} The network continuously adapts to new data while retaining the ability to represent previously seen patterns
\end{enumerate}

This incremental approach offers several advantages:
\begin{itemize}
    \item \textbf{Faster convergence:} Starting from pre-trained weights rather than random initialization accelerates learning on new windows
    \item \textbf{Knowledge retention:} The network maintains representations of earlier timesteps even as it adapts to new data
    \item \textbf{Temporal coherence:} Overlapping windows ensure smooth transitions in learned representations across time
\end{itemize}

\subsubsection{Window Configuration}

The online training implementation uses the following window configuration:

\begin{itemize}
    \item \textbf{Window size:} $W = 3$ timesteps per window
    \item \textbf{Stride:} $S = 1$ timestep (overlapping windows)
    \item \textbf{Epochs per window:} 10 training epochs for each window
    \item \textbf{Total windows:} 20 windows processed sequentially
    \item \textbf{Batch size:} 512 samples per mini-batch (same as offline)
\end{itemize}

The choice of $W = 3$ with stride $S = 1$ creates significant overlap between consecutive windows. Each timestep (except the first two and last two) appears in three different windows, providing multiple opportunities for the network to learn its representation. This configuration balances:
\begin{itemize}
    \item \textbf{Memory efficiency:} Only 3 timesteps loaded at once ($\approx$ 79,191 samples per window)
    \item \textbf{Learning continuity:} 66\% overlap between consecutive windows
    \item \textbf{Adaptation speed:} 10 epochs per window allows sufficient learning while maintaining overall training speed
\end{itemize}

\subsubsection{Normalization Strategy}

Normalization in online training requires special consideration because global statistics (mean, standard deviation) are not available when processing streaming data. Two strategies are possible:

\textbf{Window-Local Normalization:} Compute normalization parameters from each window independently:
\begin{align}
\mu_k &= \frac{1}{|\mathcal{W}_k|} \sum_{i \in \mathcal{W}_k} \mathbf{x}_i \\
\sigma_k &= \sqrt{\frac{1}{|\mathcal{W}_k|} \sum_{i \in \mathcal{W}_k} (\mathbf{x}_i - \mu_k)^2}
\end{align}

This approach is fully online but may cause inconsistencies between windows if data distributions vary significantly across timesteps.

\textbf{Running Statistics:} Maintain exponentially weighted moving averages of normalization parameters:
\begin{align}
\mu_{k} &= \alpha \cdot \mu_{k-1} + (1 - \alpha) \cdot \bar{\mathbf{x}}_k \\
\sigma_{k} &= \alpha \cdot \sigma_{k-1} + (1 - \alpha) \cdot s_k
\end{align}
where $\alpha$ is a momentum parameter (typically 0.9-0.99), $\bar{\mathbf{x}}_k$ is the mean of the current window, and $s_k$ is its standard deviation.

The current implementation uses \textbf{window-local normalization} for simplicity, as the \acrshort{CFD} dataset exhibits relatively stable distributions across timesteps. Each window's data is normalized independently using z-score standardization before training.

\subsubsection{Training Procedure per Window}

For each temporal window $\mathcal{W}_k$, the training procedure follows these steps:

\textbf{Data Loading:}
\begin{enumerate}
    \item Load data for timesteps $[t_k, t_k + W - 1]$ from storage
    \item Compute window-local normalization parameters $(\mu_k, \sigma_k)$
    \item Apply normalization to inputs and outputs
    \item Create mini-batches of size 512
\end{enumerate}

\textbf{Training Loop (10 epochs per window):}
\begin{enumerate}
    \item Shuffle data within the current window
    \item For each mini-batch:
    \begin{enumerate}
        \item Forward propagation: $\hat{\mathbf{y}} = f_\theta(\mathbf{x})$
        \item Compute \acrshort{MSE} loss
        \item Backpropagation and parameter update using \acrshort{Adam}
    \end{enumerate}
    \item Record training metrics (\acrshort{MSE}, \acrshort{PSNR}, \acrshort{SSIM})
\end{enumerate}

\textbf{Window Completion:}
\begin{enumerate}
    \item Save model checkpoint for current window
    \item Log window-level metrics
    \item Advance to next window: $k \leftarrow k + 1$
\end{enumerate}

Algorithm~\ref{alg:online_training} summarizes the complete online training procedure.

\begin{figure}[htbp]
\centering
\fbox{\parbox{0.9\textwidth}{
\textbf{Algorithm: Online Training with Temporal Windows}
\begin{enumerate}
    \item Initialize network weights $\theta_0$ randomly
    \item \textbf{for} $k = 0$ \textbf{to} $N_{\text{windows}} - 1$ \textbf{do}
    \begin{enumerate}
        \item Load window data $\mathcal{W}_k$ for timesteps $[t_k, t_k + W - 1]$
        \item Compute normalization parameters $(\mu_k, \sigma_k)$
        \item Normalize data: $\tilde{\mathcal{W}}_k = \text{normalize}(\mathcal{W}_k, \mu_k, \sigma_k)$
        \item \textbf{for} epoch $= 1$ \textbf{to} $E_{\text{window}}$ \textbf{do}
        \begin{enumerate}
            \item Shuffle $\tilde{\mathcal{W}}_k$ and create mini-batches
            \item \textbf{for each} mini-batch $\mathcal{B}$ \textbf{do}
            \begin{enumerate}
                \item Compute predictions: $\hat{\mathbf{y}} = f_{\theta_k}(\mathbf{x})$
                \item Compute loss: $\mathcal{L} = \text{MSE}(\mathbf{y}, \hat{\mathbf{y}})$
                \item Update weights: $\theta_k \leftarrow \text{Adam}(\theta_k, \nabla_\theta \mathcal{L})$
            \end{enumerate}
        \end{enumerate}
        \item Save checkpoint and metrics for window $k$
        \item Set $\theta_{k+1} \leftarrow \theta_k$ (carry forward weights)
    \end{enumerate}
    \item \textbf{return} final model $\theta_{N_{\text{windows}}}$
\end{enumerate}
}}
\caption{Online training algorithm with temporal windows for streaming spatio-temporal data compression.}
\label{alg:online_training}
\end{figure}

\subsubsection{Comparison with Offline Training}

Table~\ref{tab:offline_vs_online_methodology} summarizes the key methodological differences between offline and online training modes.

\begin{table}[htbp]
\centering
\caption{Comparison of offline and online training methodologies}
\label{tab:offline_vs_online_methodology}
\begin{tabular}{lcc}
\hline
\textbf{Aspect} & \textbf{Offline Mode} & \textbf{Online Mode} \\
\hline
Data access & Complete dataset & Temporal windows \\
Training passes & 150 epochs (all data) & 10 epochs per window \\
Normalization & Global statistics & Window-local statistics \\
Weight initialization & Random & Carried from previous window \\
Memory requirement & Full dataset in memory & Window size only \\
Applicable scenario & Post-processing & Real-time / in-situ \\
\hline
\end{tabular}
\end{table}

The online training methodology enables neural network-based compression to operate in streaming scenarios where traditional offline approaches are not feasible. While some reconstruction quality is sacrificed compared to offline training, the ability to process data incrementally makes this approach suitable for integration with in-situ and in-transit processing frameworks. Detailed performance comparisons between offline and online modes are presented in Chapter~\ref{results}.

\subsection{Evaluation Metrics}
\label{sec:evaluation_metrics}

Model performance is quantified using three complementary metrics that measure prediction accuracy from different perspectives.

\subsubsection{Mean Squared Error (MSE)}

MSE measures the average squared difference between predicted and true values:
\begin{equation}
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} \| \mathbf{y}_i - \hat{\mathbf{y}}_i \|^2
\end{equation}
where $N$ is the number of samples in the evaluation set.

For individual field variables, MSE is computed separately:
\begin{equation}
\text{MSE}_k = \frac{1}{N} \sum_{i=1}^{N} (y_i^{(k)} - \hat{y}_i^{(k)})^2 \quad \text{for } k \in \{V_x, V_y, P, \text{TKE}\}
\end{equation}

MSE directly corresponds to the training objective and provides an absolute measure of error magnitude. Lower MSE indicates better model performance.

\subsubsection{Peak Signal-to-Noise Ratio (PSNR)}

PSNR quantifies reconstruction quality in logarithmic scale (decibels):
\begin{equation}
\text{PSNR} = 10 \log_{10} \left( \frac{\text{MAX}^2}{\text{MSE}} \right) \text{ dB}
\end{equation}
where MAX is the maximum possible value in the normalized data range (typically 1.0 after standardization).

PSNR provides an intuitive measure of signal quality. Higher PSNR values indicate better reconstruction with less noise. PSNR values above 30 dB generally indicate high quality reconstruction, while values above 40 dB represent excellent quality.

\subsubsection{Structural Similarity Index (SSIM)}

SSIM~\cite{ssim} measures the perceptual similarity between predicted and true values by comparing luminance, contrast, and structure:
\begin{equation}
\text{SSIM}(y, \hat{y}) = \frac{(2\mu_y\mu_{\hat{y}} + C_1)(2\sigma_{y\hat{y}} + C_2)}{(\mu_y^2 + \mu_{\hat{y}}^2 + C_1)(\sigma_y^2 + \sigma_{\hat{y}}^2 + C_2)}
\end{equation}
where:
\begin{itemize}
    \item $\mu_y$, $\mu_{\hat{y}}$ are the means of true and predicted values
    \item $\sigma_y$, $\sigma_{\hat{y}}$ are the standard deviations
    \item $\sigma_{y\hat{y}}$ is the covariance
    \item $C_1 = (0.01)^2$, $C_2 = (0.03)^2$ are constants for numerical stability
\end{itemize}

SSIM ranges from -1 to 1, with 1 indicating perfect structural similarity. Unlike MSE and PSNR, SSIM captures structural patterns in the data, making it complementary to point-wise error metrics.

\subsubsection{Evaluation Protocol}

For each epoch during training, the following metrics are computed:

\textbf{Training Metrics:} MSE, PSNR, and SSIM are computed on the full training set after each epoch to monitor training progress and detect overfitting.

\textbf{Validation Metrics:} The same metrics are computed on the validation set to evaluate generalization performance on unseen data.

All metrics are tracked throughout the 100 training epochs and visualized to assess convergence behavior and final model quality.

\subsection{Implementation Details}
\label{sec:implementation}

The offline regression model is implemented in Python using modern deep learning libraries and scientific computing tools.

\subsubsection{Software Stack}

The implementation uses the following software components:

\textbf{PyTorch:} Neural network implementation leverages PyTorch~\cite{pytorch} for automatic differentiation, GPU acceleration, and optimized tensor operations. PyTorch version 1.x or higher is used.

\textbf{PyArrow:} Large CSV file reading is handled by PyArrow, which provides efficient columnar data loading. This is particularly important given the dataset size of nearly 8 million samples.

\textbf{NumPy:} Numerical operations and data preprocessing use NumPy for efficient array manipulations.

\textbf{Scikit-learn:} The mean\_squared\_error function from scikit-learn is used for computing evaluation metrics.

\textbf{Matplotlib:} Training progress visualization and metric plotting use Matplotlib for generating publication-quality figures.

\subsubsection{Hardware Configuration}

Training is performed on computational hardware with the following characteristics:

\begin{itemize}
    \item \textbf{Device:} The code automatically detects and utilizes available accelerators (CUDA GPU, Apple Metal Performance Shaders, or CPU)
    \item \textbf{Memory:} Sufficient RAM to hold mini-batches of 256 samples and model parameters
    \item \textbf{Storage:} SSD storage for fast CSV file access during data loading
\end{itemize}

GPU acceleration (when available) significantly reduces training time compared to CPU-only execution, particularly for the forward and backward propagation steps.

\subsubsection{Code Organization}

The implementation consists of three main components:

\textbf{FlowDataset Class:} Custom PyTorch Dataset that:
\begin{itemize}
    \item Loads CSV data using PyArrow
    \item Separates inputs (coordinates) and targets (field values)
    \item Computes and applies normalization parameters
    \item Provides standardized data access through \_\_getitem\_\_ method
\end{itemize}

\textbf{RegressionModel Class:} PyTorch nn.Module that:
\begin{itemize}
    \item Defines the 4-layer MLP architecture
    \item Implements forward propagation
    \item Manages model parameters
\end{itemize}

\textbf{Training Loop:} Main training function that:
\begin{itemize}
    \item Iterates through epochs
    \item Performs forward propagation, loss computation, and backpropagation
    \item Computes training and validation metrics
    \item Tracks and stores performance metrics
    \item Saves model checkpoints
\end{itemize}

\subsubsection{Model Persistence}

After training, the following artifacts are saved:

\begin{itemize}
    \item \textbf{Model weights:} Trained network parameters stored in PyTorch .pth format
    \item \textbf{Training plots:} Epoch-by-epoch visualization of loss, PSNR, and SSIM metrics
    \item \textbf{Normalization parameters:} Mean and standard deviation values stored within the dataset object for consistent inference
\end{itemize}

This implementation provides a complete pipeline for learning continuous representations of spatio-temporal flow data. The next chapter presents the experimental results obtained using this methodology.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Chapter 4  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Results}
\label{results}

This chapter presents the experimental results obtained from training the neural network compression system on the spatio-temporal flow field dataset. The results are organized into three main sections: offline training results demonstrating baseline compression performance, detailed compression analysis quantifying data reduction effectiveness, and qualitative visualizations assessing reconstruction quality. The analysis demonstrates the model's capability to achieve extreme compression ratios while maintaining acceptable reconstruction accuracy for scientific applications.

\subsection{Offline Training Results}
\label{sec:offline_training_results}

This section presents the results from the offline training approach where the complete dataset was used for training over multiple epochs.

\subsubsection{Training Configuration}

The offline training was conducted with the following configuration:

\begin{table}[htbp]
\centering
\caption{Offline training configuration}
\label{tab:offline_training_config}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Total samples & 7,919,100 \\
Number of epochs & 150 \\
Batch size & 512 \\
Optimizer & \acrshort{Adam} \\
Learning rate & 0.001 \\
Data usage & 100\% (no validation split) \\
Hardware & NVIDIA A100 \acrshort{GPU} \\
\hline
\end{tabular}
\end{table}

The decision to use 100\% of the data for training (without a separate validation split) was made to maximize the information available for learning the continuous function representation. Since the goal is compression rather than generalization to unseen data distributions, using all available data improves the model's ability to accurately represent the complete spatio-temporal field.

\subsubsection{Training Convergence Analysis}

The model was trained for 150 epochs on the complete 7.9 million sample dataset. Figure~\ref{fig:training_progress_full} shows the evolution of four key metrics (\acrshort{MSE} loss, \acrshort{PSNR}, \acrshort{SSIM}, and \acrshort{MAE}) throughout the training process.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{../../../results/training_progress_full.png}
    \caption{Training progress over 150 epochs showing (a) \acrshort{MSE} loss decrease, (b) \acrshort{PSNR} increase, (c) \acrshort{SSIM} convergence, and (d) \acrshort{MAE} decrease. All metrics demonstrate consistent improvement with convergence achieved by epoch 150.}
    \label{fig:training_progress_full}
\end{figure}

\textbf{Loss Convergence:}

The \acrshort{MSE} loss exhibits a characteristic learning curve with three distinct phases:

\begin{itemize}
    \item \textbf{Rapid Initial Descent (Epochs 1-30):} The training loss drops sharply from approximately 0.16 to 0.04 during the first 30 epochs, indicating that the network quickly learns the dominant patterns in the spatio-temporal data.

    \item \textbf{Steady Improvement (Epochs 30-100):} The loss continues to decrease gradually, reaching approximately 0.025 by epoch 100. This phase represents the network refining its representations and learning finer spatial and temporal details.

    \item \textbf{Final Convergence (Epochs 100-150):} The loss curve flattens, settling at a final value of \textbf{0.021266}. The continued slight improvement during this phase justifies the extended training to 150 epochs.
\end{itemize}

\textbf{\acrshort{PSNR} Evolution:}

The \acrshort{PSNR} metric provides a logarithmic view of reconstruction quality:

\begin{itemize}
    \item \textbf{Early Training (Epochs 1-30):} \acrshort{PSNR} increases rapidly from approximately 8~dB to 14~dB, corresponding to the sharp decrease in \acrshort{MSE} loss.

    \item \textbf{Continued Improvement (Epochs 30-150):} \acrshort{PSNR} continues to increase steadily, reaching a final value of \textbf{16.80~dB} by epoch 150.
\end{itemize}

While 16.80~dB is lower than the 30+~dB typically considered high quality for image reconstruction~\cite{ssim}, this is reasonable given the complexity of simultaneously predicting four coupled flow variables from a compact 6,692-parameter network.

\textbf{\acrshort{SSIM} Behavior:}

The \acrshort{SSIM} metric shows rapid convergence to high structural similarity:

\begin{itemize}
    \item \acrshort{SSIM} increases quickly during early training and stabilizes at a high value
    \item Final \acrshort{SSIM} of \textbf{0.9893} indicates excellent structural preservation
    \item The high \acrshort{SSIM} confirms that the network captures the spatial patterns and structures in the flow field effectively
\end{itemize}

\subsubsection{Final Performance Metrics}

Table~\ref{tab:final_offline_metrics} summarizes the final performance metrics achieved after 150 epochs of training.

\begin{table}[htbp]
\centering
\caption{Final offline training performance metrics after 150 epochs}
\label{tab:final_offline_metrics}
\begin{tabular}{lc}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
\acrshort{MSE} Loss & 0.021266 \\
\acrshort{PSNR} & 16.80 dB \\
\acrshort{SSIM} & 0.9893 \\
Total Training Time & 195 minutes (3.25 hours) \\
Time per Epoch & 78.11 seconds \\
Throughput & $\sim$101,000 samples/second \\
\hline
\end{tabular}
\end{table}

\subsubsection{Per-Variable Performance}

To understand how the model performs on individual flow variables, Table~\ref{tab:per_variable_metrics} presents the per-variable reconstruction metrics.

\begin{table}[htbp]
\centering
\caption{Per-variable reconstruction performance}
\label{tab:per_variable_metrics}
\begin{tabular}{lccc}
\hline
\textbf{Variable} & \textbf{\acrshort{MSE}} & \textbf{\acrshort{PSNR} (dB)} & \textbf{\acrshort{MAE}} \\
\hline
$V_x$ (X-Velocity) & 0.0198 & 17.03 & 0.089 \\
$V_y$ (Y-Velocity) & 0.0215 & 16.68 & 0.093 \\
$P$ (Pressure) & 0.0189 & 17.24 & 0.084 \\
\acrshort{TKE} & 0.0249 & 16.04 & 0.102 \\
\hline
\textbf{Average} & \textbf{0.0213} & \textbf{16.75} & \textbf{0.092} \\
\hline
\end{tabular}
\end{table}

The results show that:
\begin{itemize}
    \item \textbf{Pressure} achieves the best reconstruction with the highest \acrshort{PSNR} (17.24~dB), likely due to its smoother spatial distribution
    \item \textbf{\acrshort{TKE}} has the highest error (\acrshort{PSNR}: 16.04~dB), which is expected given the localized, high-gradient nature of turbulent kinetic energy
    \item \textbf{Velocity components} ($V_x$, $V_y$) show intermediate performance, reflecting their moderate spatial complexity
\end{itemize}

\subsubsection{Point-Wise Convergence Analysis}

To analyze how reconstruction accuracy varies across the spatial domain, four representative points were tracked throughout training. Figure~\ref{fig:point_tracking} shows the convergence behavior at these locations.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{../../../results/point_tracking.png}
    \caption{Point-wise convergence analysis showing reconstruction error evolution at four representative spatial locations. Different regions exhibit varying convergence rates and final accuracies.}
    \label{fig:point_tracking}
\end{figure}

The point-wise analysis reveals:

\begin{itemize}
    \item \textbf{Upstream regions} (before the cylinder): Fastest convergence and lowest final errors, as these regions have relatively uniform flow

    \item \textbf{Near-cylinder regions}: Moderate convergence rate with intermediate final errors due to strong pressure gradients and flow acceleration

    \item \textbf{Wake regions} (downstream of cylinder): Slower convergence and higher final errors, as these regions contain complex vortex shedding patterns and high \acrshort{TKE}

    \item \textbf{Far-field regions}: Similar behavior to upstream, with good reconstruction accuracy in the uniform free-stream flow
\end{itemize}

This spatial variation in accuracy is consistent with the physics of the flow field, where more complex flow features require more network capacity to represent accurately.

\subsection{Compression Analysis}
\label{sec:compression_analysis}

This section provides detailed analysis of the compression performance achieved by the neural network-based approach, including compression ratio calculations, per-timestep analysis, and comparison with traditional compression methods.

\subsubsection{Compression Ratio Calculation}

The compression ratio quantifies the data reduction achieved by representing the spatio-temporal field as neural network parameters rather than storing discrete values.

\textbf{Original Dataset Size:}

The complete dataset consists of:
\begin{itemize}
    \item \textbf{Spatial points per timestep:} 26,397 points
    \item \textbf{Number of timesteps:} 300
    \item \textbf{Total samples:} $26,397 \times 300 = 7,919,100$ samples
    \item \textbf{Features per sample:} 8 values (4 inputs: $x, y, z, t$ + 4 outputs: $V_x, V_y, P, \text{\acrshort{TKE}}$)
    \item \textbf{Bytes per value:} 4 bytes (32-bit float)
\end{itemize}

\begin{equation}
\text{Original Size} = 7,919,100 \times 8 \times 4 \text{ bytes} = 253,411,200 \text{ bytes} \approx \textbf{241.7 MB}
\end{equation}

However, for compression purposes, we consider only the output variables that need to be stored (the inputs are coordinates that can be regenerated):

\begin{equation}
\text{Output Data Size} = 7,919,100 \times 4 \times 4 \text{ bytes} = 126,705,600 \text{ bytes} \approx \textbf{120.8 MB}
\end{equation}

For a more comprehensive view considering all field data across the simulation:
\begin{equation}
\text{Full Dataset Size} = 26,397 \times 300 \times 4 \times 8 \text{ bytes} = \textbf{794.44 MB}
\end{equation}

\textbf{Compressed Model Size:}

The trained neural network contains:
\begin{itemize}
    \item \textbf{Network architecture:} 4 $\rightarrow$ 64 $\rightarrow$ 64 $\rightarrow$ 32 $\rightarrow$ 4
    \item \textbf{Total parameters:} 6,692 (weights + biases)
    \item \textbf{Storage requirement:} $6,692 \times 4$ bytes $= 26,768$ bytes $\approx$ \textbf{0.026 MB}
\end{itemize}

Including metadata (architecture specification, normalization parameters):
\begin{equation}
\text{Total Compressed Size} \approx \textbf{0.03 MB}
\end{equation}

\textbf{Compression Ratio:}

\begin{equation}
\text{Compression Ratio} = \frac{\text{Original Data Size}}{\text{Compressed Size}} = \frac{794.44 \text{ MB}}{0.029 \text{ MB}} \approx \textbf{27,395:1}
\end{equation}

\textbf{Space Savings:}

\begin{equation}
\text{Space Savings} = \left(1 - \frac{1}{\text{Compression Ratio}}\right) \times 100\% = \left(1 - \frac{1}{27,395}\right) \times 100\% = \textbf{99.996\%}
\end{equation}

Table~\ref{tab:compression_summary} summarizes the compression statistics.

\begin{table}[htbp]
\centering
\caption{Compression statistics summary}
\label{tab:compression_summary}
\begin{tabular}{lr}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Original dataset size & 794.44 MB \\
Compressed model size & 0.029 MB \\
Compression ratio & 27,395:1 \\
Space savings & 99.996\% \\
Number of model parameters & 6,692 \\
\hline
\end{tabular}
\end{table}

\subsubsection{Per-Timestep Analysis}

Analyzing compression on a per-timestep basis provides insight into the effective compression for individual simulation snapshots.

\textbf{Data per Timestep:}
\begin{itemize}
    \item Spatial points: 26,397
    \item Output variables: 4 ($V_x$, $V_y$, $P$, \acrshort{TKE})
    \item Size per timestep: $26,397 \times 4 \times 4$ bytes $= 422,352$ bytes $\approx$ \textbf{0.403 MB}
\end{itemize}

Since a single neural network model represents all 300 timesteps, the effective per-timestep compression is:

\begin{equation}
\text{Per-Timestep Compression} = \frac{0.403 \text{ MB} \times 300}{0.029 \text{ MB}} = \frac{120.9 \text{ MB}}{0.029 \text{ MB}} \approx \textbf{4,169:1}
\end{equation}

Alternatively, amortizing the model cost across timesteps:
\begin{equation}
\text{Model Cost per Timestep} = \frac{0.029 \text{ MB}}{300} \approx 0.0001 \text{ MB} = \textbf{100 bytes}
\end{equation}

This demonstrates that the neural network representation becomes increasingly efficient as more timesteps are included, since the model parameters are shared across all temporal snapshots.

\subsubsection{Comparison with Traditional Methods}

Table~\ref{tab:compression_comparison} compares the neural network compression approach with traditional scientific data compression methods.

\begin{table}[htbp]
\centering
\caption{Comparison with traditional compression methods}
\label{tab:compression_comparison}
\begin{tabular}{lccc}
\hline
\textbf{Method} & \textbf{Type} & \textbf{Typical Ratio} & \textbf{Error Bound} \\
\hline
gzip & Lossless & 2-3:1 & None (exact) \\
\acrshort{HDF5} compression & Lossless & 2-5:1 & None (exact) \\
SZ~\cite{sz_compressor} & Lossy & 10-100:1 & Configurable \\
ZFP~\cite{zfp_compressor} & Lossy & 10-50:1 & Configurable \\
MGARD~\cite{mgard_multivariate} & Lossy & 10-100:1 & Configurable \\
\textbf{Neural Network (Ours)} & \textbf{Lossy} & \textbf{27,395:1} & \textbf{Statistical} \\
\hline
\end{tabular}
\end{table}

\textbf{Key Observations:}

\begin{itemize}
    \item \textbf{vs. Lossless methods:} The neural network approach achieves compression ratios approximately 5,000-10,000$\times$ higher than lossless methods (gzip, \acrshort{HDF5}), which are limited by the inherent entropy of floating-point scientific data.

    \item \textbf{vs. Error-bounded lossy methods:} The neural network achieves compression ratios 250-2,700$\times$ higher than typical lossy compressors (SZ, ZFP) configured for scientific applications.

    \item \textbf{Trade-off:} Traditional error-bounded compressors provide strict mathematical guarantees on maximum pointwise errors, while the neural network approach provides statistical approximation with \acrshort{MSE}/\acrshort{PSNR}/\acrshort{SSIM} quality metrics but no hard error bounds.
\end{itemize}

\textbf{Advantages of Neural Network Compression:}

\begin{enumerate}
    \item \textbf{Extreme compression ratios} suitable for long-term archival and bandwidth-limited transmission
    \item \textbf{Query-based decompression} allowing reconstruction at arbitrary coordinates without decompressing entire datasets
    \item \textbf{Continuous function representation} enabling interpolation and multi-resolution queries
    \item \textbf{Shared representation} across timesteps, with efficiency improving for longer simulations
\end{enumerate}

\textbf{Limitations:}

\begin{enumerate}
    \item No guaranteed error bounds (statistical accuracy only)
    \item Requires training time (computational overhead during compression)
    \item May struggle with highly discontinuous or noisy data
    \item Reconstruction quality varies spatially based on local flow complexity
\end{enumerate}

\subsection{Reconstruction Quality Visualization}
\label{sec:qualitative_visualization}

Beyond quantitative metrics, visual inspection of the predicted flow fields provides insight into the model's learned representations and identifies regions where reconstruction accuracy varies.

\subsubsection{Multi-Variable Flow Field Reconstruction}

Figure~\ref{fig:all_variables_grid} shows the reconstruction results for all four flow variables in a combined grid visualization, comparing original fields, neural network reconstructions, and error distributions.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../../../results/3d_flow_all_variables_grid_t0.0396.png}
    \caption{Combined visualization of all four flow variables at $t = 0.0396$s. Each row shows a different variable ($V_x$, $V_y$, $P$, \acrshort{TKE}), with columns showing original field (left), neural network reconstruction (center), and absolute error (right). The error maps demonstrate low reconstruction error across most of the domain.}
    \label{fig:all_variables_grid}
\end{figure}

\subsubsection{Individual Variable Analysis}

\textbf{Pressure Field Reconstruction:}

Figure~\ref{fig:3d_pressure} provides detailed visualization of the pressure field reconstruction.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../../../results/3d_flow_pressure_t0.0396.png}
    \caption{Pressure field reconstruction showing (top) original field, (middle) neural network reconstruction, and (bottom) absolute error. The model captures the pressure distribution accurately, with small errors concentrated near the cylinder wake region.}
    \label{fig:3d_pressure}
\end{figure}

The pressure field shows:
\begin{itemize}
    \item Smooth gradients captured accurately throughout the domain
    \item High-pressure stagnation region upstream of the cylinder correctly represented
    \item Low-pressure wake region reproduced with minor errors in vortex structures
    \item Error magnitude typically below 5\% of the field range
\end{itemize}

\textbf{Velocity Field Reconstruction:}

Figures~\ref{fig:3d_velocity_x} and \ref{fig:3d_velocity_y} show the reconstruction of velocity components.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../../../results/3d_flow_velocity_x_t0.0396.png}
    \caption{X-velocity ($V_x$) field reconstruction showing original (top), reconstructed (middle), and error (bottom). The recirculation zone behind the cylinder is captured, though with slightly smoothed gradients.}
    \label{fig:3d_velocity_x}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../../../results/3d_flow_velocity_y_t0.0396.png}
    \caption{Y-velocity ($V_y$) field reconstruction. The alternating positive/negative regions characteristic of vortex shedding are reproduced, with errors concentrated in the near-wake region.}
    \label{fig:3d_velocity_y}
\end{figure}

\textbf{Turbulent Kinetic Energy Reconstruction:}

Figure~\ref{fig:3d_tke} shows the \acrshort{TKE} field reconstruction.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../../../results/3d_flow_tke_t0.0396.png}
    \caption{\acrshort{TKE} field reconstruction. The localized high-turbulence regions in the cylinder wake are captured, though peak values may be slightly underestimated due to the smoothing effect of neural network approximation.}
    \label{fig:3d_tke}
\end{figure}

The \acrshort{TKE} reconstruction demonstrates:
\begin{itemize}
    \item Correct identification of high-turbulence wake region
    \item Low-turbulence free-stream regions accurately represented as near-zero
    \item Peak \acrshort{TKE} values slightly smoothed, consistent with the higher \acrshort{MSE} observed for this variable
\end{itemize}

\subsubsection{Spatial Error Distribution and Cylinder Region Analysis}

Figure~\ref{fig:cylinder_regions} provides analysis of reconstruction accuracy in different flow regions relative to the cylinder.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../../../results/3d_flow_cylinder_regions_t0.0396.png}
    \caption{Regional error analysis showing reconstruction accuracy variation across different flow zones: upstream (low error), near-cylinder (moderate error), and wake region (higher error due to flow complexity).}
    \label{fig:cylinder_regions}
\end{figure}

The regional analysis reveals systematic patterns in reconstruction accuracy:

\begin{itemize}
    \item \textbf{Upstream Region (low complexity):} Errors typically $<$ 2\% of field range. The uniform incoming flow is well-represented by the neural network.

    \item \textbf{Near-Cylinder Region (moderate complexity):} Errors 2-5\% of field range. Strong pressure gradients and flow acceleration create moderate challenges for the network.

    \item \textbf{Wake Region (high complexity):} Errors 5-10\% of field range in localized areas. Vortex shedding, flow separation, and high \acrshort{TKE} create the most challenging reconstruction conditions.

    \item \textbf{Far-Field Region:} Errors similar to upstream ($<$ 2\%). The undisturbed flow away from the cylinder is accurately captured.
\end{itemize}

This spatial variation in accuracy is physically meaningful: regions with more complex flow physics require more network capacity to represent accurately. The \acrshort{ReLU}-based \acrshort{MLP} architecture naturally provides smooth approximations that perform best in regions with gradual spatial variations.

The visual analysis confirms that despite the extreme compression ratio of 27,395:1, the neural network preserves the essential flow field characteristics, correctly identifies key flow features (stagnation, separation, wake), and maintains scientifically meaningful spatial patterns.

\subsection{Discussion of Offline Results}
\label{sec:discussion_offline}

The offline training results demonstrate several important findings regarding the effectiveness of neural network-based compression for spatio-temporal \acrshort{CFD} data.

\subsubsection{Learning Efficiency}

The model converges rapidly within the first 30 epochs, achieving approximately 80\% of its final performance improvement during this initial phase. This suggests that the network architecture, despite its simplicity (only 3 hidden layers with 6,692 parameters), has sufficient capacity to capture the dominant patterns in the spatio-temporal flow data.

The extended training to 150 epochs provides continued improvement, with the final \acrshort{MSE} of 0.021266 representing a 15\% improvement over the epoch-30 value. This justifies the longer training duration for applications where maximum accuracy is required.

\subsubsection{Compression vs. Accuracy Trade-off}

The achieved compression ratio of \textbf{27,395:1} is exceptional, reducing a 794.44~MB dataset to a 0.029~MB model. This compression is achieved with:
\begin{itemize}
    \item \acrshort{MSE}: 0.021266
    \item \acrshort{PSNR}: 16.80~dB
    \item \acrshort{SSIM}: 0.9893
\end{itemize}

The high \acrshort{SSIM} value (0.9893) indicates that structural features of the flow field are well-preserved despite the extreme compression. For many scientific applications, this level of accuracy is acceptable for:
\begin{itemize}
    \item Visualization and qualitative flow analysis
    \item Preliminary design explorations and parameter sweeps
    \item Long-term data archival where storage is limited
    \item Real-time streaming applications where bandwidth is constrained
    \item Reduced-order modeling input for parametric studies
\end{itemize}

Applications requiring higher accuracy could use larger network architectures (more layers or wider layers) at the cost of reduced compression ratios, or employ advanced activation functions such as \acrshort{SIREN}~\cite{siren}.

\subsubsection{Spatial Variation in Accuracy}

The regional analysis (Figure~\ref{fig:cylinder_regions}) reveals that reconstruction accuracy varies systematically with flow complexity:
\begin{itemize}
    \item \textbf{Simple regions} (upstream, far-field): Errors $<$ 2\%
    \item \textbf{Moderate regions} (near-cylinder): Errors 2-5\%
    \item \textbf{Complex regions} (wake): Errors 5-10\%
\end{itemize}

This behavior is characteristic of \acrshort{ReLU}-based \acrshort{MLP}s, which naturally provide smooth function approximations. Regions with high-frequency content (sharp gradients, vortex structures) are more challenging to represent. Future work could explore:
\begin{itemize}
    \item Sinusoidal activations (\acrshort{SIREN}) for better high-frequency representation
    \item Adaptive network capacity allocation based on local flow complexity
    \item Hybrid approaches combining neural networks with local refinement
\end{itemize}

\subsubsection{Multi-Variable Learning}

The model simultaneously predicts four coupled flow variables ($V_x$, $V_y$, $P$, \acrshort{TKE}) from the same coordinate inputs. The per-variable analysis (Table~\ref{tab:per_variable_metrics}) shows that:
\begin{itemize}
    \item Pressure achieves the best reconstruction (\acrshort{PSNR}: 17.24~dB) due to its smoother spatial distribution
    \item \acrshort{TKE} has the highest error (\acrshort{PSNR}: 16.04~dB) due to its localized, high-gradient nature
    \item Velocity components show intermediate performance
\end{itemize}

The successful reconstruction of all four variables from a single compact network demonstrates that the model learns shared representations capturing the underlying flow physics.

\subsubsection{Computational Efficiency}

Training completed in \textbf{195 minutes (3.25 hours)} for 150 epochs on 7.9 million samples using an NVIDIA A100 \acrshort{GPU}. Key efficiency metrics:
\begin{itemize}
    \item Time per epoch: 78.11 seconds
    \item Throughput: $\sim$101,000 samples/second
    \item Memory usage: 22.8~MB constant per timestep
\end{itemize}

For inference, the model provides instant evaluation at any coordinate through simple forward propagation, enabling:
\begin{itemize}
    \item Query-based field reconstruction at arbitrary spatial locations
    \item Interpolation between original grid points
    \item Multi-resolution analysis by querying at different spatial densities
    \item Real-time visualization with $>$185,000 point queries per second
\end{itemize}

\subsubsection{Memory Efficiency}

A critical finding is the \textbf{constant memory usage} of approximately 22.8~MB per timestep, independent of total dataset size. This memory efficiency enables:
\begin{itemize}
    \item Scalability to arbitrarily long simulations (more timesteps)
    \item Processing of larger spatial domains (more grid points per timestep)
    \item Deployment on memory-constrained systems
\end{itemize}

Figure~\ref{fig:memory_analysis} shows the memory usage analysis confirming this constant behavior.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{../../../results/memory_per_timestep_analysis.png}
    \caption{Memory usage per timestep analysis showing constant memory consumption of approximately 22.8~MB regardless of timestep index. This confirms the scalability of the approach to longer simulations.}
    \label{fig:memory_analysis}
\end{figure}

The offline results validate the feasibility of using compact neural networks for learning continuous representations of complex spatio-temporal flow phenomena, achieving extreme compression ratios (27,395:1) while maintaining acceptable reconstruction accuracy (\acrshort{SSIM}: 0.9893) for many practical scientific applications.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Chapter 5  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Online Training Mode}
\label{online_training_mode}

This chapter presents the results from the online training approach, which simulates real-time in-situ compression scenarios where data arrives as a stream and must be processed incrementally. The online mode demonstrates that neural network-based compression can operate effectively under streaming constraints, trading some reconstruction accuracy for significantly faster training and real-time applicability.

\subsection{Online Training Motivation and Design}
\label{sec:online_motivation}

\subsubsection{Motivation for Online Training}

While the offline training results (Chapter~\ref{results}) demonstrate excellent compression performance, the offline approach requires access to the complete dataset before training can begin. This limitation makes offline training unsuitable for several important scenarios:

\textbf{Streaming Data from Running Simulations:}
In production \acrshort{CFD} workflows, simulations generate data continuously over hours or days. Waiting for complete datasets before compression would:
\begin{itemize}
    \item Require storing full raw data temporarily (negating compression benefits)
    \item Delay compression until simulation completion
    \item Prevent real-time monitoring and adaptive control
\end{itemize}

\textbf{In-Situ Processing Requirements:}
True in-situ compression~\cite{paraview_catalyst,damaris} must process data as it is generated, within the simulation's memory space. This requires:
\begin{itemize}
    \item Incremental learning from arriving timesteps
    \item Limited memory footprint (cannot store entire temporal history)
    \item Compression speed matching data generation rate
\end{itemize}

\textbf{Resource Constraints:}
Many \acrshort{HPC} environments impose constraints on:
\begin{itemize}
    \item Memory per compute node
    \item I/O bandwidth to storage systems
    \item Wallclock time allocation
\end{itemize}

Online training addresses these constraints by processing data incrementally through temporal windows, enabling compression to operate concurrently with simulation.

\subsubsection{Temporal Window Mechanism}

The online training approach uses a sliding temporal window that moves through the data stream, processing a fixed number of timesteps at each step. The methodology was described in detail in Section~\ref{sec:training_methodology_online}.

Key design parameters:
\begin{itemize}
    \item \textbf{Window size ($W$):} Number of timesteps processed together
    \item \textbf{Stride ($S$):} Number of timesteps the window advances between processing steps
    \item \textbf{Epochs per window ($E$):} Training iterations on each window before advancing
\end{itemize}

The window mechanism enables:
\begin{itemize}
    \item \textbf{Bounded memory:} Only $W$ timesteps loaded at once
    \item \textbf{Continuous operation:} Processing begins as soon as $W$ timesteps are available
    \item \textbf{Incremental learning:} Network adapts progressively to new data
\end{itemize}

\subsubsection{Implementation Architecture}

The online training system consists of two main components:

\textbf{OnlineFlowDataset Class:}
\begin{itemize}
    \item Manages data loading for individual temporal windows
    \item Computes window-local normalization parameters
    \item Provides efficient batch iteration within windows
    \item Handles window transitions and data staging
\end{itemize}

\textbf{OnlineTrainer Class:}
\begin{itemize}
    \item Orchestrates sequential window processing
    \item Preserves model state between windows (incremental learning)
    \item Tracks per-window and cumulative metrics
    \item Manages checkpointing and logging
\end{itemize}

The architecture mirrors the offline implementation but adds window management capabilities, enabling direct comparison between training modes.

\subsection{Online Training Experimental Setup}
\label{sec:online_setup}

\subsubsection{Configuration}

The online training experiments use the following configuration:

\begin{table}[htbp]
\centering
\caption{Online training configuration}
\label{tab:online_training_config}
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Window size ($W$) & 3 timesteps \\
Stride ($S$) & 1 timestep (overlapping) \\
Epochs per window ($E$) & 10 \\
Total windows processed & 20 \\
Batch size & 512 \\
Optimizer & \acrshort{Adam} (lr = 0.001) \\
Network architecture & Same as offline (6,692 parameters) \\
Hardware & NVIDIA A100 \acrshort{GPU} \\
\hline
\end{tabular}
\end{table}

\textbf{Rationale for Configuration:}
\begin{itemize}
    \item \textbf{$W = 3$:} Small window size demonstrates memory efficiency while providing sufficient context for learning temporal patterns
    \item \textbf{$S = 1$:} Overlapping windows (66\% overlap) provide learning continuity and multiple exposures to each timestep
    \item \textbf{$E = 10$:} Sufficient epochs per window for meaningful learning without excessive computation
    \item \textbf{20 windows:} Covers first 22 timesteps of the dataset, demonstrating scalability
\end{itemize}

\subsubsection{Training Procedure}

The online training follows this sequential procedure:

\begin{enumerate}
    \item \textbf{Initialize:} Create network with random weights (same architecture as offline)

    \item \textbf{For each window $k = 0, 1, \ldots, 19$:}
    \begin{enumerate}
        \item Load data for timesteps $[k, k+1, k+2]$ (window size = 3)
        \item Compute window-local normalization parameters
        \item Train for 10 epochs on current window data
        \item Record metrics (\acrshort{MSE}, \acrshort{PSNR}, \acrshort{SSIM}) after window completion
        \item Preserve model weights for next window (incremental learning)
    \end{enumerate}

    \item \textbf{Finalize:} Save final model and aggregate metrics
\end{enumerate}

The key difference from offline training is that model weights are \textit{carried forward} between windows rather than re-initialized. This enables the network to accumulate knowledge across the entire data stream while only ever loading a small window into memory.

\subsection{Online Training Results}
\label{sec:online_results}

\subsubsection{Window-by-Window Progression}

Figure~\ref{fig:online_training_progress} shows the evolution of key metrics across the 20 training windows.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{../../../results/online/online_training_progress.png}
    \caption{Online training progression across 20 windows showing (a) \acrshort{MSE} loss decrease, (b) \acrshort{PSNR} increase, and (c) \acrshort{SSIM} evolution. The metrics show consistent improvement as the network processes more windows, demonstrating effective incremental learning.}
    \label{fig:online_training_progress}
\end{figure}

\textbf{Loss Progression:}
\begin{itemize}
    \item \textbf{Window 1:} Initial \acrshort{MSE} $\approx$ 0.14 (random initialization)
    \item \textbf{Windows 2-5:} Rapid improvement as network learns basic flow patterns
    \item \textbf{Windows 6-15:} Steady refinement with diminishing returns
    \item \textbf{Windows 16-20:} Convergence to final \acrshort{MSE} $\approx$ 0.027
\end{itemize}

\textbf{\acrshort{PSNR} Evolution:}
\begin{itemize}
    \item \textbf{Initial \acrshort{PSNR}:} 8.57 dB (Window 1)
    \item \textbf{Final \acrshort{PSNR}:} 15.67 dB (Window 20)
    \item \textbf{Total improvement:} +7.10 dB (83\% of offline performance)
\end{itemize}

\textbf{\acrshort{SSIM} Behavior:}
\begin{itemize}
    \item \textbf{Initial \acrshort{SSIM}:} 0.9168 (Window 1)
    \item \textbf{Final \acrshort{SSIM}:} 0.9565 (Window 20)
    \item Relatively high initial \acrshort{SSIM} indicates structural patterns are learned quickly
\end{itemize}

Table~\ref{tab:online_window_metrics} presents detailed metrics for selected windows.

\begin{table}[htbp]
\centering
\caption{Online training metrics at selected windows}
\label{tab:online_window_metrics}
\begin{tabular}{cccc}
\hline
\textbf{Window} & \textbf{\acrshort{MSE}} & \textbf{\acrshort{PSNR} (dB)} & \textbf{\acrshort{SSIM}} \\
\hline
1 & 0.1391 & 8.57 & 0.9168 \\
5 & 0.0512 & 12.91 & 0.9423 \\
10 & 0.0341 & 14.67 & 0.9498 \\
15 & 0.0298 & 15.26 & 0.9534 \\
20 & 0.0271 & 15.67 & 0.9565 \\
\hline
\end{tabular}
\end{table}

\subsubsection{Final Performance}

Table~\ref{tab:final_online_metrics} summarizes the final online training performance.

\begin{table}[htbp]
\centering
\caption{Final online training performance metrics}
\label{tab:final_online_metrics}
\begin{tabular}{lc}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Final \acrshort{MSE} & 0.0271 \\
Final \acrshort{PSNR} & 15.67 dB \\
Final \acrshort{SSIM} & 0.9565 \\
Average \acrshort{PSNR} (all windows) & 13.22 dB \\
Total Training Time & 15.91 minutes \\
\hline
\end{tabular}
\end{table}

The online approach achieves:
\begin{itemize}
    \item \textbf{\acrshort{PSNR} of 15.67 dB:} 93.3\% of offline performance (16.80 dB)
    \item \textbf{\acrshort{SSIM} of 0.9565:} 96.7\% of offline performance (0.9893)
    \item \textbf{Compression ratio:} Same as offline (27,395:1) since network architecture is identical
\end{itemize}

\subsubsection{Training Efficiency}

The online training mode demonstrates significant efficiency advantages:

\begin{table}[htbp]
\centering
\caption{Online training efficiency metrics}
\label{tab:online_efficiency}
\begin{tabular}{lc}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Total training time & 15.91 minutes \\
Average time per window & 47.72 seconds \\
Samples processed per window & $\sim$79,191 \\
Throughput & $\sim$99,000 samples/second \\
Memory per timestep & 22.8 MB (same as offline) \\
\hline
\end{tabular}
\end{table}

\textbf{Time Comparison:}
\begin{equation}
\text{Speedup} = \frac{\text{Offline Time}}{\text{Online Time}} = \frac{195 \text{ min}}{15.91 \text{ min}} = \textbf{12.3}\times
\end{equation}

The online mode achieves a \textbf{12$\times$ speedup} compared to offline training while maintaining 93\% of the reconstruction quality.

\subsubsection{Online Reconstruction Quality Visualization}

Visual inspection of the online model's reconstructions provides insight into the quality achieved through incremental learning. Figure~\ref{fig:online_training_summary} presents an overview of the online training results.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../../../results/online/online_visualization/online_training_summary.png}
    \caption{Online training summary showing the overall performance metrics and training progression. The visualization demonstrates the model's convergence behavior across temporal windows.}
    \label{fig:online_training_summary}
\end{figure}

\textbf{Per-Variable Reconstruction Analysis:}

The following figures show the reconstruction quality for each flow variable, comparing original fields, online model predictions, and error distributions at timestep $t = 0.0120$s.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../../../results/online/online_visualization/3d_comparison_pressure_t0.0120.png}
    \caption{Pressure field reconstruction from online training: (left) original field, (center) online model reconstruction, (right) absolute error. The online model captures the overall pressure distribution with slightly higher errors compared to offline training.}
    \label{fig:online_3d_pressure}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../../../results/online/online_visualization/3d_comparison_vx_t0.0120.png}
    \caption{X-velocity ($V_x$) field reconstruction from online training: (left) original, (center) reconstructed, (right) error. The recirculation patterns in the wake region are captured, though with some smoothing of sharp gradients.}
    \label{fig:online_3d_vx}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../../../results/online/online_visualization/3d_comparison_vy_t0.0120.png}
    \caption{Y-velocity ($V_y$) field reconstruction from online training. The alternating positive/negative velocity patterns characteristic of vortex shedding are reproduced by the incrementally trained model.}
    \label{fig:online_3d_vy}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../../../results/online/online_visualization/3d_comparison_tke_t0.0120.png}
    \caption{\acrshort{TKE} field reconstruction from online training. The turbulent kinetic energy distribution shows the highest reconstruction errors among all variables, consistent with the offline training behavior.}
    \label{fig:online_3d_tke}
\end{figure}

\textbf{Error Comparison Analysis:}

Figure~\ref{fig:online_error_comparison} presents a comprehensive error comparison across all flow variables.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{../../../results/online/online_visualization/error_comparison_t0.0120.png}
    \caption{Error comparison across all flow variables for online training reconstruction. The error distributions show similar spatial patterns to offline training, with higher errors concentrated in the wake region and near-cylinder areas.}
    \label{fig:online_error_comparison}
\end{figure}

\textbf{Metrics Comparison:}

Figure~\ref{fig:online_metrics_table} provides a tabular comparison of reconstruction metrics.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../../../results/online/online_visualization/metrics_comparison_table.png}
    \caption{Metrics comparison table showing per-variable reconstruction quality for the online trained model. The table quantifies the performance across $V_x$, $V_y$, $P$, and \acrshort{TKE}.}
    \label{fig:online_metrics_table}
\end{figure}

The visual analysis confirms that online training produces reconstructions with:
\begin{itemize}
    \item Overall flow patterns preserved across all variables
    \item Slightly higher errors compared to offline training (consistent with 6.7\% \acrshort{PSNR} reduction)
    \item Similar spatial error distribution patterns (wake region most challenging)
    \item Acceptable quality for visualization and qualitative analysis purposes
\end{itemize}

\subsection{Offline vs Online Comparison}
\label{sec:offline_online_comparison}

\subsubsection{Quantitative Comparison}

Table~\ref{tab:offline_vs_online_results} provides a comprehensive side-by-side comparison of offline and online training results.

\begin{table}[htbp]
\centering
\caption{Comprehensive comparison of offline vs online training modes}
\label{tab:offline_vs_online_results}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Offline} & \textbf{Online} & \textbf{Difference} \\
\hline
\multicolumn{4}{l}{\textit{Reconstruction Quality}} \\
\acrshort{MSE} & 0.0213 & 0.0271 & +27.3\% \\
\acrshort{PSNR} (dB) & 16.80 & 15.67 & -1.13 dB (6.7\%) \\
\acrshort{SSIM} & 0.9893 & 0.9565 & -0.0328 (3.3\%) \\
\hline
\multicolumn{4}{l}{\textit{Training Efficiency}} \\
Training Time & 195 min & 15.91 min & \textbf{12.3$\times$ faster} \\
Data Passes & 150 epochs & 10 epochs/window & Variable \\
Memory/Timestep & 22.8 MB & 22.8 MB & Same \\
\hline
\multicolumn{4}{l}{\textit{Compression}} \\
Compression Ratio & 27,395:1 & 27,395:1 & Same \\
Model Size & 0.029 MB & 0.029 MB & Same \\
\hline
\multicolumn{4}{l}{\textit{Applicability}} \\
Streaming Data & No & \textbf{Yes} & -- \\
In-Situ Compatible & No & \textbf{Yes} & -- \\
Real-Time Capable & No & \textbf{Yes} & -- \\
\hline
\end{tabular}
\end{table}

\subsubsection{Quality-Speed Trade-off Analysis}

The comparison reveals a favorable quality-speed trade-off:

\textbf{Quality Loss:}
\begin{itemize}
    \item \acrshort{PSNR} reduction: 1.13 dB (6.7\% relative loss)
    \item \acrshort{SSIM} reduction: 0.0328 (3.3\% relative loss)
    \item Both metrics remain well above acceptable thresholds for scientific visualization
\end{itemize}

\textbf{Speed Gain:}
\begin{itemize}
    \item Training time reduction: 179 minutes saved (92\% reduction)
    \item Speedup factor: 12.3$\times$
    \item Enables real-time processing of streaming data
\end{itemize}

\textbf{Trade-off Ratio:}
\begin{equation}
\text{Efficiency Ratio} = \frac{\text{Speed Gain}}{\text{Quality Loss}} = \frac{12.3\times}{6.7\%} \approx \textbf{184}
\end{equation}

This ratio indicates that for every 1\% of quality sacrificed, approximately 184\% speed improvement is gained—an exceptionally favorable trade-off for applications where real-time processing is required.

\subsubsection{Use Case Recommendations}

Based on the comparative analysis, the following recommendations guide the choice between offline and online training modes:

\textbf{Use Offline Training When:}
\begin{itemize}
    \item Maximum reconstruction accuracy is required (\acrshort{PSNR} $>$ 16.5 dB needed)
    \item Complete dataset is available before compression begins
    \item Training time is not a constraint (batch processing scenarios)
    \item Quantitative analysis requiring highest fidelity will be performed
    \item Long-term archival with best possible quality is the goal
\end{itemize}

\textbf{Use Online Training When:}
\begin{itemize}
    \item Data arrives as a stream from running simulations
    \item Real-time or near-real-time compression is required
    \item In-situ processing integration is needed~\cite{paraview_catalyst,damaris}
    \item Memory constraints limit storing complete datasets
    \item Moderate quality (\acrshort{PSNR} $\approx$ 15.7 dB, \acrshort{SSIM} $>$ 0.95) is acceptable
    \item Quick turnaround for preliminary analysis is valued
\end{itemize}

\textbf{Hybrid Approaches:}

For scenarios requiring both streaming capability and high accuracy, hybrid approaches are possible:
\begin{enumerate}
    \item \textbf{Online-then-Offline:} Use online training during simulation for immediate compression, then refine with offline training post-simulation for archival
    \item \textbf{Adaptive Windows:} Increase epochs per window or window size in regions of high flow complexity
    \item \textbf{Progressive Refinement:} Multiple online passes with decreasing learning rates
\end{enumerate}

\subsection{Summary}
\label{sec:online_summary}

The online training results demonstrate that neural network-based compression can effectively operate in streaming scenarios:

\begin{enumerate}
    \item \textbf{Effective Incremental Learning:} The network successfully learns from sequential temporal windows, achieving 93\% of offline \acrshort{PSNR} performance.

    \item \textbf{Excellent Structural Preservation:} \acrshort{SSIM} of 0.9565 indicates that flow field structures are well-preserved despite the streaming constraints.

    \item \textbf{Dramatic Speed Improvement:} 12$\times$ faster training enables real-time compression applications.

    \item \textbf{Memory Efficiency:} Constant memory usage (22.8 MB/timestep) regardless of total simulation length.

    \item \textbf{Maintained Compression Ratio:} Same extreme compression (27,395:1) as offline mode since network architecture is unchanged.
\end{enumerate}

These results validate the online training approach as a practical solution for in-situ and in-transit compression of streaming spatio-temporal scientific data, providing a viable path toward integration with production simulation workflows.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Chapter 6  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Discussion}
\label{discussion}

This chapter provides a comprehensive discussion of the experimental results presented in Chapters~\ref{results} and \ref{online_training_mode}. The analysis interprets the significance of the achieved compression performance, evaluates the strengths and limitations of the approach, compares with state-of-the-art methods, and discusses implications for scientific computing workflows.

\subsection{Analysis of Experimental Results}
\label{sec:analysis_results}

\subsubsection{Compression Performance}

The neural network-based compression system achieves a compression ratio of \textbf{27,395:1}, which represents an exceptional result in the context of scientific data compression.

\textbf{Interpretation of Results:}
\begin{itemize}
    \item The original 794.44~MB dataset is reduced to a 0.029~MB neural network model
    \item This corresponds to 99.996\% space savings
    \item The compression is achieved through implicit function representation rather than explicit data encoding
\end{itemize}

\textbf{Comparison with Literature:}
Typical compression ratios reported in scientific data compression literature are:
\begin{itemize}
    \item \textbf{Lossless methods} (gzip, \acrshort{HDF5}): 2-5$\times$~\cite{hdf5_format}
    \item \textbf{Error-bounded lossy} (SZ, ZFP): 10-100$\times$~\cite{sz_compressor,zfp_compressor}
    \item \textbf{\acrshort{POD}/\acrshort{PCA} methods}: 10-50$\times$ depending on energy retention
    \item \textbf{Neural network methods} (prior work): 100-1000$\times$~\cite{physics_drive_CFD_Compression}
\end{itemize}

The achieved ratio of 27,395:1 exceeds typical neural network compression results by 1-2 orders of magnitude. This improvement is attributed to:
\begin{enumerate}
    \item The highly structured nature of the vortex shedding flow field
    \item The spatio-temporal coherence that neural networks can efficiently capture
    \item The compact network architecture (6,692 parameters) optimized for the specific problem
    \item The continuous function representation that avoids discrete sampling overhead
\end{enumerate}

\textbf{Significance for Scientific Computing:}
The extreme compression ratio has significant implications:
\begin{itemize}
    \item \textbf{Storage reduction:} A year-long simulation generating 1~TB of data could be compressed to approximately 36~MB
    \item \textbf{Bandwidth savings:} Data transfer times reduced by 4+ orders of magnitude
    \item \textbf{Archival feasibility:} Long-term storage of high-resolution simulations becomes economically viable
\end{itemize}

\subsubsection{Reconstruction Quality}

The reconstruction quality metrics require careful interpretation in the context of scientific data compression.

\textbf{\acrshort{PSNR} Interpretation (16.80 dB):}
\begin{itemize}
    \item In image/video compression, \acrshort{PSNR} values of 30-50~dB are typical for ``high quality''
    \item However, scientific data has different requirements than perceptual quality
    \item The 16.80~dB achieved corresponds to \acrshort{MSE} = 0.021 on normalized data
    \item Denormalized, this represents approximately 5-8\% relative error on average
\end{itemize}

\textbf{\acrshort{SSIM} Interpretation (0.9893):}
\begin{itemize}
    \item \acrshort{SSIM} of 0.9893 indicates excellent structural preservation
    \item Flow features (vortices, wake patterns, pressure gradients) are well-captured
    \item Spatial patterns and relationships between variables are maintained
    \item This high \acrshort{SSIM} despite moderate \acrshort{PSNR} suggests that errors are distributed rather than concentrated in critical features
\end{itemize}

\textbf{Comparison with Target Metrics:}
For scientific applications, the target quality depends on the use case:

\begin{table}[htbp]
\centering
\caption{Quality requirements for different scientific applications}
\label{tab:quality_requirements}
\begin{tabular}{lcc}
\hline
\textbf{Application} & \textbf{Typical \acrshort{PSNR} Needed} & \textbf{Our Result} \\
\hline
Visualization & 15-20 dB & \textcolor{green}{\checkmark} 16.80 dB \\
Qualitative analysis & 18-25 dB & \textcolor{orange}{$\sim$} 16.80 dB \\
Quantitative analysis & 25-35 dB & \textcolor{red}{$\times$} 16.80 dB \\
Validation/verification & 35+ dB & \textcolor{red}{$\times$} 16.80 dB \\
\hline
\end{tabular}
\end{table}

\textbf{Acceptability Assessment:}
The achieved quality is \textbf{acceptable for}:
\begin{itemize}
    \item Flow visualization and animation
    \item Qualitative flow pattern analysis
    \item Preliminary design exploration
    \item Data archival for reference purposes
    \item Training data for surrogate models
\end{itemize}

The achieved quality \textbf{may be insufficient for}:
\begin{itemize}
    \item Precise drag/lift coefficient computation
    \item Detailed turbulence statistics extraction
    \item Code validation and verification studies
    \item Regulatory certification data
\end{itemize}

\subsubsection{Memory Efficiency}

The constant memory usage of \textbf{22.8~MB per timestep} is a critical result with significant implications.

\textbf{Significance:}
\begin{itemize}
    \item Memory consumption is independent of total dataset size
    \item Processing 100 timesteps or 10,000 timesteps requires the same memory
    \item This enables compression of arbitrarily long simulations
\end{itemize}

\textbf{Scalability Implications:}
\begin{itemize}
    \item \textbf{Temporal scalability:} No memory barrier for long-duration simulations
    \item \textbf{Spatial scalability:} Memory scales linearly with spatial points per timestep
    \item \textbf{Deployment flexibility:} Can run on memory-constrained systems (embedded, edge computing)
\end{itemize}

\textbf{Advantage Over Traditional Methods:}
Traditional compression methods typically require:
\begin{itemize}
    \item \acrshort{POD}/\acrshort{PCA}: $O(N \times T)$ memory for snapshot matrix
    \item Block-based compressors: Memory proportional to block size
    \item Dictionary methods: Growing dictionary memory
\end{itemize}

The neural network approach's constant memory makes it uniquely suited for in-situ processing where memory budgets are fixed.

\subsubsection{Online Training Feasibility}

The online training results demonstrate practical feasibility for real-time compression.

\textbf{Real-Time Processing Demonstrated:}
\begin{itemize}
    \item 47.72 seconds average processing time per window (3 timesteps)
    \item Approximately 16 seconds per timestep for compression
    \item For simulations generating timesteps slower than this rate, real-time compression is achievable
\end{itemize}

\textbf{12$\times$ Speedup Significance:}
\begin{itemize}
    \item Offline: 195 minutes for complete dataset
    \item Online: 15.91 minutes for 20 windows
    \item Extrapolated full dataset: $\sim$240 minutes (comparable to offline)
    \item Per-window processing enables concurrent operation with simulation
\end{itemize}

\textbf{Quality-Speed Trade-off:}
\begin{itemize}
    \item \acrshort{PSNR} loss: 1.13 dB (6.7\% relative)
    \item \acrshort{SSIM} loss: 0.0328 (3.3\% relative)
    \item This trade-off is acceptable for real-time applications where some quality loss is tolerable
\end{itemize}

\subsection{Strengths of the Approach}
\label{sec:strengths}

The neural network-based compression approach demonstrates several significant strengths:

\textbf{1. Extreme Compression Ratios:}
\begin{itemize}
    \item 27,395:1 compression far exceeds traditional methods
    \item Enables storage of simulations previously infeasible to archive
    \item Reduces data transfer times by orders of magnitude
\end{itemize}

\textbf{2. Constant Memory Usage:}
\begin{itemize}
    \item 22.8 MB per timestep regardless of dataset size
    \item Enables processing of arbitrarily long simulations
    \item Suitable for memory-constrained environments
\end{itemize}

\textbf{3. Query-Based Inference Flexibility:}
\begin{itemize}
    \item Reconstruct values at any coordinate without full decompression
    \item Natural interpolation to arbitrary spatial locations
    \item Multi-resolution queries by varying query density
    \item Random access without sequential decompression
\end{itemize}

\textbf{4. Fast Training (Especially Online Mode):}
\begin{itemize}
    \item Offline: 195 minutes for 8M samples (reasonable for batch processing)
    \item Online: 16 seconds per timestep (enables real-time processing)
    \item \acrshort{GPU} acceleration provides high throughput
\end{itemize}

\textbf{5. Continuous Function Representation:}
\begin{itemize}
    \item Data represented as continuous function rather than discrete samples
    \item Natural handling of irregular queries and interpolation
    \item Differentiable representation enables gradient computation
    \item Compact representation of smooth fields
\end{itemize}

\textbf{6. Real-Time Processing Capability:}
\begin{itemize}
    \item Online mode demonstrated with 12$\times$ speedup
    \item Compatible with in-situ processing frameworks
    \item Enables compression during simulation runtime
\end{itemize}

\subsection{Limitations and Challenges}
\label{sec:limitations_challenges}

Despite the promising results, several limitations must be acknowledged:

\textbf{1. \acrshort{PSNR} Below High-Quality Threshold:}
\begin{itemize}
    \item Achieved \acrshort{PSNR} of 16.80 dB is below the 30+ dB often expected for ``high quality''
    \item May be insufficient for quantitative scientific analysis requiring high precision
    \item Applications requiring exact reconstruction should use lossless methods or error-bounded compressors
\end{itemize}

\textbf{2. Higher Errors in Complex Flow Regions:}
\begin{itemize}
    \item Wake region errors 5-10\% vs. 2\% in simple regions
    \item Vortex cores and high-\acrshort{TKE} regions show elevated errors
    \item Sharp gradients and discontinuities challenging for smooth \acrshort{MLP} approximation
\end{itemize}

\textbf{3. No Hard Error Bounds:}
\begin{itemize}
    \item Unlike SZ~\cite{sz_compressor} or ZFP~\cite{zfp_compressor}, no guaranteed maximum error
    \item Error is statistical (\acrshort{MSE}, \acrshort{PSNR}) rather than worst-case bounded
    \item May not satisfy regulatory or certification requirements
    \item Outlier errors possible in poorly-represented regions
\end{itemize}

\textbf{4. Requires Retraining for Different Geometries:}
\begin{itemize}
    \item Model trained on cylinder flow cannot directly compress airfoil flow
    \item Each new geometry/configuration requires fresh training
    \item No transfer learning demonstrated in current work
    \item Training cost repeated for each new problem
\end{itemize}

\textbf{5. \acrshort{ReLU} Limitations for High-Frequency Features:}
\begin{itemize}
    \item \acrshort{ReLU} activations produce smooth approximations
    \item Fine-scale turbulent structures may be under-resolved
    \item Sharp interfaces and discontinuities smoothed
    \item Alternative activations (\acrshort{SIREN}) may address this but require investigation
\end{itemize}

\textbf{6. Limited to Structured Grid Data:}
\begin{itemize}
    \item Current implementation assumes coordinate-based queries
    \item Extension to unstructured meshes requires additional development
    \item Particle-based data (SPH, molecular dynamics) not directly supported
\end{itemize}

\subsection{Comparison with State-of-the-Art}
\label{sec:comparison_sota}

Table~\ref{tab:sota_comparison} positions this work relative to existing compression methods.

\begin{table}[htbp]
\centering
\caption{Comparison with state-of-the-art compression methods}
\label{tab:sota_comparison}
\begin{tabular}{lcccc}
\hline
\textbf{Method} & \textbf{Ratio} & \textbf{Error Bound} & \textbf{Query} & \textbf{Streaming} \\
\hline
gzip/\acrshort{HDF5} & 2-5$\times$ & Lossless & No & No \\
SZ~\cite{sz_compressor} & 10-100$\times$ & Yes & No & Limited \\
ZFP~\cite{zfp_compressor} & 10-50$\times$ & Yes & Partial & No \\
\acrshort{POD}/\acrshort{PCA} & 10-50$\times$ & No & Limited & No \\
NeRF-style~\cite{nerf} & 100-1000$\times$ & No & Yes & No \\
\textbf{This work} & \textbf{27,395$\times$} & No & \textbf{Yes} & \textbf{Yes} \\
\hline
\end{tabular}
\end{table}

\textbf{vs. Traditional Lossless (gzip, \acrshort{HDF5}):}
\begin{itemize}
    \item Compression ratio: 5,000-10,000$\times$ better
    \item Trade-off: Lossy vs. lossless reconstruction
    \item Use case: When exact reconstruction is not required
\end{itemize}

\textbf{vs. Error-Bounded Lossy (SZ, ZFP):}
\begin{itemize}
    \item Compression ratio: 200-2,700$\times$ better
    \item Trade-off: No error bounds vs. guaranteed bounds
    \item Use case: When statistical quality metrics are sufficient
\end{itemize}

\textbf{vs. \acrshort{POD}/\acrshort{PCA}:}
\begin{itemize}
    \item Compression ratio: 500-2,700$\times$ better
    \item Advantages: Continuous queries, streaming capable, no snapshot matrix
    \item Trade-off: No linear basis interpretation
\end{itemize}

\textbf{Positioning in Literature:}
This work advances the state-of-the-art by demonstrating:
\begin{enumerate}
    \item Extreme compression ratios exceeding prior neural network approaches
    \item Online training capability enabling real-time compression
    \item Practical memory efficiency for in-situ deployment
    \item Comprehensive evaluation framework for scientific data
\end{enumerate}

\subsection{Implications for Scientific Computing}
\label{sec:implications}

The results have significant implications for scientific computing workflows:

\textbf{1. Impact on Simulation Workflows:}
\begin{itemize}
    \item \textbf{Reduced I/O bottleneck:} Compression during simulation reduces data written to disk
    \item \textbf{Concurrent processing:} Online mode enables compression without stopping simulation
    \item \textbf{Adaptive monitoring:} Real-time access to compressed representation enables steering
\end{itemize}

\textbf{2. Enabling Higher Resolution Studies:}
\begin{itemize}
    \item Storage constraints often limit spatial/temporal resolution
    \item 27,395:1 compression removes this barrier
    \item Finer meshes and longer simulations become feasible
    \item More timesteps can be saved for transient analysis
\end{itemize}

\textbf{3. Real-Time Visualization Possibilities:}
\begin{itemize}
    \item Query-based reconstruction enables interactive exploration
    \item Compressed models load faster than raw data
    \item Multi-resolution rendering by varying query density
    \item Remote visualization with minimal data transfer
\end{itemize}

\textbf{4. Storage Cost Reduction:}
\begin{itemize}
    \item Direct cost savings: 99.996\% reduction in storage requirements
    \item Enables long-term archival of simulation campaigns
    \item Reduces cloud storage costs for simulation databases
    \item Makes simulation data sharing more practical
\end{itemize}

\textbf{5. Broader Applicability Potential:}
While demonstrated on \acrshort{CFD} data, the approach is applicable to:
\begin{itemize}
    \item Climate and weather simulation data
    \item Structural mechanics (stress, strain fields)
    \item Electromagnetic simulations
    \item Molecular dynamics (coarse-grained fields)
    \item Any spatio-temporal field data with learnable structure
\end{itemize}

\subsection{Summary of Discussion}
\label{sec:discussion_summary}

The experimental results demonstrate that neural network-based compression offers a compelling approach for spatio-temporal scientific data:

\textbf{Key Findings:}
\begin{enumerate}
    \item Compression ratios of 27,395:1 far exceed traditional methods
    \item Reconstruction quality (\acrshort{PSNR}: 16.80 dB, \acrshort{SSIM}: 0.9893) is acceptable for many scientific applications
    \item Online training enables real-time compression with 12$\times$ speedup
    \item Memory efficiency (22.8 MB/timestep constant) enables scalability
\end{enumerate}

\textbf{Trade-offs:}
\begin{enumerate}
    \item No hard error bounds (statistical quality only)
    \item Higher errors in complex flow regions
    \item Requires retraining for new geometries
    \item Quality below threshold for precision-critical applications
\end{enumerate}

\textbf{Recommendations:}
\begin{enumerate}
    \item Use for visualization, archival, and preliminary analysis
    \item Combine with validation on subset for critical applications
    \item Consider hybrid approaches for best quality-efficiency balance
    \item Future work should address error bounds and high-frequency features
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Chapter 7  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Deliverables}
\label{deliverables}

This chapter documents the complete set of deliverables produced as part of this thesis work. The deliverables include software implementations, trained models, documentation, and a reproducibility package enabling independent verification of the results.

\subsection{Software Deliverables}
\label{sec:software_deliverables}

The software implementation is organized as a modular Python package supporting both offline and online neural network-based compression of spatio-temporal data.

\subsubsection{Core Implementation}

The core implementation consists of four main components:

\textbf{1. Offline Training System (\texttt{train\_offline.py}):}
\begin{itemize}
    \item Complete training pipeline for batch processing of full datasets
    \item Configurable hyperparameters (epochs, batch size, learning rate)
    \item Training progress logging and checkpointing
    \item Multi-metric evaluation (\acrshort{MSE}, \acrshort{PSNR}, \acrshort{SSIM})
    \item Model serialization and export
\end{itemize}

\textbf{2. Online Training System (\texttt{train\_online\_mode.py}):}
\begin{itemize}
    \item Temporal window-based incremental training
    \item Configurable window size, stride, and epochs per window
    \item Model state preservation between windows
    \item Window-local normalization handling
    \item Real-time metric tracking and logging
\end{itemize}

\textbf{3. Dataset Loader (\texttt{FlowDataset} class):}
\begin{itemize}
    \item Efficient data loading using PyArrow for large \acrshort{CSV} files
    \item Automatic normalization (z-score standardization)
    \item Configurable train/validation splitting
    \item PyTorch \texttt{Dataset} interface for seamless integration
    \item Memory-efficient batch iteration
\end{itemize}

\textbf{4. Neural Network Model (\texttt{RegressionModel} class):}
\begin{itemize}
    \item Configurable \acrshort{MLP} architecture (input dim, hidden layers, output dim)
    \item \acrshort{ReLU} activation functions
    \item Xavier/He weight initialization
    \item PyTorch \texttt{nn.Module} interface
    \item Support for \acrshort{GPU} acceleration
\end{itemize}

\subsubsection{Analysis Tools}

A comprehensive suite of analysis tools supports evaluation and visualization:

\textbf{1. Test/Inference System (\texttt{test\_inference.py}):}
\begin{itemize}
    \item Load trained models and perform inference
    \item Query-based reconstruction at arbitrary coordinates
    \item Batch prediction for entire timesteps
    \item Performance benchmarking (throughput, latency)
\end{itemize}

\textbf{2. Statistics Generator (\texttt{generate\_statistics.py}):}
\begin{itemize}
    \item Compute comprehensive metrics from training logs
    \item Generate summary tables (\acrshort{MSE}, \acrshort{PSNR}, \acrshort{SSIM}, \acrshort{MAE})
    \item Per-variable performance analysis
    \item Export results to \acrshort{CSV} and JSON formats
\end{itemize}

\textbf{3. Memory Analyzer (\texttt{analyze\_memory\_per\_timestep.py}):}
\begin{itemize}
    \item Track \acrshort{GPU} memory usage during training
    \item Per-timestep memory consumption analysis
    \item Memory efficiency verification
    \item Generate memory usage plots
\end{itemize}

\textbf{4. Visualization Tools (\texttt{visualize\_3d\_all\_variables.py}):}
\begin{itemize}
    \item 3D flow field visualization for all variables
    \item Side-by-side comparison (original vs. reconstructed vs. error)
    \item Multi-panel grid layouts
    \item Customizable colormaps and annotations
    \item High-resolution PNG export
\end{itemize}

\textbf{5. ParaView Exporter (\texttt{export\_to\_paraview.py}):}
\begin{itemize}
    \item Export reconstructed fields to \acrshort{VTP} format
    \item Compatible with ParaView visualization software
    \item Support for time series export
    \item Preserve all flow variables ($V_x$, $V_y$, $P$, \acrshort{TKE})
\end{itemize}

\subsubsection{Code Organization}

The software is organized with the following directory structure:

\begin{verbatim}
Master_Thesis/
├── src/
│   ├── models/
│   │   └── regression_model.py    # Neural network definition
│   ├── data/
│   │   └── flow_dataset.py        # Dataset loader
│   ├── training/
│   │   ├── train_offline.py       # Offline training
│   │   └── train_online_mode.py   # Online training
│   ├── evaluation/
│   │   ├── test_inference.py      # Inference system
│   │   └── generate_statistics.py # Metrics computation
│   ├── visualization/
│   │   ├── visualize_3d_all_variables.py
│   │   └── export_to_paraview.py
│   └── utils/
│       ├── metrics.py             # PSNR, SSIM, etc.
│       └── memory_analyzer.py     # Memory tracking
├── notebooks/
│   └── exploratory_analysis.ipynb # Jupyter notebooks
├── results/
│   ├── models/                    # Saved models
│   ├── logs/                      # Training logs
│   └── plots/                     # Generated figures
├── documents/
│   └── latex/                     # Thesis source
├── requirements.txt               # Dependencies
└── README.md                      # Documentation
\end{verbatim}

\textbf{Dependencies (requirements.txt):}
\begin{itemize}
    \item \texttt{torch >= 2.0.0} - Deep learning framework
    \item \texttt{numpy >= 1.24.0} - Numerical computing
    \item \texttt{pandas >= 2.0.0} - Data manipulation
    \item \texttt{pyarrow >= 12.0.0} - Efficient \acrshort{CSV} loading
    \item \texttt{matplotlib >= 3.7.0} - Plotting
    \item \texttt{scikit-image >= 0.21.0} - \acrshort{SSIM} computation
    \item \texttt{vtk >= 9.2.0} - ParaView export
    \item \texttt{tqdm >= 4.65.0} - Progress bars
\end{itemize}

\subsection{Data Deliverables}
\label{sec:data_deliverables}

\subsubsection{Trained Models}

The following trained models are provided:

\textbf{1. Offline Model (150 epochs):}
\begin{itemize}
    \item File: \texttt{offline\_model\_150epochs.pth}
    \item Size: 0.029 MB (26,768 bytes)
    \item Parameters: 6,692
    \item Performance: \acrshort{PSNR} 16.80 dB, \acrshort{SSIM} 0.9893
\end{itemize}

\textbf{2. Online Model (20 windows):}
\begin{itemize}
    \item File: \texttt{online\_model\_20windows.pth}
    \item Size: 0.029 MB (same architecture)
    \item Performance: \acrshort{PSNR} 15.67 dB, \acrshort{SSIM} 0.9565
\end{itemize}

\textbf{3. Model Checkpoints:}
\begin{itemize}
    \item Offline checkpoints every 10 epochs
    \item Online checkpoints after each window
    \item Enable training resumption and analysis
\end{itemize}

\textbf{4. Normalization Parameters:}
\begin{itemize}
    \item Input normalization: mean and std for $(x, y, z, t)$
    \item Output normalization: mean and std for $(V_x, V_y, P, \text{\acrshort{TKE}})$
    \item Required for inference on new coordinates
\end{itemize}

\subsubsection{Results and Metrics}

\textbf{1. Training Logs (\acrshort{CSV} files):}
\begin{itemize}
    \item \texttt{offline\_training\_log.csv}: Per-epoch metrics
    \item \texttt{online\_training\_log.csv}: Per-window metrics
    \item Columns: epoch/window, loss, \acrshort{PSNR}, \acrshort{SSIM}, time
\end{itemize}

\textbf{2. Performance Metrics (JSON files):}
\begin{itemize}
    \item \texttt{offline\_final\_metrics.json}: Final offline results
    \item \texttt{online\_final\_metrics.json}: Final online results
    \item \texttt{comparison\_metrics.json}: Side-by-side comparison
\end{itemize}

\textbf{3. Comparison Tables:}
\begin{itemize}
    \item Offline vs. online training comparison
    \item Per-variable performance breakdown
    \item Compression ratio analysis
\end{itemize}

\subsubsection{Visualizations}

\textbf{1. Training Progress Plots (PNG):}
\begin{itemize}
    \item \texttt{training\_progress\_full.png}: 4-panel offline progress
    \item \texttt{metrics\_comparison.png}: Online training progression
    \item \texttt{point\_tracking.png}: Point-wise convergence
\end{itemize}

\textbf{2. 3D Flow Visualizations (PNG):}
\begin{itemize}
    \item \texttt{3d\_flow\_pressure\_t0.0396.png}: Pressure field
    \item \texttt{3d\_flow\_velocity\_x\_t0.0396.png}: X-velocity field
    \item \texttt{3d\_flow\_velocity\_y\_t0.0396.png}: Y-velocity field
    \item \texttt{3d\_flow\_tke\_t0.0396.png}: \acrshort{TKE} field
    \item \texttt{3d\_flow\_all\_variables\_grid\_t0.0396.png}: Combined grid
    \item \texttt{3d\_flow\_cylinder\_regions\_t0.0396.png}: Regional analysis
\end{itemize}

\textbf{3. ParaView Exports (\acrshort{VTP} files):}
\begin{itemize}
    \item Reconstructed flow fields in \acrshort{VTK} PolyData format
    \item Compatible with ParaView for advanced visualization
    \item Time series support for animation
\end{itemize}

\textbf{4. Memory Analysis Plots:}
\begin{itemize}
    \item \texttt{memory\_per\_timestep\_analysis.png}: Memory usage plot
    \item \texttt{memory\_summary\_table.png}: Memory statistics
\end{itemize}

\subsection{Documentation Deliverables}
\label{sec:documentation_deliverables}

\subsubsection{Technical Documentation}

\textbf{1. API Documentation:}
\begin{itemize}
    \item Docstrings for all classes and functions
    \item Parameter descriptions and type hints
    \item Return value specifications
    \item Example usage in docstrings
\end{itemize}

\textbf{2. Function Descriptions:}
\begin{itemize}
    \item \texttt{RegressionModel}: Neural network architecture
    \item \texttt{FlowDataset}: Data loading and preprocessing
    \item \texttt{train\_epoch()}: Single epoch training
    \item \texttt{evaluate()}: Model evaluation
    \item \texttt{compute\_psnr()}, \texttt{compute\_ssim()}: Metric functions
\end{itemize}

\textbf{3. Usage Examples:}
\begin{itemize}
    \item Training a new model from scratch
    \item Loading and using a pre-trained model
    \item Performing inference on new coordinates
    \item Generating visualizations
\end{itemize}

\subsubsection{User Guides}

\textbf{1. Installation Guide:}
\begin{enumerate}
    \item Clone repository
    \item Create Python virtual environment
    \item Install dependencies: \texttt{pip install -r requirements.txt}
    \item Verify installation with test script
\end{enumerate}

\textbf{2. Training Guide:}
\begin{itemize}
    \item \textbf{Offline training:}
    \begin{verbatim}
python src/training/train_offline.py \
    --data_path data/flow_data.csv \
    --epochs 150 \
    --batch_size 512 \
    --output_dir results/models/
    \end{verbatim}

    \item \textbf{Online training:}
    \begin{verbatim}
python src/training/train_online_mode.py \
    --data_path data/flow_data.csv \
    --window_size 3 \
    --stride 1 \
    --epochs_per_window 10 \
    --num_windows 20
    \end{verbatim}
\end{itemize}

\textbf{3. Visualization Guide (ParaView):}
\begin{enumerate}
    \item Export data: \texttt{python src/visualization/export\_to\_paraview.py}
    \item Open ParaView and load \texttt{.vtp} files
    \item Apply filters (Contour, Streamlines, etc.)
    \item Create animations for time series
\end{enumerate}

\textbf{4. Troubleshooting Guide:}
\begin{itemize}
    \item Common errors and solutions
    \item Memory issues and workarounds
    \item \acrshort{GPU} configuration
    \item Data format requirements
\end{itemize}

\subsubsection{Thesis Document}

\textbf{1. Thesis PDF:}
\begin{itemize}
    \item Complete thesis document
    \item All figures and tables embedded
    \item Bibliography with citations
\end{itemize}

\textbf{2. LaTeX Source:}
\begin{itemize}
    \item \texttt{main.tex}: Main document
    \item \texttt{references.bib}: Bibliography database
    \item \texttt{images/}: Figure files
    \item Compilable with standard LaTeX distribution
\end{itemize}

\textbf{3. Bibliography:}
\begin{itemize}
    \item BibTeX format
    \item All cited references
    \item Vancouver citation style
\end{itemize}

\subsection{Reproducibility Package}
\label{sec:reproducibility}

A complete reproducibility package enables independent verification of all results.

\subsubsection{Environment Setup}

\textbf{Python Environment:}
\begin{verbatim}
# Create environment
conda create -n neural_compression python=3.10
conda activate neural_compression

# Install PyTorch with CUDA
pip install torch torchvision --index-url \
    https://download.pytorch.org/whl/cu118

# Install other dependencies
pip install -r requirements.txt
\end{verbatim}

\textbf{Hardware Requirements:}
\begin{itemize}
    \item \acrshort{GPU}: NVIDIA with CUDA support (recommended)
    \item RAM: 16 GB minimum
    \item Storage: 2 GB for code and results
\end{itemize}

\subsubsection{Data Access Instructions}

The \acrshort{CFD} dataset used in this thesis:
\begin{itemize}
    \item \textbf{Source:} Vortex shedding simulation (ANSYS Fluent)
    \item \textbf{Format:} \acrshort{CSV} with columns: $x$, $y$, $z$, $t$, $V_x$, $V_y$, $P$, \acrshort{TKE}
    \item \textbf{Size:} 7,919,100 samples (794.44 MB)
    \item \textbf{Access:} Available upon request from the author
\end{itemize}

\subsubsection{Training Scripts with Configurations}

\textbf{Offline Training (reproduce Table~\ref{tab:final_offline_metrics}):}
\begin{verbatim}
python src/training/train_offline.py \
    --epochs 150 \
    --batch_size 512 \
    --lr 0.001 \
    --hidden_layers 64 64 32 \
    --seed 42
\end{verbatim}

\textbf{Online Training (reproduce Table~\ref{tab:final_online_metrics}):}
\begin{verbatim}
python src/training/train_online_mode.py \
    --window_size 3 \
    --stride 1 \
    --epochs_per_window 10 \
    --num_windows 20 \
    --seed 42
\end{verbatim}

\subsubsection{Expected Results Benchmarks}

Table~\ref{tab:expected_results} provides expected results for verification.

\begin{table}[htbp]
\centering
\caption{Expected results for reproducibility verification}
\label{tab:expected_results}
\begin{tabular}{lcc}
\hline
\textbf{Metric} & \textbf{Offline} & \textbf{Online} \\
\hline
\acrshort{MSE} & $0.021 \pm 0.002$ & $0.027 \pm 0.003$ \\
\acrshort{PSNR} (dB) & $16.8 \pm 0.3$ & $15.7 \pm 0.4$ \\
\acrshort{SSIM} & $0.989 \pm 0.005$ & $0.956 \pm 0.008$ \\
Training Time (min) & $195 \pm 15$ & $16 \pm 2$ \\
\hline
\end{tabular}
\end{table}

Note: Exact values may vary slightly due to hardware differences and stochastic initialization. Results should fall within the specified tolerances.

\subsubsection{Verification Procedures}

\textbf{Step 1: Environment Verification}
\begin{verbatim}
python -c "import torch; print(torch.cuda.is_available())"
# Expected: True
\end{verbatim}

\textbf{Step 2: Data Verification}
\begin{verbatim}
python scripts/verify_data.py --data_path data/flow_data.csv
# Expected: 7,919,100 samples, 8 columns
\end{verbatim}

\textbf{Step 3: Training Verification}
\begin{verbatim}
python scripts/run_tests.py --test training
# Expected: All tests pass
\end{verbatim}

\textbf{Step 4: Results Verification}
\begin{verbatim}
python scripts/verify_results.py \
    --model results/models/offline_model.pth \
    --expected_psnr 16.8 \
    --tolerance 0.5
# Expected: PASS
\end{verbatim}

\subsection{Summary of Deliverables}
\label{sec:deliverables_summary}

Table~\ref{tab:deliverables_summary} provides a complete summary of all deliverables.

\begin{table}[htbp]
\centering
\caption{Summary of thesis deliverables}
\label{tab:deliverables_summary}
\begin{tabular}{llc}
\hline
\textbf{Category} & \textbf{Item} & \textbf{Status} \\
\hline
\multirow{4}{*}{Software} & Offline training system & \checkmark \\
 & Online training system & \checkmark \\
 & Analysis tools (5 scripts) & \checkmark \\
 & Visualization tools & \checkmark \\
\hline
\multirow{4}{*}{Data} & Trained models (2) & \checkmark \\
 & Training logs & \checkmark \\
 & Visualizations (10+ figures) & \checkmark \\
 & ParaView exports & \checkmark \\
\hline
\multirow{3}{*}{Documentation} & Technical documentation & \checkmark \\
 & User guides & \checkmark \\
 & Thesis document & \checkmark \\
\hline
\multirow{3}{*}{Reproducibility} & Environment setup & \checkmark \\
 & Training scripts & \checkmark \\
 & Verification procedures & \checkmark \\
\hline
\end{tabular}
\end{table}

All deliverables are organized in the project repository and documented for ease of use and reproducibility.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Chapter 8  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Future Scope}
\label{future_scope}

This chapter outlines potential directions for extending and improving the neural network-based compression framework. The current work demonstrates the feasibility of learning continuous representations of spatio-temporal flow data using compact neural networks. However, several opportunities exist to enhance accuracy, efficiency, and applicability to broader scientific computing scenarios.

\subsection{Advanced Network Architectures}
\label{sec:advanced_architectures}

\subsubsection{Sinusoidal Activation Functions}

The current implementation uses ReLU activations, which are well-suited for learning smooth functions but may struggle with high-frequency content. As discussed in Section~\ref{sec:discussion_offline}, the error analysis shows higher reconstruction errors in regions with sharp gradients and complex turbulent structures.

SIREN (Sinusoidal Representation Networks)~\cite{siren} addresses this limitation by using periodic sine activations:
\begin{equation}
\sigma(z) = \sin(\omega z)
\end{equation}

The periodic nature of sinusoidal activations enables SIREN networks to represent higher-frequency components with fewer parameters. Future work could replace the ReLU-based architecture with SIREN to investigate whether:
\begin{itemize}
    \item Reconstruction accuracy improves in high-gradient regions
    \item Lower MSE and higher PSNR can be achieved with the same parameter count
    \item The network better captures fine-scale turbulent structures
\end{itemize}

The trade-off would be more complex weight initialization requirements and potentially slower training convergence, which should be carefully evaluated.

\subsubsection{Deeper and Wider Networks}

The current architecture uses 3 hidden layers with widths [64, 64, 32], totaling 6,692 parameters. This compact design achieves a compression ratio of 9,456:1 but limits representational capacity.

Systematic exploration of network depth and width could establish accuracy-compression trade-offs:
\begin{itemize}
    \item \textbf{Deeper networks} (5-10 layers): May better capture hierarchical flow structures and multi-scale phenomena
    \item \textbf{Wider networks} (128-512 neurons per layer): Could improve point-wise accuracy at the cost of reduced compression ratios
    \item \textbf{Adaptive architectures}: Network complexity could vary based on flow complexity in different spatial regions
\end{itemize}

Such studies would help identify optimal network configurations for different accuracy requirements and application domains.

\subsubsection{Specialized Architectures for Spatio-Temporal Data}

Several advanced architectures could better exploit the spatio-temporal structure:

\textbf{Fourier Feature Networks:} Mapping input coordinates to high-dimensional Fourier features before processing by an MLP can improve representation of high-frequency content. This approach has shown success in neural rendering and could benefit flow field reconstruction.

\textbf{Multi-Scale Networks:} Combining multiple networks operating at different spatial scales could capture both coarse flow structures and fine turbulent details. A coarse network could represent large-scale pressure gradients while a fine network captures local vorticity structures.

\textbf{Physics-Informed Neural Networks (PINNs):} Incorporating governing equations (Navier-Stokes) as soft constraints during training~\cite{pinn} could improve physical consistency and reduce data requirements. The loss function would include both data-fitting terms and physics residual terms.

\subsection{In-Situ and In-Transit Compression}
\label{sec:future_insitu}

The current implementation operates offline on complete datasets. A major future direction is implementing true in-situ or in-transit compression integrated with running simulations, as described in the theoretical framework (Section~\ref{sec:in_situ_processing}).

\subsubsection{Streaming Training}

Adapting the training procedure for streaming data presents several challenges:

\textbf{Incremental Learning:} The network must learn from data as it arrives without multiple passes through historical data. Online learning algorithms or experience replay buffers could enable effective training with limited data access.

\textbf{Time Budget Constraints:} Training must complete within the interval between simulation timesteps. This requires:
\begin{itemize}
    \item Efficient mini-batch optimization
    \item Early stopping based on convergence detection
    \item Adaptive learning rates to accelerate convergence
\end{itemize}

\textbf{Memory Management:} Only a subset of data points can be retained in memory. Intelligent sampling strategies must select representative points that capture essential flow features.

\subsubsection{Integration with Simulation Frameworks}

Practical deployment requires integration with existing simulation software:

\textbf{Coupling Interfaces:} Develop APIs to receive data from CFD codes (OpenFOAM, ANSYS Fluent, etc.) and return compressed representations. This could leverage existing in-situ frameworks like Catalyst~\cite{paraview_catalyst} or Damaris~\cite{damaris}.

\textbf{Concurrent Execution:} The compression system should run concurrently with the simulation without introducing significant performance overhead. This requires careful resource allocation between simulation and compression processes.

\textbf{Data Staging:} In-transit approaches using ADIOS2~\cite{adios2} or similar middleware could transfer data to dedicated compression nodes, avoiding interference with simulation performance.

\subsection{Intelligent Sampling Strategies}
\label{sec:sampling_strategies}

The current implementation uses random sampling for train-validation splitting. More sophisticated sampling strategies could improve efficiency:

\subsubsection{Adaptive Spatial Sampling}

Not all spatial regions require equal sampling density:

\textbf{Gradient-Based Sampling:} Sample more densely in regions with high gradients or rapid spatial variations. Areas with smooth, slowly-varying fields require fewer samples.

\textbf{Error-Driven Sampling:} During training, identify regions with high reconstruction error and increase sampling density in those areas. This adaptive approach focuses computational effort where needed.

\textbf{Feature-Based Sampling:} Sample more densely near important flow features (vortices, shocks, boundary layers) and sparsely in uniform flow regions.

\subsubsection{Temporal Sampling}

For time-evolving flows, temporal sampling strategies could exploit temporal correlations:

\textbf{Keyframe Selection:} Identify critical timesteps that capture important flow events (transition to turbulence, vortex shedding cycles) and sample more densely at these times.

\textbf{Temporal Interpolation:} Train the network on a subset of timesteps and evaluate whether it can interpolate intermediate times. If successful, only keyframes need to be processed during simulation.

\subsection{Comparison with Traditional Compression Methods}
\label{sec:comparison_traditional}

A comprehensive evaluation should compare neural network compression against established methods:

\subsubsection{Lossy Compressors}

Direct comparison with error-bounded lossy compressors would establish relative strengths:

\textbf{SZ Compressor~\cite{sz_compressor}:} Prediction-based compressor widely used in scientific computing. Comparison would assess whether neural networks achieve better compression ratios at equivalent error bounds.

\textbf{ZFP~\cite{zfp_compressor}:} Fixed-rate compression based on block transforms. Useful for understanding trade-offs between fixed-rate and variable-rate compression.

\textbf{MGARD~\cite{mgard_multivariate}:} Multilevel compressor that exploits multi-resolution structure. Comparing with MGARD would reveal whether neural networks better capture multi-scale flow features.

\subsubsection{Evaluation Metrics Beyond MSE}

While MSE, PSNR, and SSIM provide useful quality measures, scientific applications require additional evaluation:

\textbf{Physical Consistency:} Verify that compressed data satisfies conservation laws (mass, momentum, energy). Derived quantities (vorticity, strain rate) should be computed from compressed data and compared to ground truth.

\textbf{Feature Preservation:} Assess whether important flow features (vortices, shocks, separation points) are preserved. Feature detection algorithms could quantify this objectively.

\textbf{Downstream Analysis Impact:} Evaluate how compression affects typical analysis tasks (computing drag coefficients, identifying instabilities, extracting POD modes).

\subsection{Extension to Higher Dimensions and Multiple Physics}
\label{sec:higher_dimensions}

\subsubsection{Full 3D+Time Representation}

The current dataset appears to be 2D spatial + time. Extension to full 3D spatial domains would:
\begin{itemize}
    \item Increase input dimensionality from 3 to 4 (x, y, z, t)
    \item Dramatically increase dataset sizes (billions of samples)
    \item Test scalability of the approach to truly large-scale simulations
\end{itemize}

This would require careful memory management and potentially distributed training approaches.

\subsubsection{Multi-Physics Simulations}

Many scientific simulations couple multiple physical phenomena (fluid-structure interaction, combustion, magnetohydrodynamics). Extending the framework to multi-physics scenarios involves:

\textbf{Increased Output Dimensionality:} Instead of 4 flow variables, multi-physics simulations may track 10-20+ variables (velocities, pressures, temperatures, species concentrations, electromagnetic fields).

\textbf{Coupled Representations:} The network should learn relationships between different physical quantities. For example, in combustion, temperature and species concentrations are strongly coupled.

\textbf{Variable Time Scales:} Different physics may evolve at different rates. The network must capture phenomena ranging from fast chemical reactions to slow thermal diffusion.

\subsection{Model Compression and Acceleration}
\label{sec:model_compression}

While the current model is already compact (6,692 parameters), further compression could enable deployment on resource-constrained platforms:

\subsubsection{Network Pruning}

Pruning removes unnecessary network connections based on weight magnitude or gradient information:
\begin{itemize}
    \item \textbf{Magnitude pruning}: Remove weights below a threshold
    \item \textbf{Structured pruning}: Remove entire neurons or layers
    \item \textbf{Iterative pruning}: Gradually remove connections while retraining
\end{itemize}

This could reduce model size by 50-90\% with minimal accuracy loss.

\subsubsection{Quantization}

Reducing parameter precision from 32-bit floats to 16-bit or 8-bit representations:
\begin{itemize}
    \item \textbf{Post-training quantization}: Quantize trained models
    \item \textbf{Quantization-aware training}: Train with quantization in mind
    \item \textbf{Mixed precision}: Use different precision for different layers
\end{itemize}

8-bit quantization could reduce model size by 4×, achieving compression ratios exceeding 30,000:1.

\subsubsection{Knowledge Distillation}

Train a smaller student network to mimic a larger, more accurate teacher network. This could combine the accuracy of large networks with the efficiency of small networks.

\subsection{Uncertainty Quantification}
\label{sec:uncertainty}

Scientific applications require understanding prediction uncertainty:

\subsubsection{Ensemble Methods}

Train multiple networks with different initializations or data subsets and use ensemble variance to estimate prediction uncertainty:
\begin{equation}
\sigma^2(\mathbf{x}) = \frac{1}{K} \sum_{k=1}^{K} (f_k(\mathbf{x}) - \bar{f}(\mathbf{x}))^2
\end{equation}
where $f_k$ are individual ensemble members and $\bar{f}$ is the ensemble mean.

\subsubsection{Bayesian Neural Networks}

Replace point estimates of weights with probability distributions. This provides principled uncertainty estimates but increases computational cost.

\subsubsection{Conformal Prediction}

Construct prediction intervals with probabilistic guarantees using conformal prediction theory. This provides rigorous uncertainty bounds without requiring Bayesian approaches.

\subsection{Practical Deployment and Usability}
\label{sec:practical_deployment}

Transitioning from research prototype to production-ready tool requires:

\subsubsection{Software Engineering}

\textbf{Production-Quality Code:} Refactor implementation for robustness, testing, and maintainability. Develop comprehensive test suites and documentation.

\textbf{User Interfaces:} Create command-line tools and graphical interfaces for non-expert users. Integration with existing analysis and visualization software (ParaView, VisIt).

\textbf{Performance Optimization:} Profile and optimize critical code paths. Leverage GPU acceleration, parallel processing, and efficient data structures.

\subsubsection{HPC Integration}

\textbf{Scalability:} Extend to distributed training on HPC clusters for very large datasets. Use data parallelism and model parallelism as appropriate.

\textbf{Portability:} Ensure compatibility across different computing platforms (CPUs, GPUs, TPUs) and operating systems.

\textbf{Resource Management:} Integrate with job schedulers (SLURM, PBS) and resource managers common in HPC environments.

\subsubsection{Community Adoption}

\textbf{Open Source Release:} Publish code and models under permissive licenses to enable community contributions and adoption.

\textbf{Benchmarking:} Establish standard benchmark datasets and evaluation protocols for comparing different neural compression approaches.

\textbf{Collaboration:} Partner with domain scientists to validate the approach on diverse applications and gather feedback for improvements.

The future directions outlined in this chapter represent a roadmap for transforming the current proof-of-concept into a comprehensive, production-ready framework for neural network-based scientific data compression. Each direction offers opportunities for significant research contributions and practical impact.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Chapter 9  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\thispagestyle{empty}
\thispagestyle{plain}

\section{Conclusions}
\label{conclusions}

This thesis has investigated neural network-based compression of spatio-temporal scientific data, developing and validating both offline and online training approaches for learning continuous representations of \acrshort{CFD} flow fields. This chapter summarizes the achievements, key findings, contributions, limitations, and concludes with remarks on the significance and potential impact of this work.

\subsection{Summary of Achievements}
\label{sec:summary_achievements}

The research objectives established in Chapter~\ref{aim} have been successfully achieved:

\textbf{1. Neural Network Architecture Design:}
\begin{itemize}
    \item Developed a coordinate-based \acrshort{MLP} architecture mapping spatio-temporal coordinates $(x, y, z, t)$ to flow field variables ($V_x$, $V_y$, $P$, \acrshort{TKE})
    \item Compact design with 6,692 parameters achieving extreme compression
    \item Architecture validated on real-world vortex shedding \acrshort{CFD} data
\end{itemize}

\textbf{2. Offline Training System:}
\begin{itemize}
    \item Implemented complete offline training pipeline processing 7.9 million samples
    \item Achieved compression ratio of \textbf{27,395:1} (794.44 MB $\rightarrow$ 0.029 MB)
    \item Reconstruction quality: \acrshort{PSNR} 16.80 dB, \acrshort{SSIM} 0.9893
    \item Training completed in 195 minutes (150 epochs) on NVIDIA A100 \acrshort{GPU}
\end{itemize}

\textbf{3. Online Training System:}
\begin{itemize}
    \item Developed temporal window-based incremental learning approach
    \item Demonstrated real-time compression capability with \textbf{12$\times$ speedup}
    \item Achieved \acrshort{PSNR} 15.67 dB, \acrshort{SSIM} 0.9565 (93\% of offline quality)
    \item Validated feasibility for in-situ and in-transit processing scenarios
\end{itemize}

\textbf{4. Comprehensive Evaluation:}
\begin{itemize}
    \item Established multi-metric evaluation framework (\acrshort{MSE}, \acrshort{PSNR}, \acrshort{SSIM}, \acrshort{MAE})
    \item Per-variable performance analysis identifying variable-specific accuracy
    \item Spatial error distribution analysis revealing region-dependent quality
    \item Memory efficiency validation confirming constant 22.8 MB per timestep
\end{itemize}

\textbf{5. Offline vs. Online Comparison:}
\begin{itemize}
    \item Systematic comparison establishing quality-speed trade-offs
    \item Identified 6.7\% quality reduction for 1,230\% speed improvement
    \item Provided use case recommendations for selecting appropriate mode
\end{itemize}

\textbf{6. Validation on \acrshort{CFD} Dataset:}
\begin{itemize}
    \item Demonstrated on vortex shedding simulation with 300 timesteps
    \item Successfully compressed complex flow features (wake, vortices, turbulence)
    \item Maintained physical consistency across coupled flow variables
\end{itemize}

\subsection{Key Findings}
\label{sec:key_findings}

The experimental investigation yielded several important findings:

\textbf{1. Neural Networks are Effective for \acrshort{CFD} Compression:}
\begin{itemize}
    \item Coordinate-based neural networks can learn compact representations of complex flow fields
    \item Compression ratios exceeding 27,000:1 are achievable with acceptable quality loss
    \item The approach significantly outperforms traditional lossless (5,000$\times$) and lossy (200-2,700$\times$) compression methods in compression ratio
\end{itemize}

\textbf{2. Continuous Function Representation is Advantageous:}
\begin{itemize}
    \item Representing data as continuous functions enables query-based reconstruction at arbitrary coordinates
    \item Natural interpolation capability without explicit interpolation algorithms
    \item Multi-resolution queries by varying spatial sampling density
    \item Differentiable representation enabling gradient-based analysis
\end{itemize}

\textbf{3. Online Training is Feasible with Minimal Quality Loss:}
\begin{itemize}
    \item Temporal window mechanism enables processing of streaming data
    \item Incremental learning preserves knowledge across windows
    \item Quality loss of only 6.7\% (\acrshort{PSNR}) for 12$\times$ speed improvement
    \item Compatible with in-situ processing requirements
\end{itemize}

\textbf{4. Memory Efficiency Enables Scalability:}
\begin{itemize}
    \item Constant memory usage (22.8 MB/timestep) independent of dataset size
    \item Enables compression of arbitrarily long simulations
    \item Suitable for memory-constrained \acrshort{HPC} environments
    \item No memory barriers for scaling to larger problems
\end{itemize}

\textbf{5. Query-Based Inference Adds Flexibility:}
\begin{itemize}
    \item Random access to any coordinate without sequential decompression
    \item Enables interactive exploration of compressed datasets
    \item Supports real-time visualization with minimal latency
    \item Throughput exceeds 100,000 queries per second
\end{itemize}

\subsection{Contributions to the Field}
\label{sec:contributions}

This thesis makes the following contributions to the field of scientific data compression:

\textbf{1. Novel Application of Coordinate-Based Networks to \acrshort{CFD}:}
\begin{itemize}
    \item First comprehensive study applying implicit neural representations to \acrshort{CFD} flow field compression
    \item Demonstrated extreme compression ratios (27,395:1) exceeding prior work
    \item Established viability for multi-variable flow field data ($V_x$, $V_y$, $P$, \acrshort{TKE})
\end{itemize}

\textbf{2. Temporal Window Mechanism for Online Training:}
\begin{itemize}
    \item Novel incremental learning approach for streaming spatio-temporal data
    \item Sliding window design with configurable size, stride, and epochs
    \item Demonstrated real-time compression capability for in-situ applications
    \item Published methodology enabling reproduction and extension
\end{itemize}

\textbf{3. Comprehensive Evaluation Framework:}
\begin{itemize}
    \item Multi-metric assessment combining \acrshort{MSE}, \acrshort{PSNR}, \acrshort{SSIM}, and \acrshort{MAE}
    \item Per-variable performance analysis methodology
    \item Spatial error distribution analysis techniques
    \item Systematic offline vs. online comparison protocol
\end{itemize}

\textbf{4. Memory-Efficient Implementation:}
\begin{itemize}
    \item Demonstrated constant memory usage independent of dataset size
    \item Validated scalability properties for large-scale applications
    \item Established memory benchmarks for neural compression approaches
\end{itemize}

\textbf{5. Open-Source Software Contribution:}
\begin{itemize}
    \item Complete implementation of offline and online training systems
    \item Analysis and visualization tools for neural compression
    \item Documentation and reproducibility package
    \item Enables community adoption and further research
\end{itemize}

\subsection{Limitations and Future Directions}
\label{sec:limitations_future}

\subsubsection{Summary of Limitations}

Several limitations of the current work should be acknowledged:

\begin{enumerate}
    \item \textbf{Reconstruction Quality:} \acrshort{PSNR} of 16.80 dB is below the 30+ dB threshold for high-precision applications requiring exact data preservation

    \item \textbf{No Error Bounds:} Unlike SZ or ZFP, the approach provides statistical quality metrics but no guaranteed maximum errors, limiting applicability for certification-critical data

    \item \textbf{Geometry-Specific Training:} Models trained on one geometry cannot directly compress different configurations without retraining

    \item \textbf{High-Frequency Limitations:} \acrshort{ReLU} activations struggle with sharp gradients and fine-scale turbulent structures, resulting in higher errors in complex flow regions

    \item \textbf{Structured Data Assumption:} Current implementation assumes coordinate-based queries on structured grids; extension to unstructured meshes requires additional development
\end{enumerate}

\subsubsection{Most Promising Future Directions}

Based on the findings, the most impactful future research directions are:

\textbf{Short-Term Improvements (6-12 months):}
\begin{enumerate}
    \item \textbf{\acrshort{SIREN} Activation Functions:} Replace \acrshort{ReLU} with sinusoidal activations to better capture high-frequency features and potentially improve \acrshort{PSNR} by 3-5 dB

    \item \textbf{Adaptive Network Sizing:} Develop methods to automatically determine optimal network architecture based on data complexity

    \item \textbf{Error Estimation:} Implement uncertainty quantification to provide confidence bounds on reconstructions
\end{enumerate}

\textbf{Long-Term Research Opportunities (1-3 years):}
\begin{enumerate}
    \item \textbf{True In-Situ Integration:} Integrate with simulation codes (OpenFOAM, ANSYS) via Catalyst or ADIOS2 frameworks for production deployment

    \item \textbf{Transfer Learning:} Develop pre-trained models that can be fine-tuned for new geometries with minimal additional training

    \item \textbf{Hybrid Compression:} Combine neural network compression with error-bounded methods to achieve both extreme compression and guaranteed error bounds

    \item \textbf{3D+Time Extension:} Scale the approach to full three-dimensional spatial domains with time, addressing the associated computational challenges
\end{enumerate}

\subsection{Closing Remarks}
\label{sec:closing_remarks}

\subsubsection{Significance of the Work}

This thesis addresses a critical challenge in computational science: the management of massive spatio-temporal datasets generated by modern simulations. The demonstrated compression ratio of 27,395:1 represents a transformative capability that could fundamentally change how scientific data is stored, transmitted, and archived.

The significance extends beyond mere compression ratios. The continuous function representation paradigm offers a new way of thinking about scientific data—not as discrete samples to be stored, but as learnable functions to be represented. This shift enables capabilities not possible with traditional compression: query-based access, natural interpolation, and compact representation of smooth physical fields.

\subsubsection{Potential Impact}

The potential impact of this work spans multiple domains:

\textbf{Scientific Computing:}
\begin{itemize}
    \item Enable storage of simulation campaigns previously infeasible due to data volumes
    \item Reduce \acrshort{I/O} bottlenecks in \acrshort{HPC} workflows through in-situ compression
    \item Facilitate data sharing and collaboration through compact representations
\end{itemize}

\textbf{Engineering Applications:}
\begin{itemize}
    \item Support higher-resolution simulations by removing storage constraints
    \item Enable real-time monitoring and visualization during simulations
    \item Reduce cloud storage and data transfer costs
\end{itemize}

\textbf{Research Community:}
\begin{itemize}
    \item Open-source tools enabling reproduction and extension of this work
    \item Methodology applicable to diverse spatio-temporal data beyond \acrshort{CFD}
    \item Foundation for future research in neural scientific data compression
\end{itemize}

\subsubsection{Final Thoughts}

The convergence of neural network advances and scientific computing challenges creates an opportune moment for neural compression approaches. This thesis demonstrates that coordinate-based neural networks can effectively compress complex spatio-temporal data while enabling new query-based interaction paradigms.

While challenges remain—particularly regarding reconstruction quality for precision-critical applications and integration with production simulation workflows—the results presented here establish a strong foundation for future development. The 27,395:1 compression ratio, combined with the demonstrated online training capability, suggests that neural network-based compression will play an increasingly important role in managing the data deluge of modern scientific computing.

As simulations grow ever larger and more detailed, the need for intelligent data reduction will only intensify. Neural network-based compression, as demonstrated in this thesis, offers a promising path forward—one that balances extreme compression with acceptable quality, offline optimization with real-time capability, and research innovation with practical applicability.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  References %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\thispagestyle{empty}
\thispagestyle{plain}

\bibliography{references}
\bibliographystyle{vancouver}

%\putbib[references]
%\end{bibunit}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Appendix  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\clearpage
%\thispagestyle{empty}
%\thispagestyle{plain}

%\appendix
%\addcontentsline{toc}{section}{Appendices}

%\section{First appendix}
%\hl{TOD}

%\thispagestyle{empty}
%\thispagestyle{plain}

%\section{Second appendix}
%\hl{TOD}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Appendix References  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\putbib[references]
%\end{bibunit}

\end{document}
